#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆé‡æ’å™¨æ¨¡å— - åœ¨åŸæœ‰åŸºç¡€ä¸Šæ·»åŠ ä¼˜åŒ–åŠŸèƒ½
æ–°å¢åŠŸèƒ½ï¼š
1. å¤šæ¨¡å‹ensembleé‡æ’
2. å­¦ä¹ å¼é‡æ’å™¨
3. è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥
4. ç»“æœå¤šæ ·æ€§ä¼˜åŒ–
5. ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’
"""

import numpy as np
import os
from sentence_transformers import CrossEncoder
import re
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity

# å¯¼å…¥åŸæœ‰é‡æ’å™¨
from .reranker import CrossEncoderReranker, SimpleReranker


class MultiModelEnsembleReranker:
    """å¤šæ¨¡å‹ensembleé‡æ’å™¨"""
    
    def __init__(self, model_names=None, keywords=None, weights=None):
        self.keywords = keywords
        self.models = {}
        self.weights = weights or {}
        
        # é»˜è®¤æ¨¡å‹é…ç½®
        if model_names is None:
            model_names = [
                'cross-encoder/ms-marco-MiniLM-L-6-v2',
                'cross-encoder/ms-marco-TinyBERT-L-2-v2'
            ]
        
        # åŠ è½½å¤šä¸ªæ¨¡å‹
        for model_name in model_names:
            try:
                print(f"æ­£åœ¨åŠ è½½é‡æ’æ¨¡å‹: {model_name}")
                # æ·»åŠ è®¾å¤‡å‚æ•°å’Œæ›´å®‰å…¨çš„åŠ è½½æ–¹å¼
                model = CrossEncoder(model_name, device='cpu', trust_remote_code=False)
                self.models[model_name] = model
                if model_name not in self.weights:
                    self.weights[model_name] = 1.0
                print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {e}")
                
                # å°è¯•æ¸…ç†ç¼“å­˜åé‡æ–°åŠ è½½
                try:
                    import shutil
                    cache_dirs = [
                        os.path.expanduser("~/.cache/torch/sentence_transformers"),
                        os.path.expanduser("~/.cache/huggingface/transformers"),
                        os.path.expanduser("~/.cache/huggingface/hub")
                    ]
                    
                    for cache_dir in cache_dirs:
                        if os.path.exists(cache_dir):
                            try:
                                shutil.rmtree(cache_dir)
                            except:
                                pass
                    
                    print(f"ğŸ”„ æ¸…ç†ç¼“å­˜åé‡è¯•: {model_name}")
                    model = CrossEncoder(model_name, device='cpu', trust_remote_code=False)
                    self.models[model_name] = model
                    if model_name not in self.weights:
                        self.weights[model_name] = 1.0
                    print(f"âœ… æ¸…ç†ç¼“å­˜åæ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
                    
                except Exception as e2:
                    print(f"âš ï¸ æ¸…ç†ç¼“å­˜åä»ç„¶å¤±è´¥: {e2}")
                    continue
        
        if not self.models:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„é‡æ’æ¨¡å‹")
    
    def rerank(self, candidates, k=None):
        """ä½¿ç”¨å¤šæ¨¡å‹ensembleè¿›è¡Œé‡æ’"""
        if not self.models or not candidates:
            return candidates
        
        print(f"ğŸ”„ ä½¿ç”¨ {len(self.models)} ä¸ªæ¨¡å‹è¿›è¡Œensembleé‡æ’...")
        
        try:
            query = ' '.join(self.keywords) if self.keywords else "relevant content"
            query_text_pairs = [(query, item['content']) for item in candidates]
            
            # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
            all_scores = {}
            for model_name, model in self.models.items():
                print(f"  ä½¿ç”¨æ¨¡å‹ {model_name} é¢„æµ‹...")
                scores = model.predict(query_text_pairs, show_progress_bar=False)
                all_scores[model_name] = scores
            
            # è®¡ç®—ensembleåˆ†æ•°
            ensemble_scores = []
            for i in range(len(candidates)):
                weighted_score = 0.0
                total_weight = 0.0
                
                for model_name, scores in all_scores.items():
                    weight = self.weights.get(model_name, 1.0)
                    weighted_score += scores[i] * weight
                    total_weight += weight
                
                ensemble_score = weighted_score / total_weight if total_weight > 0 else 0.0
                ensemble_scores.append(ensemble_score)
            
            # åˆ†æ•°åå¤„ç†
            ensemble_scores = np.array(ensemble_scores)
            
            # ä½¿ç”¨æ›´æ¸©å’Œçš„å½’ä¸€åŒ–
            p10 = np.percentile(ensemble_scores, 10)
            p90 = np.percentile(ensemble_scores, 90)
            
            reranked_results = []
            for i, item in enumerate(candidates):
                raw_score = float(ensemble_scores[i])
                
                # å½’ä¸€åŒ–åˆ° [0.1, 0.9] èŒƒå›´
                if p90 > p10:
                    normalized_score = 0.1 + 0.8 * (raw_score - p10) / (p90 - p10)
                    normalized_score = max(0.1, min(0.9, normalized_score))
                else:
                    normalized_score = 0.5
                
                new_item = item.copy()
                new_item['ensemble_rerank_score'] = normalized_score
                new_item['raw_ensemble_score'] = raw_score
                reranked_results.append(new_item)
            
            # æ’åº
            reranked_results.sort(key=lambda x: x['ensemble_rerank_score'], reverse=True)
            
            if k is not None:
                reranked_results = reranked_results[:k]
            
            print(f"âœ… Ensembleé‡æ’å®Œæˆï¼Œè¿”å› {len(reranked_results)} ä¸ªç»“æœ")
            
            if reranked_results:
                scores = [item['ensemble_rerank_score'] for item in reranked_results]
                print(f"  Ensembleåˆ†æ•°èŒƒå›´: {min(scores):.3f} - {max(scores):.3f}")
            
            return reranked_results
            
        except Exception as e:
            print(f"âŒ Ensembleé‡æ’å¤±è´¥: {e}")
            return candidates
    
    def is_available(self):
        return len(self.models) > 0


class SemanticConsistencyChecker:
    """è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self, embedding_model=None):
        self.embedding_model = embedding_model
    
    def check_consistency(self, candidates, query_keywords):
        """æ£€æŸ¥ç»“æœçš„è¯­ä¹‰ä¸€è‡´æ€§"""
        if not self.embedding_model or len(candidates) < 2:
            return candidates
        
        print("ğŸ” è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥...")
        
        try:
            # æå–å€™é€‰æ–‡æœ¬
            candidate_texts = [item['content'] for item in candidates]
            query_text = ' '.join(query_keywords)
            
            # ç”Ÿæˆembeddings
            all_texts = [query_text] + candidate_texts
            embeddings = self.embedding_model.encode(all_texts)
            
            query_embedding = embeddings[0:1]
            candidate_embeddings = embeddings[1:]
            
            # è®¡ç®—ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
            query_similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]
            
            # è®¡ç®—å€™é€‰æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
            candidate_similarities = cosine_similarity(candidate_embeddings)
            
            # ä¸ºæ¯ä¸ªå€™é€‰è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
            consistency_scores = []
            for i in range(len(candidates)):
                # ä¸æŸ¥è¯¢çš„ä¸€è‡´æ€§
                query_consistency = query_similarities[i]
                
                # ä¸å…¶ä»–é«˜åˆ†å€™é€‰çš„ä¸€è‡´æ€§
                other_similarities = candidate_similarities[i]
                # é€‰æ‹©å‰kä¸ªé«˜åˆ†å€™é€‰ï¼ˆæ’é™¤è‡ªå·±ï¼‰
                top_k = min(5, len(candidates))
                top_indices = np.argsort(other_similarities)[-top_k-1:-1]  # æ’é™¤è‡ªå·±
                peer_consistency = np.mean(other_similarities[top_indices]) if len(top_indices) > 0 else 0
                
                # ç»¼åˆä¸€è‡´æ€§åˆ†æ•°
                consistency_score = 0.7 * query_consistency + 0.3 * peer_consistency
                consistency_scores.append(consistency_score)
            
            # æ·»åŠ ä¸€è‡´æ€§åˆ†æ•°åˆ°å€™é€‰ç»“æœ
            for i, candidate in enumerate(candidates):
                candidate['consistency_score'] = consistency_scores[i]
            
            print(f"âœ… è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ")
            return candidates
            
        except Exception as e:
            print(f"âŒ è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return candidates


class DiversityOptimizer:
    """ç»“æœå¤šæ ·æ€§ä¼˜åŒ–å™¨"""
    
    def __init__(self, embedding_model=None, diversity_threshold=0.8):
        self.embedding_model = embedding_model
        self.diversity_threshold = diversity_threshold
    
    def optimize_diversity(self, candidates, target_count=None):
        """ä¼˜åŒ–ç»“æœå¤šæ ·æ€§ï¼Œé¿å…é‡å¤å†…å®¹ - æ”¹è¿›ç‰ˆæœ¬"""
        if not self.embedding_model or len(candidates) <= 1:
            return candidates
        
        print(f"ğŸ¯ ä¼˜åŒ–ç»“æœå¤šæ ·æ€§ (ç›®æ ‡: {target_count or len(candidates)} ä¸ªç»“æœ)...")
        
        try:
            # æå–å€™é€‰æ–‡æœ¬å†…å®¹
            candidate_texts = [item['content'] for item in candidates]
            
            # ç”Ÿæˆembeddings
            embeddings = self.embedding_model.encode(candidate_texts)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = cosine_similarity(embeddings)
            
            # æ”¹è¿›çš„å¤šæ ·æ€§é€‰æ‹©ç®—æ³•
            selected_indices = []
            remaining_indices = list(range(len(candidates)))
            
            # é¦–å…ˆé€‰æ‹©æœ€é«˜åˆ†çš„å€™é€‰
            if candidates:
                best_idx = 0  # å‡è®¾å·²ç»æŒ‰åˆ†æ•°æ’åº
                selected_indices.append(best_idx)
                remaining_indices.remove(best_idx)
            
            # è´ªå¿ƒé€‰æ‹©å‰©ä½™å€™é€‰ï¼Œå¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§
            target_size = target_count if target_count else min(len(candidates), 20)
            
            while len(selected_indices) < target_size and remaining_indices:
                best_candidate_idx = None
                best_score = -1
                
                for idx in remaining_indices:
                    # è®¡ç®—ä¸å·²é€‰æ‹©å€™é€‰çš„æœ€å¤§ç›¸ä¼¼åº¦
                    max_similarity = 0
                    for selected_idx in selected_indices:
                        similarity = similarity_matrix[idx][selected_idx]
                        max_similarity = max(max_similarity, similarity)
                    
                    # å¤šæ ·æ€§åˆ†æ•° (1 - æœ€å¤§ç›¸ä¼¼åº¦)
                    diversity_score = 1 - max_similarity
                    
                    # è·å–å€™é€‰çš„è´¨é‡åˆ†æ•°
                    quality_score = candidates[idx].get('rerank_score', 0)
                    
                    # ç»¼åˆåˆ†æ•°ï¼šå¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§
                    # åŠ¨æ€è°ƒæ•´æƒé‡ï¼šåæœŸæ›´æ³¨é‡å¤šæ ·æ€§
                    diversity_weight = 0.3 + 0.4 * (len(selected_indices) / target_size)
                    quality_weight = 1 - diversity_weight
                    
                    combined_score = quality_weight * quality_score + diversity_weight * diversity_score
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_candidate_idx = idx
                
                if best_candidate_idx is not None:
                    selected_indices.append(best_candidate_idx)
                    remaining_indices.remove(best_candidate_idx)
                else:
                    break
            
            # æ„å»ºå¤šæ ·åŒ–ç»“æœ
            diversified_results = [candidates[i] for i in selected_indices]
            
            # æ·»åŠ å¤šæ ·æ€§ç»Ÿè®¡ä¿¡æ¯
            if len(diversified_results) > 1:
                final_embeddings = embeddings[selected_indices]
                final_similarities = cosine_similarity(final_embeddings)
                
                # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦ï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
                mask = np.ones_like(final_similarities, dtype=bool)
                np.fill_diagonal(mask, False)
                avg_similarity = np.mean(final_similarities[mask])
                
                print(f"âœ… å¤šæ ·æ€§ä¼˜åŒ–å®Œæˆï¼Œå¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
            else:
                print(f"âœ… å¤šæ ·æ€§ä¼˜åŒ–å®Œæˆ")
            
            return diversified_results
            
        except Exception as e:
            print(f"âŒ å¤šæ ·æ€§ä¼˜åŒ–å¤±è´¥: {e}")
            return candidates[:target_count] if target_count else candidates
        
        print(f"ğŸ¯ ä¼˜åŒ–ç»“æœå¤šæ ·æ€§ (é˜ˆå€¼: {self.diversity_threshold})...")
        
        try:
            # ç”Ÿæˆembeddings
            candidate_texts = [item['content'] for item in candidates]
            embeddings = self.embedding_model.encode(candidate_texts)
            
            # è´ªå¿ƒé€‰æ‹©å¤šæ ·åŒ–ç»“æœ
            selected_candidates = []
            selected_embeddings = []
            remaining_candidates = candidates.copy()
            remaining_embeddings = embeddings.copy()
            
            # é¦–å…ˆé€‰æ‹©æœ€é«˜åˆ†çš„å€™é€‰
            if remaining_candidates:
                best_idx = 0  # å‡è®¾å·²ç»æŒ‰åˆ†æ•°æ’åº
                selected_candidates.append(remaining_candidates.pop(best_idx))
                selected_embeddings.append(remaining_embeddings[best_idx])
                remaining_embeddings = np.delete(remaining_embeddings, best_idx, axis=0)
            
            # è´ªå¿ƒé€‰æ‹©å‰©ä½™å€™é€‰
            target_count = target_count or len(candidates)
            while len(selected_candidates) < target_count and remaining_candidates:
                best_candidate_idx = None
                best_score = -1
                
                for i, candidate in enumerate(remaining_candidates):
                    # è®¡ç®—ä¸å·²é€‰æ‹©å€™é€‰çš„æœ€å¤§ç›¸ä¼¼åº¦
                    if selected_embeddings:
                        similarities = cosine_similarity(
                            remaining_embeddings[i:i+1], 
                            np.array(selected_embeddings)
                        )[0]
                        max_similarity = np.max(similarities)
                    else:
                        max_similarity = 0
                    
                    # å¤šæ ·æ€§åˆ†æ•°ï¼šåŸå§‹åˆ†æ•° - ç›¸ä¼¼åº¦æƒ©ç½š
                    original_score = candidate.get('rerank_score', candidate.get('similarity_score', 0))
                    diversity_penalty = max(0, max_similarity - self.diversity_threshold)
                    diversity_score = original_score - diversity_penalty
                    
                    if diversity_score > best_score:
                        best_score = diversity_score
                        best_candidate_idx = i
                
                # é€‰æ‹©æœ€ä½³å€™é€‰
                if best_candidate_idx is not None:
                    selected_candidate = remaining_candidates.pop(best_candidate_idx)
                    selected_embedding = remaining_embeddings[best_candidate_idx]
                    
                    selected_candidate['diversity_score'] = best_score
                    selected_candidates.append(selected_candidate)
                    selected_embeddings.append(selected_embedding)
                    
                    remaining_embeddings = np.delete(remaining_embeddings, best_candidate_idx, axis=0)
                else:
                    break
            
            print(f"âœ… å¤šæ ·æ€§ä¼˜åŒ–å®Œæˆï¼Œé€‰æ‹©äº† {len(selected_candidates)} ä¸ªå¤šæ ·åŒ–ç»“æœ")
            return selected_candidates
            
        except Exception as e:
            print(f"âŒ å¤šæ ·æ€§ä¼˜åŒ–å¤±è´¥: {e}")
            return candidates


class ContextAwareReranker:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’å™¨"""
    
    def __init__(self, keywords=None):
        self.keywords = keywords or []
        
        # å®šä¹‰ä¸Šä¸‹æ–‡çº¿ç´¢
        self.context_clues = {
            'emotional_intensity': [
                'deeply', 'intensely', 'overwhelming', 'burning', 'consuming',
                'terrible', 'horrible', 'dreadful', 'fierce', 'passionate'
            ],
            'character_development': [
                'character', 'personality', 'nature', 'soul', 'heart', 'mind',
                'transformation', 'change', 'becoming', 'turning into'
            ],
            'action_consequence': [
                'result', 'consequence', 'outcome', 'because', 'therefore',
                'led to', 'caused', 'brought about', 'resulted in'
            ],
            'dialogue_speech': [
                'said', 'spoke', 'declared', 'exclaimed', 'whispered',
                'cried', 'shouted', 'uttered', 'proclaimed'
            ]
        }
    
    def rerank_by_context(self, candidates):
        """åŸºäºä¸Šä¸‹æ–‡çº¿ç´¢é‡æ–°æ’åº"""
        if not candidates:
            return candidates
        
        print("ğŸ­ åŸºäºä¸Šä¸‹æ–‡çº¿ç´¢é‡æ’...")
        
        for candidate in candidates:
            content = candidate['content'].lower()
            context_score = 0.0
            
            # è®¡ç®—å„ç§ä¸Šä¸‹æ–‡çº¿ç´¢çš„åˆ†æ•°
            for context_type, clues in self.context_clues.items():
                type_score = 0.0
                for clue in clues:
                    if clue in content:
                        type_score += 1
                
                # å½’ä¸€åŒ–å¹¶åŠ æƒ
                if clues:
                    type_score = type_score / len(clues)
                    
                    # ä¸åŒç±»å‹çš„æƒé‡
                    if context_type == 'emotional_intensity':
                        context_score += type_score * 0.4
                    elif context_type == 'character_development':
                        context_score += type_score * 0.3
                    elif context_type == 'action_consequence':
                        context_score += type_score * 0.2
                    elif context_type == 'dialogue_speech':
                        context_score += type_score * 0.1
            
            # å…³é”®è¯å¯†åº¦å¥–åŠ±
            keyword_density = self._calculate_keyword_density(content, candidate.get('found_keywords', []))
            context_score += keyword_density * 0.2
            
            candidate['context_score'] = context_score
        
        # ç»“åˆåŸå§‹åˆ†æ•°å’Œä¸Šä¸‹æ–‡åˆ†æ•°
        for candidate in candidates:
            original_score = candidate.get('rerank_score', candidate.get('similarity_score', 0))
            context_score = candidate.get('context_score', 0)
            
            # åŠ æƒç»„åˆ
            combined_score = 0.7 * original_score + 0.3 * context_score
            candidate['context_aware_score'] = combined_score
        
        # æŒ‰ç»„åˆåˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x.get('context_aware_score', 0), reverse=True)
        
        print(f"âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’å®Œæˆ")
        return candidates
    
    def _calculate_keyword_density(self, content, found_keywords):
        """è®¡ç®—å…³é”®è¯å¯†åº¦"""
        if not found_keywords:
            return 0.0
        
        words = content.split()
        if not words:
            return 0.0
        
        keyword_count = sum(content.count(kw.lower()) for kw in found_keywords)
        return min(1.0, keyword_count / len(words) * 10)  # å½’ä¸€åŒ–åˆ° [0, 1]


class EnhancedCrossEncoderReranker(CrossEncoderReranker):
    """å¢å¼ºç‰ˆCross-Encoderé‡æ’å™¨"""
    
    def __init__(self, model_name='cross-encoder/ms-marco-MiniLM-L-6-v2', threshold=0.1, 
                 keywords=None, enable_ensemble=False, enable_consistency_check=True,
                 enable_diversity=True, enable_context_aware=True, embedding_model=None):
        super().__init__(model_name, threshold, keywords)
        
        # å¢å¼ºç»„ä»¶
        self.ensemble_reranker = None
        if enable_ensemble:
            self.ensemble_reranker = MultiModelEnsembleReranker(keywords=keywords)
        
        self.consistency_checker = None
        if enable_consistency_check and embedding_model:
            self.consistency_checker = SemanticConsistencyChecker(embedding_model)
        
        self.diversity_optimizer = None
        if enable_diversity and embedding_model:
            self.diversity_optimizer = DiversityOptimizer(embedding_model)
        
        self.context_reranker = None
        if enable_context_aware:
            self.context_reranker = ContextAwareReranker(keywords)
    
    def rerank(self, candidates, k=None):
        """å¢å¼ºç‰ˆé‡æ’åº - ä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿è¿”å›è¶³å¤Ÿçš„ç»“æœ"""
        if not candidates:
            return candidates
        
        print(f"ğŸš€ ä½¿ç”¨å¢å¼ºç‰ˆé‡æ’å™¨å¤„ç† {len(candidates)} ä¸ªå€™é€‰...")
        
        # ç¡®å®šç›®æ ‡ç»“æœæ•°é‡ - ä¸è¿‡äºæ¿€è¿›åœ°å‡å°‘
        target_k = k or len(candidates)
        working_k = max(target_k, min(len(candidates), 20))  # è‡³å°‘ä¿ç•™20ä¸ªæˆ–æ‰€æœ‰å€™é€‰
        
        # é˜¶æ®µ1ï¼šåŸºç¡€é‡æ’
        if self.model:
            candidates = super().rerank(candidates, k=None)  # ä¸åœ¨è¿™é‡Œæˆªæ–­
        
        # é˜¶æ®µ2ï¼šEnsembleé‡æ’ï¼ˆå¯é€‰ï¼‰
        if self.ensemble_reranker and self.ensemble_reranker.is_available():
            candidates = self.ensemble_reranker.rerank(candidates, k=None)
        
        # é˜¶æ®µ3ï¼šè¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆä¸è¿‡æ»¤ç»“æœï¼‰
        if self.consistency_checker:
            candidates = self.consistency_checker.check_consistency(candidates, self.keywords)
        
        # é˜¶æ®µ4ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’ï¼ˆä¸è¿‡æ»¤ç»“æœï¼‰
        if self.context_reranker:
            candidates = self.context_reranker.rerank_by_context(candidates)
        
        # é˜¶æ®µ5ï¼šå¤šæ ·æ€§ä¼˜åŒ–ï¼ˆåªåœ¨ç»“æœè¿‡å¤šæ—¶ä½¿ç”¨ï¼Œä¸”ä¿å®ˆå¤„ç†ï¼‰
        if self.diversity_optimizer and len(candidates) > working_k:
            # åªåœ¨å€™é€‰æ•°é‡æ˜æ˜¾è¶…è¿‡éœ€æ±‚æ—¶æ‰è¿›è¡Œå¤šæ ·æ€§ä¼˜åŒ–
            if len(candidates) > working_k * 1.5:
                candidates = self.diversity_optimizer.optimize_diversity(candidates, working_k)
            else:
                # è½»å¾®çš„å¤šæ ·æ€§å¤„ç†ï¼Œä¸»è¦åŸºäºåˆ†æ•°æ’åº
                candidates = sorted(candidates, 
                                  key=lambda x: x.get('context_aware_score', 
                                                     x.get('ensemble_rerank_score', 
                                                          x.get('rerank_score', 
                                                               x.get('similarity_score', 0)))), 
                                  reverse=True)
        
        # æœ€ç»ˆæˆªæ–­åˆ°ç›®æ ‡æ•°é‡
        if target_k and len(candidates) > target_k:
            candidates = candidates[:target_k]
        
        print(f"âœ… å¢å¼ºç‰ˆé‡æ’å®Œæˆï¼Œæœ€ç»ˆè¿”å› {len(candidates)} ä¸ªç»“æœ")
        
        # æ˜¾ç¤ºæœ€ç»ˆåˆ†æ•°ç»Ÿè®¡
        if candidates:
            final_scores = []
            for candidate in candidates:
                # ä½¿ç”¨æœ€ç»ˆçš„åˆ†æ•°
                if 'context_aware_score' in candidate:
                    final_scores.append(candidate['context_aware_score'])
                elif 'ensemble_rerank_score' in candidate:
                    final_scores.append(candidate['ensemble_rerank_score'])
                elif 'rerank_score' in candidate:
                    final_scores.append(candidate['rerank_score'])
                else:
                    final_scores.append(candidate.get('similarity_score', 0))
            
            if final_scores:
                print(f"  æœ€ç»ˆåˆ†æ•°èŒƒå›´: {min(final_scores):.3f} - {max(final_scores):.3f}")
                
                high_quality = len([s for s in final_scores if s >= 0.7])
                medium_quality = len([s for s in final_scores if 0.4 <= s < 0.7])
                low_quality = len([s for s in final_scores if s < 0.4])
                print(f"  è´¨é‡åˆ†å¸ƒ: é«˜({high_quality}) ä¸­({medium_quality}) ä½({low_quality})")
        
        return candidates


class AdvancedContextAwareReranker:
    """é«˜çº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’å™¨ - é’ˆå¯¹æ–‡å­¦ä½œå“ä¼˜åŒ–"""
    
    def __init__(self, embedding_model=None, keywords=None):
        self.embedding_model = embedding_model
        self.keywords = keywords or []
        
        # æ–‡å­¦ä¸»é¢˜çš„ä¸Šä¸‹æ–‡æ¨¡å¼
        self.literary_contexts = {
            'power_ambition': {
                'keywords': ['power', 'ambition', 'throne', 'crown', 'rule', 'king', 'queen'],
                'positive_indicators': ['seek', 'desire', 'want', 'crave', 'hunger', 'pursue', 'aspire'],
                'emotional_markers': ['burning', 'consuming', 'overwhelming', 'fierce', 'desperate']
            },
            'guilt_conscience': {
                'keywords': ['guilt', 'conscience', 'remorse', 'shame', 'regret'],
                'positive_indicators': ['haunt', 'torment', 'trouble', 'burden', 'weigh'],
                'emotional_markers': ['heavy', 'crushing', 'unbearable', 'terrible', 'horrible']
            },
            'fear_terror': {
                'keywords': ['fear', 'terror', 'dread', 'horror', 'panic'],
                'positive_indicators': ['grip', 'seize', 'overcome', 'fill', 'consume'],
                'emotional_markers': ['paralyzing', 'numbing', 'overwhelming', 'terrible', 'dreadful']
            },
            'love_passion': {
                'keywords': ['love', 'passion', 'affection', 'devotion', 'heart'],
                'positive_indicators': ['burn', 'consume', 'fill', 'overwhelm', 'possess'],
                'emotional_markers': ['deep', 'intense', 'burning', 'passionate', 'overwhelming']
            }
        }
    
    def _analyze_literary_context(self, content, found_keywords):
        """åˆ†ææ–‡å­¦ä¸Šä¸‹æ–‡"""
        content_lower = content.lower()
        max_context_score = 0
        
        for theme, patterns in self.literary_contexts.items():
            theme_score = 0
            
            # æ£€æŸ¥ä¸»é¢˜å…³é”®è¯
            theme_keywords = sum(1 for kw in patterns['keywords'] if kw in content_lower)
            
            # æ£€æŸ¥ç§¯ææŒ‡æ ‡
            positive_count = sum(1 for indicator in patterns['positive_indicators'] if indicator in content_lower)
            
            # æ£€æŸ¥æƒ…æ„Ÿæ ‡è®°
            emotional_count = sum(1 for marker in patterns['emotional_markers'] if marker in content_lower)
            
            # æ£€æŸ¥ç”¨æˆ·æŸ¥è¯¢å…³é”®è¯çš„åŒ¹é…
            query_match = sum(1 for kw in found_keywords if kw.lower() in [k.lower() for k in patterns['keywords']])
            
            if theme_keywords > 0 or query_match > 0:
                theme_score = 0.4 * query_match + 0.3 * positive_count + 0.3 * emotional_count
                max_context_score = max(max_context_score, theme_score)
        
        return min(max_context_score, 1.0)
    
    def rerank(self, candidates, k=None):
        """ä½¿ç”¨é«˜çº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’"""
        if not candidates:
            return candidates
        
        print(f"ğŸ§  ä½¿ç”¨é«˜çº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’å™¨å¤„ç† {len(candidates)} ä¸ªå€™é€‰...")
        
        for candidate in candidates:
            content = candidate['content']
            found_keywords = candidate.get('found_keywords', [])
            
            # åˆ†ææ–‡å­¦ä¸Šä¸‹æ–‡
            context_score = self._analyze_literary_context(content, found_keywords)
            
            # è·å–åŸæœ‰åˆ†æ•°
            original_score = candidate.get('rerank_score', candidate.get('similarity_score', 0))
            
            # è®¡ç®—ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ•°
            candidate['advanced_context_score'] = 0.7 * original_score + 0.3 * context_score
        
        # æŒ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ•°æ’åº
        candidates.sort(key=lambda x: x.get('advanced_context_score', 0), reverse=True)
        
        if k is not None:
            candidates = candidates[:k]
        
        print(f"âœ… é«˜çº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’å®Œæˆï¼Œè¿”å› {len(candidates)} ä¸ªç»“æœ")
        return candidates
    
    def is_available(self):
        return True


def create_enhanced_reranker(method="enhanced_cross_encoder", embedding_model=None, **kwargs):
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå¢å¼ºç‰ˆé‡æ’å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    # é«˜çº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’å™¨ï¼ˆä¸“ä¸ºæ–‡å­¦ä½œå“ä¼˜åŒ–ï¼‰
    if method == "advanced_context_aware":
        return AdvancedContextAwareReranker(
            embedding_model=embedding_model,
            keywords=kwargs.get('keywords'),
        )
    
    # å¢å¼ºCross-Encoderé‡æ’å™¨
    elif method == "enhanced_cross_encoder":
        return EnhancedCrossEncoderReranker(
            embedding_model=embedding_model,
            enable_ensemble=kwargs.get('enable_ensemble', False),
            enable_consistency_check=kwargs.get('enable_consistency_check', True),
            enable_diversity=kwargs.get('enable_diversity', True),
            enable_context_aware=kwargs.get('enable_context_aware', True),
            **{k: v for k, v in kwargs.items() if k not in [
                'enable_ensemble', 'enable_consistency_check', 
                'enable_diversity', 'enable_context_aware'
            ]}
        )
    
    # å¤šæ¨¡å‹ensembleé‡æ’å™¨
    elif method == "ensemble":
        return MultiModelEnsembleReranker(**kwargs)
    
    # å¤šæ ·æ€§ä¼˜åŒ–é‡æ’å™¨
    elif method == "diversity_optimizer":
        return DiversityOptimizer(
            embedding_model=embedding_model,
            diversity_threshold=kwargs.get('diversity_threshold', 0.8)
        )
    
    # è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥å™¨
    elif method == "consistency_checker":
        return SemanticConsistencyChecker(embedding_model=embedding_model)
    
    # åŸºç¡€Cross-Encoderé‡æ’å™¨
    elif method == "cross_encoder":
        return CrossEncoderReranker(**kwargs)
    
    # åŸºç¡€ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’å™¨
    elif method == "context_aware":
        return ContextAwareReranker(keywords=kwargs.get('keywords'))
    
    # ç®€å•é‡æ’å™¨
    elif method == "simple":
        return SimpleReranker(**kwargs)
    
    # é»˜è®¤è¿”å›é«˜çº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡æ’å™¨ï¼ˆæœ€é€‚åˆæ–‡å­¦ä½œå“ï¼‰
    else:
        return AdvancedContextAwareReranker(
            embedding_model=embedding_model,
            keywords=kwargs.get('keywords'),
        )
