#!/usr/bin/env python3
"""
æ–‡æœ¬ä¸»é¢˜åˆ†æå™¨æ¨¡å—
åˆ†ææ–‡æœ¬çš„ä¸»è¦ä¸»é¢˜ã€æƒ…æ„Ÿå€¾å‘ã€æ–‡å­¦å…ƒç´ ç­‰
"""

import numpy as np
import re
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Set
import json
import os


class LiteraryThemeAnalyzer:
    """æ–‡å­¦ä½œå“ä¸»é¢˜åˆ†æå™¨"""
    
    def __init__(self):
        # ä¸»è¦æ–‡å­¦ä¸»é¢˜è¯å…¸
        self.theme_keywords = {
            'ambition': {
                'primary': ['ambition', 'ambitions', 'ambitious', 'aspiration', 'aspire', 'strive'],
                'secondary': ['power', 'throne', 'crown', 'rule', 'control', 'dominion', 'authority', 
                            'command', 'sovereignty', 'empire', 'kingdom', 'reign', 'master'],
                'contextual': ['climb', 'rise', 'achieve', 'obtain', 'gain', 'pursue', 'seek', 'reach']
            },
            'guilt': {
                'primary': ['guilt', 'guilty', 'conscience', 'remorse', 'shame', 'regret'],
                'secondary': ['sin', 'wrong', 'evil', 'wicked', 'corrupt', 'blame', 'fault', 'crime'],
                'contextual': ['repent', 'confess', 'forgive', 'punish', 'atone', 'sorry', 'ashamed']
            },
            'power': {
                'primary': ['power', 'powerful', 'authority', 'control', 'dominion', 'rule'],
                'secondary': ['king', 'queen', 'throne', 'crown', 'royal', 'noble', 'lord', 'master'],
                'contextual': ['command', 'order', 'obey', 'submit', 'govern', 'lead', 'conquer']
            },
            'love': {
                'primary': ['love', 'beloved', 'lover', 'passion', 'romantic', 'romance'],
                'secondary': ['heart', 'soul', 'dear', 'sweet', 'tender', 'gentle', 'kiss', 'embrace'],
                'contextual': ['devotion', 'affection', 'cherish', 'adore', 'treasure', 'faithful']
            },
            'death': {
                'primary': ['death', 'dead', 'die', 'dying', 'killed', 'murder', 'murdered'],
                'secondary': ['grave', 'tomb', 'corpse', 'ghost', 'spirit', 'soul', 'afterlife'],
                'contextual': ['mortal', 'mortality', 'fatal', 'doom', 'fate', 'destiny', 'end']
            },
            'betrayal': {
                'primary': ['betray', 'betrayal', 'treachery', 'traitor', 'deceive', 'deceit'],
                'secondary': ['lie', 'false', 'unfaithful', 'disloyal', 'cheat', 'trick'],
                'contextual': ['trust', 'faith', 'loyalty', 'honest', 'truth', 'promise', 'oath']
            },
            'fear': {
                'primary': ['fear', 'afraid', 'terror', 'dread', 'horror', 'panic', 'frighten'],
                'secondary': ['scary', 'frightening', 'terrifying', 'horrible', 'dreadful', 'awful'],
                'contextual': ['anxiety', 'worry', 'concern', 'nervous', 'tremble', 'shake', 'flee']
            },
            'madness': {
                'primary': ['mad', 'madness', 'insane', 'insanity', 'crazy', 'lunacy', 'deranged'],
                'secondary': ['mental', 'mind', 'brain', 'thoughts', 'reason', 'sanity', 'rational'],
                'contextual': ['confusion', 'delusion', 'hallucination', 'obsession', 'mania']
            },
            'fate': {
                'primary': ['fate', 'destiny', 'fortune', 'doom', 'predetermined', 'inevitable'],
                'secondary': ['future', 'prophecy', 'foretell', 'predict', 'omen', 'portent'],
                'contextual': ['chance', 'luck', 'fortune', 'providence', 'divine', 'gods']
            },
            'honor': {
                'primary': ['honor', 'honour', 'honorable', 'noble', 'dignity', 'integrity'],
                'secondary': ['respect', 'reputation', 'glory', 'fame', 'renown', 'prestige'],
                'contextual': ['virtue', 'moral', 'ethics', 'principle', 'value', 'character']
            }
        }
        
        # æƒ…æ„Ÿå¼ºåº¦è¯æ±‡
        self.emotion_intensity = {
            'high': ['overwhelming', 'consuming', 'burning', 'fierce', 'intense', 'powerful',
                    'desperate', 'passionate', 'furious', 'terrifying', 'devastating'],
            'medium': ['strong', 'deep', 'significant', 'notable', 'considerable', 'substantial'],
            'low': ['mild', 'slight', 'gentle', 'soft', 'quiet', 'calm', 'peaceful']
        }
        
        # æ–‡å­¦è®¾å¤‡è¯æ±‡
        self.literary_devices = {
            'metaphor': ['like', 'as', 'metaphor', 'symbol', 'represent', 'signify'],
            'irony': ['irony', 'ironic', 'paradox', 'contradictory', 'opposite'],
            'foreshadowing': ['hint', 'suggest', 'forebode', 'omen', 'portent', 'sign'],
            'imagery': ['see', 'sight', 'vision', 'image', 'picture', 'appear', 'look']
        }
        
        # è§’è‰²ç±»å‹è¯†åˆ«
        self.character_types = {
            'protagonist': ['hero', 'main', 'protagonist', 'central', 'lead'],
            'antagonist': ['villain', 'enemy', 'opponent', 'antagonist', 'evil'],
            'tragic_hero': ['tragic', 'flawed', 'downfall', 'hubris', 'pride'],
            'innocent': ['innocent', 'pure', 'naive', 'young', 'child']
        }
    
    def analyze_text_themes(self, text_chunks: List[Dict], progress_callback=None) -> Dict:
        """åˆ†ææ–‡æœ¬çš„ä¸»è¦ä¸»é¢˜"""
        def update_progress(msg):
            if progress_callback:
                progress_callback(msg)
            else:
                print(msg)
                
        update_progress("ğŸ” å¼€å§‹æ–‡æœ¬ä¸»é¢˜åˆ†æ...")
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å†…å®¹
        full_text = ' '.join([chunk.get('content', '') for chunk in text_chunks]).lower()
        
        # ä¸»é¢˜åˆ†æ
        update_progress("  åˆ†æä¸»é¢˜åˆ†å¸ƒ...")
        theme_scores = self._analyze_themes(full_text)
        
        # æƒ…æ„Ÿåˆ†æ
        update_progress("  åˆ†ææƒ…æ„Ÿå€¾å‘...")
        emotion_analysis = self._analyze_emotions(full_text)
        
        # æ–‡å­¦è®¾å¤‡åˆ†æ
        update_progress("  åˆ†ææ–‡å­¦æ‰‹æ³•...")
        literary_analysis = self._analyze_literary_devices(full_text)
        
        # è§’è‰²åˆ†æ
        update_progress("  åˆ†æè§’è‰²ç‰¹å¾...")
        character_analysis = self._analyze_characters(text_chunks)
        
        # ç»“æ„åˆ†æ
        update_progress("  åˆ†ææ–‡æœ¬ç»“æ„...")
        structure_analysis = self._analyze_text_structure(text_chunks)
        
        # ç”Ÿæˆä¸»é¢˜æ‘˜è¦
        update_progress("  ç”Ÿæˆä¸»é¢˜æ‘˜è¦...")
        theme_summary = self._generate_theme_summary(theme_scores, emotion_analysis)
        
        analysis_result = {
            'themes': theme_scores,
            'emotions': emotion_analysis,
            'literary_devices': literary_analysis,
            'characters': character_analysis,
            'structure': structure_analysis,
            'summary': theme_summary,
            'text_statistics': {
                'total_chunks': len(text_chunks),
                'total_words': len(full_text.split()),
                'unique_words': len(set(full_text.split())),
                'average_chunk_length': np.mean([len(chunk.get('content', '').split()) for chunk in text_chunks])
            }
        }
        
        update_progress(f"âœ… ä¸»é¢˜åˆ†æå®Œæˆï¼Œè¯†åˆ«å‡º {len([t for t, s in theme_scores.items() if s > 0.1])} ä¸ªä¸»è¦ä¸»é¢˜")
        
        return analysis_result
    
    def _analyze_themes(self, text: str) -> Dict[str, float]:
        """åˆ†ææ–‡æœ¬ä¸­çš„ä¸»é¢˜å¼ºåº¦ - ä¿®å¤ç‰ˆæœ¬"""
        theme_scores = {}
        words = text.split()
        total_words = len(words)
        
        if total_words == 0:
            return {theme: 0.0 for theme in self.theme_keywords.keys()}
        
        # é¦–å…ˆè®¡ç®—æ‰€æœ‰ä¸»é¢˜çš„åŸå§‹åˆ†æ•°
        raw_scores = {}
        for theme, keywords in self.theme_keywords.items():
            score = 0.0
            
            # è®¡ç®—ä¸»è¦å…³é”®è¯åˆ†æ•°
            primary_count = sum(text.count(word) for word in keywords['primary'])
            score += primary_count * 3.0  # å¢åŠ ä¸»è¦å…³é”®è¯æƒé‡
            
            # è®¡ç®—æ¬¡è¦å…³é”®è¯åˆ†æ•°
            secondary_count = sum(text.count(word) for word in keywords['secondary'])
            score += secondary_count * 1.5  # å¢åŠ æ¬¡è¦å…³é”®è¯æƒé‡
            
            # è®¡ç®—ä¸Šä¸‹æ–‡å…³é”®è¯åˆ†æ•°
            contextual_count = sum(text.count(word) for word in keywords['contextual'])
            score += contextual_count * 0.8
            
            raw_scores[theme] = score
        
        # è®¡ç®—æ€»åˆ†æ•°å¹¶è¿›è¡Œç›¸å¯¹å½’ä¸€åŒ–
        total_raw_score = sum(raw_scores.values())
        
        if total_raw_score > 0:
            # ä½¿ç”¨ç›¸å¯¹å½’ä¸€åŒ–ï¼šæ¯ä¸ªä¸»é¢˜çš„åˆ†æ•°æ˜¯å…¶åœ¨æ€»åˆ†ä¸­çš„æ¯”ä¾‹
            for theme, raw_score in raw_scores.items():
                if raw_score > 0:
                    # è®¡ç®—ç›¸å¯¹å¼ºåº¦
                    relative_strength = raw_score / total_raw_score
                    # åº”ç”¨å¹³æ»‘å‡½æ•°é¿å…è¿‡åº¦é›†ä¸­
                    smoothed_score = relative_strength ** 0.7  # å¹³æ–¹æ ¹å¹³æ»‘
                    theme_scores[theme] = min(smoothed_score, 1.0)
                else:
                    theme_scores[theme] = 0.0
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ä¸»é¢˜å…³é”®è¯ï¼Œæ‰€æœ‰ä¸»é¢˜å¾—åˆ†ä¸º0
            theme_scores = {theme: 0.0 for theme in self.theme_keywords.keys()}
        
        # é‡æ–°å½’ä¸€åŒ–ç¡®ä¿åˆ†æ•°åˆç†åˆ†å¸ƒ
        max_score = max(theme_scores.values()) if theme_scores.values() else 0
        if max_score > 0:
            normalization_factor = 1.0 / max_score
            for theme in theme_scores:
                theme_scores[theme] = min(theme_scores[theme] * normalization_factor, 1.0)
        
        return theme_scores
    
    def _analyze_emotions(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå¼ºåº¦å’Œç±»å‹"""
        emotion_analysis = {
            'intensity_distribution': {},
            'dominant_emotions': [],
            'emotional_complexity': 0.0
        }
        
        # åˆ†ææƒ…æ„Ÿå¼ºåº¦åˆ†å¸ƒ
        for intensity, words in self.emotion_intensity.items():
            count = sum(text.count(word) for word in words)
            emotion_analysis['intensity_distribution'][intensity] = count
        
        # ç¡®å®šä¸»å¯¼æƒ…æ„Ÿ
        total_intensity = sum(emotion_analysis['intensity_distribution'].values())
        if total_intensity > 0:
            for intensity, count in emotion_analysis['intensity_distribution'].items():
                if count / total_intensity > 0.3:
                    emotion_analysis['dominant_emotions'].append(intensity)
        
        # è®¡ç®—æƒ…æ„Ÿå¤æ‚åº¦
        non_zero_intensities = sum(1 for count in emotion_analysis['intensity_distribution'].values() if count > 0)
        emotion_analysis['emotional_complexity'] = non_zero_intensities / len(self.emotion_intensity)
        
        return emotion_analysis
    
    def _analyze_literary_devices(self, text: str) -> Dict[str, int]:
        """åˆ†ææ–‡å­¦è®¾å¤‡çš„ä½¿ç”¨"""
        device_counts = {}
        
        for device, indicators in self.literary_devices.items():
            count = sum(text.count(indicator) for indicator in indicators)
            device_counts[device] = count
        
        return device_counts
    
    def _analyze_characters(self, text_chunks: List[Dict]) -> Dict:
        """åˆ†æè§’è‰²ç±»å‹å’Œç‰¹å¾"""
        character_analysis = {
            'character_types': {},
            'character_mentions': {},
            'dialogue_density': 0.0
        }
        
        # åˆå¹¶æ–‡æœ¬
        full_text = ' '.join([chunk.get('content', '') for chunk in text_chunks]).lower()
        
        # åˆ†æè§’è‰²ç±»å‹
        for char_type, indicators in self.character_types.items():
            count = sum(full_text.count(indicator) for indicator in indicators)
            character_analysis['character_types'][char_type] = count
        
        # åˆ†æå¯¹è¯å¯†åº¦
        total_chunks = len(text_chunks)
        dialogue_chunks = sum(1 for chunk in text_chunks 
                             if '"' in chunk.get('content', '') or "'" in chunk.get('content', ''))
        
        if total_chunks > 0:
            character_analysis['dialogue_density'] = dialogue_chunks / total_chunks
        
        # æå–å¯èƒ½çš„è§’è‰²åç§°ï¼ˆå¤§å†™è¯æ±‡ï¼Œæ’é™¤å¸¸è§è¯ï¼‰
        common_words = {'THE', 'AND', 'OR', 'BUT', 'IF', 'THEN', 'WHEN', 'WHERE', 'WHY', 'HOW', 
                       'WHAT', 'WHO', 'WHICH', 'THAT', 'THIS', 'THESE', 'THOSE', 'I', 'YOU', 
                       'HE', 'SHE', 'IT', 'WE', 'THEY', 'MY', 'YOUR', 'HIS', 'HER', 'ITS', 
                       'OUR', 'THEIR', 'ME', 'HIM', 'HER', 'US', 'THEM'}
        
        possible_names = []
        for chunk in text_chunks:
            words = chunk.get('content', '').split()
            for word in words:
                clean_word = re.sub(r'[^\w]', '', word)
                if (clean_word.isupper() and len(clean_word) > 2 and 
                    clean_word not in common_words):
                    possible_names.append(clean_word)
        
        name_counts = Counter(possible_names)
        # åªä¿ç•™å‡ºç°å¤šæ¬¡çš„åç§°
        character_analysis['character_mentions'] = {
            name: count for name, count in name_counts.items() if count >= 3
        }
        
        return character_analysis
    
    def _analyze_text_structure(self, text_chunks: List[Dict]) -> Dict:
        """åˆ†ææ–‡æœ¬ç»“æ„ç‰¹å¾"""
        structure_analysis = {
            'chunk_length_distribution': {},
            'content_density_by_page': {},
            'thematic_progression': {}
        }
        
        # åˆ†ææ–‡æœ¬å—é•¿åº¦åˆ†å¸ƒ
        lengths = [len(chunk.get('content', '').split()) for chunk in text_chunks]
        if lengths:
            structure_analysis['chunk_length_distribution'] = {
                'mean': np.mean(lengths),
                'std': np.std(lengths),
                'min': min(lengths),
                'max': max(lengths),
                'median': np.median(lengths)
            }
        
        # æŒ‰é¡µé¢åˆ†æå†…å®¹å¯†åº¦
        page_content = defaultdict(list)
        for chunk in text_chunks:
            page_num = chunk.get('page_num', 1)
            content_length = len(chunk.get('content', '').split())
            page_content[page_num].append(content_length)
        
        for page, lengths in page_content.items():
            structure_analysis['content_density_by_page'][page] = {
                'total_words': sum(lengths),
                'chunks_count': len(lengths),
                'avg_chunk_length': np.mean(lengths) if lengths else 0
            }
        
        return structure_analysis
    
    def _generate_theme_summary(self, theme_scores: Dict[str, float], 
                               emotion_analysis: Dict) -> Dict[str, str]:
        """ç”Ÿæˆä¸»é¢˜åˆ†ææ‘˜è¦"""
        # æ‰¾å‡ºä¸»è¦ä¸»é¢˜ï¼ˆåˆ†æ•° > 0.1ï¼‰
        major_themes = [(theme, score) for theme, score in theme_scores.items() if score > 0.1]
        major_themes.sort(key=lambda x: x[1], reverse=True)
        
        # æ‰¾å‡ºæ¬¡è¦ä¸»é¢˜ï¼ˆåˆ†æ•° 0.05-0.1ï¼‰
        minor_themes = [(theme, score) for theme, score in theme_scores.items() 
                       if 0.05 <= score <= 0.1]
        minor_themes.sort(key=lambda x: x[1], reverse=True)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = {
            'primary_themes': [theme for theme, _ in major_themes[:3]],
            'secondary_themes': [theme for theme, _ in minor_themes[:3]],
            'theme_complexity': len(major_themes) + len(minor_themes),
            'dominant_emotional_tone': emotion_analysis.get('dominant_emotions', []),
            'thematic_richness': 'high' if len(major_themes) >= 4 else 
                               'medium' if len(major_themes) >= 2 else 'low'
        }
        
        return summary
    
    def get_theme_based_keywords(self, analysis_result: Dict) -> List[str]:
        """åŸºäºä¸»é¢˜åˆ†æç»“æœç”Ÿæˆæ‰©å±•å…³é”®è¯"""
        expanded_keywords = []
        
        # åŸºäºä¸»è¦ä¸»é¢˜æ·»åŠ å…³é”®è¯
        primary_themes = analysis_result['summary']['primary_themes']
        
        for theme in primary_themes:
            if theme in self.theme_keywords:
                # æ·»åŠ ä¸»è¦å’Œæ¬¡è¦å…³é”®è¯
                expanded_keywords.extend(self.theme_keywords[theme]['primary'][:3])
                expanded_keywords.extend(self.theme_keywords[theme]['secondary'][:5])
        
        # åŸºäºè§’è‰²åˆ†ææ·»åŠ å…³é”®è¯
        char_mentions = analysis_result['characters']['character_mentions']
        # æ·»åŠ æœ€å¸¸æåˆ°çš„è§’è‰²å
        top_characters = sorted(char_mentions.items(), key=lambda x: x[1], reverse=True)[:3]
        expanded_keywords.extend([name.lower() for name, _ in top_characters])
        
        # å»é‡å¹¶è¿”å›
        return list(set(expanded_keywords))
    
    def save_analysis_result(self, analysis_result: Dict, output_path: str):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
            def convert_for_json(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, dict):
                    return {key: convert_for_json(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_for_json(item) for item in obj]
                else:
                    return obj
            
            json_compatible_result = convert_for_json(analysis_result)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(json_compatible_result, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“ ä¸»é¢˜åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
    
    def print_analysis_summary(self, analysis_result: Dict):
        """æ‰“å°åˆ†æç»“æœæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“š æ–‡æœ¬ä¸»é¢˜åˆ†ææ‘˜è¦")
        print("="*60)
        
        # åŸºæœ¬ç»Ÿè®¡
        stats = analysis_result['text_statistics']
        print(f"ğŸ“Š æ–‡æœ¬ç»Ÿè®¡:")
        print(f"  æ€»æ–‡æœ¬å—æ•°: {stats['total_chunks']}")
        print(f"  æ€»è¯æ•°: {stats['total_words']}")
        print(f"  å¹³å‡å—é•¿åº¦: {stats['average_chunk_length']:.1f} è¯")
        
        # ä¸»è¦ä¸»é¢˜
        summary = analysis_result['summary']
        print(f"\nğŸ­ ä¸»è¦ä¸»é¢˜:")
        for i, theme in enumerate(summary['primary_themes'], 1):
            score = analysis_result['themes'][theme]
            print(f"  {i}. {theme.title()} (å¼ºåº¦: {score:.3f})")
        
        # æ¬¡è¦ä¸»é¢˜
        if summary['secondary_themes']:
            print(f"\nğŸ“ æ¬¡è¦ä¸»é¢˜:")
            for theme in summary['secondary_themes']:
                score = analysis_result['themes'][theme]
                print(f"  - {theme.title()} (å¼ºåº¦: {score:.3f})")
        
        # æƒ…æ„Ÿåˆ†æ
        emotions = analysis_result['emotions']
        print(f"\nğŸ’­ æƒ…æ„Ÿç‰¹å¾:")
        print(f"  ä¸»å¯¼æƒ…æ„Ÿå¼ºåº¦: {', '.join(emotions['dominant_emotions']) if emotions['dominant_emotions'] else 'å¹³è¡¡'}")
        print(f"  æƒ…æ„Ÿå¤æ‚åº¦: {emotions['emotional_complexity']:.2f}")
        
        # è§’è‰²åˆ†æ
        characters = analysis_result['characters']
        if characters['character_mentions']:
            print(f"\nğŸ‘¥ ä¸»è¦è§’è‰²:")
            for name, count in list(characters['character_mentions'].items())[:5]:
                print(f"  - {name}: {count} æ¬¡æåŠ")
        
        print(f"  å¯¹è¯å¯†åº¦: {characters['dialogue_density']:.2%}")
        
        # æ–‡å­¦ç‰¹å¾
        print(f"\nâœ¨ ä¸»é¢˜ä¸°å¯Œåº¦: {summary['thematic_richness'].upper()}")
        
        print("="*60)


def create_theme_analyzer() -> LiteraryThemeAnalyzer:
    """åˆ›å»ºä¸»é¢˜åˆ†æå™¨å®ä¾‹"""
    return LiteraryThemeAnalyzer()
