#!/usr/bin/env python3
"""
å…³é”®è¯æ‰©å±•æ¨¡å— - ä¼˜åŒ–ç‰ˆ
è´Ÿè´£ä½¿ç”¨WordNetç­‰å·¥å…·æ‰©å±•å…³é”®è¯ï¼Œæé«˜å¬å›ç‡
æ”¯æŒè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤ã€è¯æ€§æ ‡æ³¨ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‰©å±•ç­‰åŠŸèƒ½
"""

import nltk
import numpy as np
from collections import Counter, defaultdict
from sklearn.metrics.pairwise import cosine_similarity

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# å¯¼å…¥æˆ‘ä»¬æ”¹è¿›çš„æ¨¡å‹åŠ è½½å‡½æ•°
try:
    from .retriever import load_sentence_transformer_model, SimpleVocabModel
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from modules.retriever import load_sentence_transformer_model, SimpleVocabModel
    except ImportError:
        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šå®šä¹‰ç®€å•çš„åŠ è½½å‡½æ•°
        def load_sentence_transformer_model(model_path):
            try:
                return SentenceTransformer(model_path)
            except:
                return None
        
        class SimpleVocabModel:
            def encode(self, texts):
                import numpy as np
                if isinstance(texts, str):
                    texts = [texts]
                return np.random.random((len(texts), 384))  # ç®€å•çš„éšæœºå‘é‡


class KeywordExpander:
    """å…³é”®è¯æ‰©å±•å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, method='wordnet', semantic_model=None, document_type='literary'):
        self.method = method
        self.document_type = document_type
        self.semantic_model = semantic_model
        self.use_simple_model = False
        
        if method == 'wordnet':
            self._ensure_wordnet_data()
        
        # åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if semantic_model is None and SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                # å°è¯•ä½¿ç”¨æ”¹è¿›çš„æ¨¡å‹åŠ è½½æœºåˆ¶
                print("ğŸ” æ­£åœ¨åŠ è½½å…³é”®è¯æ‰©å±•è¯­ä¹‰æ¨¡å‹...")
                self.semantic_model = load_sentence_transformer_model('sentence-transformers/all-MiniLM-L6-v2')
                
                if self.semantic_model is None:
                    print("ğŸ”„ è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€å•è¯æ±‡æ¨¡å‹")
                    self.semantic_model = SimpleVocabModel()
                    self.use_simple_model = True
                else:
                    print("âœ“ è¯­ä¹‰æ¨¡å‹å·²åŠ è½½ï¼Œå°†ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤")
                    
            except Exception as e:
                print(f"âš ï¸ è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ ä½¿ç”¨ç®€å•è¯æ±‡æ¨¡å‹")
                self.semantic_model = SimpleVocabModel()
                self.use_simple_model = True
        
        # é¢†åŸŸç‰¹å®šè¯å…¸ï¼ˆæ–‡å­¦ä½œå“ï¼‰
        self.literary_domain_expansions = {
            # æƒ…æ„Ÿå’Œå¿ƒç†çŠ¶æ€
            'ambition': ['aspiration', 'ambitiousness', 'drive', 'determination', 'pursuit', 'yearning', 'hunger', 'thirst'],
            'guilt': ['shame', 'remorse', 'regret', 'conscience', 'culpability', 'self-reproach', 'penitence', 'contrition'],
            'jealousy': ['envy', 'resentment', 'suspicion', 'possessiveness', 'covetousness', 'spite', 'malice'],
            'love': ['affection', 'devotion', 'passion', 'romance', 'adoration', 'fondness', 'tenderness', 'ardor'],
            'fear': ['terror', 'dread', 'anxiety', 'trepidation', 'apprehension', 'horror', 'fright', 'alarm'],
            'courage': ['bravery', 'valor', 'heroism', 'boldness', 'fortitude', 'daring', 'gallantry', 'mettle'],
            'anger': ['wrath', 'fury', 'rage', 'ire', 'indignation', 'outrage', 'choler', 'spleen'],
            
            # é“å¾·å’Œå“²å­¦æ¦‚å¿µ
            'evil': ['wickedness', 'malice', 'corruption', 'darkness', 'depravity', 'iniquity', 'sin', 'vice'],
            'good': ['virtue', 'righteousness', 'nobility', 'honor', 'integrity', 'purity', 'goodness', 'merit'],
            'justice': ['fairness', 'equity', 'righteousness', 'lawfulness', 'retribution', 'vindication'],
            'honor': ['dignity', 'integrity', 'nobility', 'respect', 'prestige', 'reputation', 'esteem', 'glory'],
            'betrayal': ['treachery', 'disloyalty', 'deception', 'treason', 'backstabbing', 'perfidy', 'duplicity'],
            
            # æƒåŠ›å’Œæ”¿æ²»
            'power': ['authority', 'control', 'dominance', 'influence', 'might', 'sway', 'command', 'sovereignty'],
            'king': ['monarch', 'sovereign', 'ruler', 'emperor', 'majesty', 'crown', 'throne', 'royalty'],
            'throne': ['crown', 'scepter', 'sovereignty', 'kingship', 'dominion', 'rule', 'monarchy'],
            'crown': ['diadem', 'coronet', 'sovereignty', 'royalty', 'majesty', 'kingship'],
            
            # å‘½è¿å’Œé¢„è¨€
            'destiny': ['fate', 'fortune', 'prophecy', 'predestination', 'doom', 'lot', 'kismet'],
            'prophecy': ['prediction', 'foretelling', 'divination', 'oracle', 'prognostication', 'revelation'],
            'fate': ['destiny', 'doom', 'fortune', 'lot', 'providence', 'kismet', 'predestination'],
            
            # è¶…è‡ªç„¶å’Œé­”æ³•
            'magic': ['sorcery', 'wizardry', 'enchantment', 'spellcraft', 'witchcraft', 'conjuring'],
            'ghost': ['spirit', 'specter', 'phantom', 'apparition', 'wraith', 'shade', 'banshee'],
            'witch': ['sorceress', 'enchantress', 'hag', 'crone', 'sibyl', 'pythoness'],
            
            # ç”Ÿæ­»ä¸»é¢˜
            'death': ['mortality', 'demise', 'passing', 'perishing', 'end', 'expiration', 'doom', 'grave'],
            'murder': ['killing', 'assassination', 'slaying', 'homicide', 'bloodshed', 'execution'],
            'blood': ['gore', 'crimson', 'scarlet', 'bloodshed', 'slaughter', 'carnage'],
            
            # å¤ä»‡å’ŒæŠ¥åº”
            'revenge': ['vengeance', 'retribution', 'retaliation', 'payback', 'reprisal', 'vindication'],
            'punishment': ['retribution', 'penalty', 'chastisement', 'discipline', 'correction', 'nemesis'],
        }
    
    def _ensure_wordnet_data(self):
        """ç¡®ä¿WordNetæ•°æ®å·²ä¸‹è½½"""
        try:
            from nltk.corpus import wordnet
            # æµ‹è¯•æ˜¯å¦å¯ä»¥è®¿é—®WordNet
            wordnet.synsets('test')
        except LookupError:
            print("æ­£åœ¨ä¸‹è½½WordNetæ•°æ®...")
            try:
                nltk.download('wordnet', quiet=True)
                nltk.download('omw-1.4', quiet=True)
            except Exception as e:
                print(f"âš ï¸ WordNetæ•°æ®ä¸‹è½½å¤±è´¥: {e}")
    
    def expand_keywords(self, keywords, max_synonyms_per_word=3, max_related_per_word=2, 
                       semantic_threshold=0.6, use_hierarchical=True):
        """
        æ‰©å±•å…³é”®è¯åˆ—è¡¨ - ä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒè¯­ä¹‰è¿‡æ»¤å’Œå±‚æ¬¡åŒ–æ‰©å±•
        
        Args:
            keywords: åŸå§‹å…³é”®è¯åˆ—è¡¨
            max_synonyms_per_word: æ¯ä¸ªè¯æœ€å¤šæ·»åŠ çš„åŒä¹‰è¯æ•°é‡
            max_related_per_word: æ¯ä¸ªè¯æœ€å¤šæ·»åŠ çš„ç›¸å…³è¯æ•°é‡
            semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
            use_hierarchical: æ˜¯å¦ä½¿ç”¨å±‚æ¬¡åŒ–æ‰©å±•
        
        Returns:
            æ‰©å±•åçš„å…³é”®è¯å­—å…¸ï¼ŒåŒ…å«æƒé‡ä¿¡æ¯
        """
        try:
            from nltk.corpus import wordnet
            
            # ä½¿ç”¨å­—å…¸å­˜å‚¨å…³é”®è¯åŠå…¶æƒé‡
            expanded_keywords = {}
            
            # åŸå§‹å…³é”®è¯æƒé‡ä¸º1.0
            for keyword in keywords:
                expanded_keywords[keyword] = 1.0
            
            original_count = len(keywords)
            
            for keyword in keywords:
                print(f"  æ­£åœ¨æ‰©å±•å…³é”®è¯: '{keyword}'")
                
                # 1. ä½¿ç”¨é¢†åŸŸç‰¹å®šè¯å…¸ï¼ˆé«˜æƒé‡ï¼‰
                domain_expansions = self._get_domain_expansions(keyword)
                for exp_word in domain_expansions:
                    if exp_word not in expanded_keywords:
                        expanded_keywords[exp_word] = 0.9
                
                # 2. è·å–é«˜è´¨é‡åŒä¹‰è¯ï¼ˆä¸­é«˜æƒé‡ï¼‰
                synonyms = self._get_semantic_filtered_synonyms(
                    keyword, max_synonyms_per_word, semantic_threshold
                )
                for syn in synonyms:
                    if syn not in expanded_keywords:
                        expanded_keywords[syn] = 0.8
                
                # 3. è·å–ç›¸å…³è¯ï¼ˆä¸­ç­‰æƒé‡ï¼‰
                if use_hierarchical:
                    related_words = self._get_hierarchical_related_words(
                        keyword, max_related_per_word
                    )
                    for rel_word, weight in related_words.items():
                        if rel_word not in expanded_keywords:
                            expanded_keywords[rel_word] = weight * 0.6
                else:
                    related = self._get_related_words(keyword, max_related_per_word)
                    for rel in related:
                        if rel not in expanded_keywords:
                            expanded_keywords[rel] = 0.6
                
                # 4. è·å–è¯å½¢å˜åŒ–ï¼ˆè¾ƒä½æƒé‡ï¼‰
                morphological_variants = self._get_morphological_variants(keyword)
                for variant in morphological_variants:
                    if variant not in expanded_keywords:
                        expanded_keywords[variant] = 0.7
                     # è¿‡æ»¤æ‰è¿‡äºé€šç”¨çš„è¯æ±‡
            filtered_expanded = self._filter_generic_words(
                expanded_keywords, keywords
            )
            
            print(f"  åŸå§‹å…³é”®è¯: {original_count} ä¸ª")
            print(f"  æ‰©å±•åå…³é”®è¯: {len(filtered_expanded)} ä¸ª")
            print(f"  æ–°å¢å…³é”®è¯: {len(filtered_expanded) - original_count} ä¸ª")
            
            # æ˜¾ç¤ºæ‰©å±•ç»“æœæ‘˜è¦
            self._print_expansion_summary(filtered_expanded, keywords)
            
            return filtered_expanded
            
        except ImportError:
            print("âš ï¸ NLTK WordNetæœªå®‰è£…ï¼Œè·³è¿‡å…³é”®è¯æ‰©å±•")
            return {kw: 1.0 for kw in keywords}
        except Exception as e:
            print(f"âš ï¸ å…³é”®è¯æ‰©å±•æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return {kw: 1.0 for kw in keywords}

    def _get_domain_expansions(self, keyword):
        """è·å–é¢†åŸŸç‰¹å®šçš„è¯æ±‡æ‰©å±•"""
        keyword_lower = keyword.lower()
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é¢†åŸŸè¯å…¸ä¸­
        if keyword_lower in self.literary_domain_expansions:
            return self.literary_domain_expansions[keyword_lower][:3]  # é™åˆ¶æ•°é‡
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []

    def _get_semantic_filtered_synonyms(self, word, max_count, threshold):
        """è·å–è¯­ä¹‰è¿‡æ»¤åçš„åŒä¹‰è¯"""
        synonyms = self._get_high_quality_synonyms(word, max_count * 2)  # è·å–æ›´å¤šå€™é€‰
        
        if not self.semantic_model or not synonyms:
            return synonyms[:max_count]
        
        try:
            # ä½¿ç”¨è¯­ä¹‰æ¨¡å‹è¿‡æ»¤
            word_embedding = self.semantic_model.encode([word])
            synonym_embeddings = self.semantic_model.encode(synonyms)
            
            similarities = cosine_similarity(word_embedding, synonym_embeddings)[0]
            
            # è¿‡æ»¤ä½ç›¸ä¼¼åº¦çš„è¯æ±‡
            filtered_synonyms = []
            for i, sim in enumerate(similarities):
                if sim >= threshold:
                    filtered_synonyms.append(synonyms[i])
            
            return filtered_synonyms[:max_count]
        
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰è¿‡æ»¤å¤±è´¥: {e}")
            return synonyms[:max_count]

    def _get_hierarchical_related_words(self, word, max_count):
        """è·å–å±‚æ¬¡åŒ–ç›¸å…³è¯æ±‡"""
        try:
            from nltk.corpus import wordnet
            related_words = {}
            
            synsets = wordnet.synsets(word)
            if not synsets:
                return related_words
            
            for synset in synsets[:2]:  # é™åˆ¶åŒä¹‰è¯é›†æ•°é‡
                # ä¸Šä½è¯
                for hypernym in synset.hypernyms()[:1]:
                    for lemma in hypernym.lemmas()[:1]:
                        name = lemma.name().replace('_', ' ')
                        if name != word and len(name) > 2:
                            related_words[name] = 0.7
                
                # ä¸‹ä½è¯
                for hyponym in synset.hyponyms()[:1]:
                    for lemma in hyponym.lemmas()[:1]:
                        name = lemma.name().replace('_', ' ')
                        if name != word and len(name) > 2:
                            related_words[name] = 0.6
                
                # åŒçº§è¯ï¼ˆé€šè¿‡å…±åŒä¸Šä½è¯ï¼‰
                for hypernym in synset.hypernyms()[:1]:
                    for sibling in hypernym.hyponyms()[:2]:
                        if sibling != synset:
                            for lemma in sibling.lemmas()[:1]:
                                name = lemma.name().replace('_', ' ')
                                if name != word and len(name) > 2:
                                    related_words[name] = 0.5
            
            # æŒ‰æƒé‡æ’åºå¹¶é™åˆ¶æ•°é‡
            sorted_related = sorted(related_words.items(), key=lambda x: x[1], reverse=True)
            return dict(sorted_related[:max_count])
        
        except Exception as e:
            print(f"âš ï¸ å±‚æ¬¡åŒ–æ‰©å±•å¤±è´¥: {e}")
            return {}

    def _filter_generic_words(self, expanded_keywords, original_keywords):
        """è¿‡æ»¤æ‰è¿‡äºé€šç”¨çš„æ‰©å±•è¯æ±‡ï¼Œæ”¯æŒæƒé‡ä¿¡æ¯"""
        # è¿‡æ»¤æ‰çš„é€šç”¨è¯æ±‡
        generic_words = {
            'desire', 'want', 'need', 'wish', 'hope', 'like', 'love', 'feel', 'think', 'believe',
            'try', 'attempt', 'effort', 'work', 'do', 'make', 'get', 'have', 'take', 'give',
            'see', 'look', 'find', 'show', 'tell', 'say', 'speak', 'talk', 'ask', 'answer',
            'come', 'go', 'move', 'turn', 'walk', 'run', 'stand', 'sit', 'live', 'stay',
            'happen', 'occur', 'appear', 'seem', 'become', 'remain', 'continue', 'start',
            'begin', 'end', 'stop', 'finish', 'complete', 'change', 'improve', 'develop',
            'create', 'build', 'form', 'produce', 'provide', 'offer', 'serve', 'help',
            'use', 'apply', 'employ', 'utilize', 'spend', 'pay', 'cost', 'buy', 'sell',
            'win', 'lose', 'gain', 'earn', 'achieve', 'reach', 'meet', 'join', 'leave',
            'open', 'close', 'cut', 'break', 'fix', 'repair', 'clean', 'wash', 'wear',
            'carry', 'hold', 'keep', 'put', 'place', 'set', 'lay', 'throw', 'catch',
            'play', 'enjoy', 'fun', 'game', 'sport', 'music', 'song', 'book', 'read',
            'write', 'draw', 'paint', 'color', 'picture', 'photo', 'image', 'watch',
            'listen', 'hear', 'sound', 'voice', 'word', 'language', 'speak', 'talk'
        }
        
        # Check if expanded_keywords is a dict (with weights) or a set/list
        if isinstance(expanded_keywords, dict):
            filtered = {}
            for keyword, weight in expanded_keywords.items():
                # ä¿ç•™åŸå§‹å…³é”®è¯
                if keyword in original_keywords:
                    filtered[keyword] = weight
                # ä¿ç•™éé€šç”¨è¯æ±‡
                elif keyword not in generic_words:
                    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯è¿‡äºç®€å•çš„è¯æ±‡
                    if len(keyword) > 3 and not keyword.isdigit():
                        filtered[keyword] = weight
            return filtered
        else:
            # ä¿ç•™åŸå§‹å…³é”®è¯
            filtered = set(original_keywords)
            for keyword in expanded_keywords:
                # ä¿ç•™åŸå§‹å…³é”®è¯
                if keyword in original_keywords:
                    filtered.add(keyword)
                # ä¿ç•™éé€šç”¨è¯æ±‡
                elif keyword not in generic_words:
                    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯è¿‡äºç®€å•çš„è¯æ±‡
                    if len(keyword) > 3 and not keyword.isdigit():
                        filtered.add(keyword)
            return filtered

    def _print_expansion_summary(self, expanded_keywords, original_keywords):
        """æ‰“å°æ‰©å±•ç»“æœæ‘˜è¦"""
        new_keywords = [k for k in expanded_keywords.keys() if k not in original_keywords]
        if new_keywords:
            print(f"  æ–°å¢å…³é”®è¯: {new_keywords[:5]}")  # åªæ˜¾ç¤ºå‰5ä¸ª

    def _get_morphological_variants(self, word):
        """è·å–è¯å½¢å˜ä½“ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        variants = set()
        
        # åŸºæœ¬çš„åç¼€å˜æ¢
        suffixes_to_try = ['s', 'es', 'ed', 'ing', 'er', 'est', 'ly', 'ness', 'ment', 'tion', 'able', 'ful']
        suffixes_to_remove = ['s', 'es', 'ed', 'ing', 'er', 'est', 'ly', 'ness', 'ment', 'tion', 'able', 'ful']
        
        # å°è¯•æ·»åŠ åç¼€
        for suffix in suffixes_to_try:
            variants.add(word + suffix)
        
        # å°è¯•å»é™¤åç¼€
        for suffix in suffixes_to_remove:
            if word.endswith(suffix) and len(word) > len(suffix) + 2:
                variants.add(word[:-len(suffix)])
        
        # å»é™¤åŸè¯æœ¬èº«
        variants.discard(word)
        
        # é™åˆ¶è¿”å›æ•°é‡
        return list(variants)[:3]
    
    def _get_synonyms(self, word, max_count=5):
        """è·å–åŒä¹‰è¯"""
        try:
            from nltk.corpus import wordnet
            
            synonyms = set()
            for syn in wordnet.synsets(word):
                for lemma in syn.lemmas():
                    synonym = lemma.name().lower().replace('_', ' ')
                    if len(synonym) > 2 and synonym != word:
                        synonyms.add(synonym)
                        if len(synonyms) >= max_count:
                            break
                if len(synonyms) >= max_count:
                    break
            
            return list(synonyms)
            
        except Exception:
            return []
    
    def _get_related_words(self, word, max_count=3):
        """è·å–ç›¸å…³è¯ï¼ˆä¸Šä½è¯å’Œä¸‹ä½è¯ï¼‰"""
        try:
            from nltk.corpus import wordnet
            
            related = set()
            
            for syn in wordnet.synsets(word):
                # ä¸Šä½è¯ (hypernyms)
                for hyper in syn.hypernyms():
                    for lemma in hyper.lemmas():
                        related_word = lemma.name().lower().replace('_', ' ')
                        if len(related_word) > 2 and related_word != word:
                            related.add(related_word)
                            if len(related) >= max_count:
                                break
                    if len(related) >= max_count:
                        break
                
                # ä¸‹ä½è¯ (hyponyms) - é™åˆ¶æ•°é‡é¿å…è¿‡åº¦æ‰©å±•
                if len(related) < max_count:
                    for hypo in syn.hyponyms()[:2]:  # åªå–å‰2ä¸ª
                        for lemma in hypo.lemmas():
                            related_word = lemma.name().lower().replace('_', ' ')
                            if len(related_word) > 2 and related_word != word:
                                related.add(related_word)
                                if len(related) >= max_count:
                                    break
                        if len(related) >= max_count:
                            break
                
                if len(related) >= max_count:
                    break
            
            return list(related)
            
        except Exception:
            return []
    
    def _get_high_quality_synonyms(self, word, max_count=3):
        """è·å–é«˜è´¨é‡åŒä¹‰è¯ï¼Œé¿å…è¿‡äºé€šç”¨çš„è¯æ±‡"""
        try:
            from nltk.corpus import wordnet
            
            synonyms = set()
            
            # è¿‡æ»¤æ‰çš„é€šç”¨è¯æ±‡
            generic_words = {'desire', 'want', 'need', 'like', 'love', 'get', 'have', 'make', 'do', 'go', 'come', 'take', 'give', 'see', 'know', 'think', 'feel', 'say', 'tell', 'find', 'use', 'work', 'try', 'ask', 'seem', 'turn', 'move', 'live', 'believe', 'hold', 'bring', 'happen', 'write', 'provide', 'sit', 'stand', 'lose', 'pay', 'meet', 'include', 'continue', 'set', 'learn', 'change', 'lead', 'understand', 'watch', 'follow', 'stop', 'create', 'speak', 'read', 'allow', 'add', 'spend', 'grow', 'open', 'walk', 'win', 'offer', 'remember', 'consider', 'appear', 'buy', 'wait', 'serve', 'die', 'send', 'expect', 'build', 'stay', 'fall', 'cut', 'reach', 'kill', 'remain'}
            
            for syn in wordnet.synsets(word):
                for lemma in syn.lemmas():
                    synonym = lemma.name().lower().replace('_', ' ')
                    if (len(synonym) > 2 and 
                        synonym != word and 
                        synonym not in generic_words and
                        not synonym.endswith('ing') and  # é¿å…åŠ¨åè¯
                        not synonym.endswith('ed')):     # é¿å…è¿‡å»åˆ†è¯
                        synonyms.add(synonym)
                        if len(synonyms) >= max_count:
                            break
                if len(synonyms) >= max_count:
                    break
            
            return list(synonyms)
            
        except Exception:
            return []
    
    def process_keywords(self, keywords_input):
        """
        å¤„ç†å…³é”®è¯è¾“å…¥ï¼Œæ”¯æŒé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
        
        Args:
            keywords_input: å…³é”®è¯è¾“å…¥ï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        
        Returns:
            å¤„ç†åçš„å…³é”®è¯åˆ—è¡¨
        """
        if isinstance(keywords_input, str):
            # æ”¯æŒé€—å·åˆ†éš”çš„å…³é”®è¯
            keywords = [k.strip().lower() for k in keywords_input.split(',')]
        elif isinstance(keywords_input, list):
            keywords = [k.strip().lower() for k in keywords_input]
        else:
            keywords = [str(keywords_input).strip().lower()]
        
        return [k for k in keywords if k]  # è¿‡æ»¤ç©ºå…³é”®è¯
