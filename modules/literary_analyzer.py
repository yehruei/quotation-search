#!/usr/bin/env python3
"""
æ–‡å­¦åˆ†ææ¨¡å—
æä¾›çœŸå®çš„æ–‡å­¦è¦ç´ åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. äººç‰©å…±ç°åˆ†æ
2. ä¸»é¢˜é¢‘ç‡ç»Ÿè®¡
3. æƒ…æ„Ÿå€¾å‘åˆ†æ
4. åœºæ™¯åˆ†å¸ƒåˆ†æ
"""

import re
import nltk
from collections import Counter, defaultdict
from itertools import combinations
import pandas as pd
import numpy as np
from datetime import datetime
import os

# ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('averaged_perceptron_tagger')

from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tag import pos_tag


class LiteraryAnalyzer:
    """æ–‡å­¦åˆ†æå™¨"""
    
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        
        # é¢„å®šä¹‰çš„äººç‰©åç§°æ¨¡å¼ï¼ˆå¯æ‰©å±•ï¼‰
        self.character_patterns = [
            # èå£«æ¯”äºšã€Šéº¦å…‹ç™½ã€‹ä¸­çš„ä¸»è¦äººç‰©
            r'\b(Macbeth|Lady Macbeth|Duncan|Banquo|Malcolm|Macduff|Ross|Lennox|Angus|Menteith|Caithness)\b',
            # ã€Šå“ˆåˆ©æ³¢ç‰¹ã€‹ä¸­çš„ä¸»è¦äººç‰©
            r'\b(Harry|Potter|Hermione|Granger|Ron|Weasley|Dumbledore|Snape|Voldemort|Hagrid|McGonagall)\b',
            # é€šç”¨äººåæ¨¡å¼
            r'\b[A-Z][a-z]+ [A-Z][a-z]+\b',  # å§“åæ ¼å¼
            r'\b[A-Z][a-z]{2,}\b'  # ä¸“æœ‰åè¯ï¼ˆå¯èƒ½æ˜¯äººåï¼‰
        ]
        
        # é¢„å®šä¹‰çš„ä¸»é¢˜å…³é”®è¯
        self.theme_keywords = {
            'power': ['power', 'authority', 'control', 'dominance', 'rule', 'command', 'throne', 'crown', 'king', 'queen'],
            'betrayal': ['betray', 'betrayal', 'treachery', 'deceive', 'deception', 'backstab', 'unfaithful'],
            'guilt': ['guilt', 'guilty', 'conscience', 'remorse', 'regret', 'shame', 'sin', 'forgive'],
            'death': ['death', 'die', 'kill', 'murder', 'slay', 'blood', 'corpse', 'grave', 'funeral'],
            'love': ['love', 'beloved', 'affection', 'romance', 'heart', 'dear', 'sweet', 'kiss'],
            'fear': ['fear', 'afraid', 'terror', 'horror', 'dread', 'panic', 'frighten', 'scared'],
            'ambition': ['ambition', 'ambitious', 'desire', 'want', 'aspire', 'dream', 'goal', 'achieve'],
            'revenge': ['revenge', 'vengeance', 'avenge', 'retaliate', 'payback', 'retribution'],
            'honor': ['honor', 'honour', 'noble', 'dignity', 'respect', 'virtue', 'glory'],
            'madness': ['mad', 'madness', 'insane', 'crazy', 'lunatic', 'mental', 'mind', 'sanity']
        }
        
        # æƒ…æ„Ÿè¯æ±‡
        self.emotion_keywords = {
            'positive': ['joy', 'happy', 'glad', 'pleased', 'delight', 'love', 'hope', 'peace', 'triumph'],
            'negative': ['sad', 'anger', 'hate', 'fear', 'despair', 'sorrow', 'pain', 'suffer', 'grief'],
            'neutral': ['think', 'consider', 'believe', 'seem', 'appear', 'perhaps', 'maybe']
        }
    
    def extract_characters(self, text_chunks):
        """æå–æ–‡æœ¬ä¸­çš„äººç‰©åç§° - ä¼˜åŒ–ç‰ˆæœ¬"""
        characters = Counter()
        character_positions = defaultdict(list)  # è®°å½•æ¯ä¸ªäººç‰©å‡ºç°çš„ä½ç½®
        
        # æ‰©å±•åœç”¨è¯ä»¥è¿‡æ»¤æ— å…³è¯æ±‡
        extended_stop_words = self.stop_words.union({
            'the', 'and', 'his', 'her', 'him', 'she', 'he', 'they', 'them', 'their',
            'was', 'were', 'been', 'being', 'have', 'has', 'had', 'will', 'would',
            'could', 'should', 'may', 'might', 'can', 'must', 'shall', 'this', 'that',
            'these', 'those', 'who', 'what', 'where', 'when', 'why', 'how', 'which',
            'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',
            'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',
            'very', 'just', 'now', 'even', 'also', 'however', 'although', 'because'
        })
        
        # é™åˆ¶å¤„ç†çš„æ–‡æœ¬å—æ•°é‡
        max_chunks = 100
        processed_chunks = text_chunks[:max_chunks] if len(text_chunks) > max_chunks else text_chunks
        
        for chunk_idx, chunk in enumerate(processed_chunks):
            try:
                content = chunk.get('content', '')
                chunk_id = chunk.get('id', f'chunk_{chunk_idx}')
                
                # é™åˆ¶å•ä¸ªæ–‡æœ¬å—çš„é•¿åº¦ä»¥æé«˜æ€§èƒ½
                if len(content) > 5000:  # é™åˆ¶ä¸º5000å­—ç¬¦
                    content = content[:5000]
                
                # ä½¿ç”¨å¤šç§æ¨¡å¼æå–äººç‰©
                for pattern in self.character_patterns:
                    try:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        for match in matches:
                            # æ ‡å‡†åŒ–äººç‰©åç§°
                            char_name = match.strip().title()
                            
                            # æ›´ä¸¥æ ¼çš„è¿‡æ»¤æ¡ä»¶
                            if (len(char_name) > 2 and 
                                char_name.lower() not in extended_stop_words and
                                not char_name.lower() in ['act', 'scene', 'enter', 'exit', 'exeunt'] and
                                not re.match(r'^[IVX]+$', char_name) and  # æ’é™¤ç½—é©¬æ•°å­—
                                not char_name.isdigit() and  # æ’é™¤çº¯æ•°å­—
                                any(c.isalpha() for c in char_name)):  # å¿…é¡»åŒ…å«å­—æ¯
                                
                                characters[char_name] += 1
                                character_positions[char_name].append({
                                    'chunk_id': chunk_id,
                                    'chunk_index': chunk_idx,
                                    'page_num': chunk.get('page_num', 1)
                                })
                    except Exception as pattern_error:
                        print(f"âš ï¸ æ¨¡å¼åŒ¹é…é”™è¯¯: {pattern_error}")
                        continue
                        
            except Exception as chunk_error:
                print(f"âš ï¸ å¤„ç†æ–‡æœ¬å— {chunk_idx} æ—¶å‡ºé”™: {chunk_error}")
                continue
        
        return dict(characters), dict(character_positions)
    
    def analyze_character_cooccurrence(self, text_chunks, min_occurrences=2):
        """åˆ†æäººç‰©å…±ç°å…³ç³» - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # é™åˆ¶å¤„ç†çš„æ–‡æœ¬å—æ•°é‡ä»¥é¿å…æ€§èƒ½é—®é¢˜
            max_chunks = 100  # é™åˆ¶æœ€å¤šå¤„ç†100ä¸ªæ–‡æœ¬å—
            if len(text_chunks) > max_chunks:
                print(f"âš ï¸ æ–‡æœ¬å—æ•°é‡è¿‡å¤š({len(text_chunks)})ï¼Œå°†åªå¤„ç†å‰{max_chunks}ä¸ªä»¥æé«˜æ€§èƒ½")
                text_chunks = text_chunks[:max_chunks]
            
            characters, character_positions = self.extract_characters(text_chunks)
            
            # è¿‡æ»¤å‡ºç°é¢‘ç‡ä½çš„äººç‰©
            frequent_characters = {name: count for name, count in characters.items() 
                                 if count >= min_occurrences}
            
            if len(frequent_characters) < 2:
                return {}, frequent_characters, {}
            
            # åªä¿ç•™æœ€å¸¸è§çš„äººç‰©ä»¥æé«˜æ€§èƒ½
            max_characters = 10
            if len(frequent_characters) > max_characters:
                print(f"âš ï¸ äººç‰©æ•°é‡è¿‡å¤š({len(frequent_characters)})ï¼Œå°†åªåˆ†æå‰{max_characters}ä¸ªæœ€å¸¸è§äººç‰©")
                frequent_characters = dict(sorted(frequent_characters.items(), 
                                                key=lambda x: x[1], reverse=True)[:max_characters])
            
            # åˆ†æå…±ç°å…³ç³»
            cooccurrence_matrix = defaultdict(int)
            cooccurrence_details = defaultdict(list)
            
            for chunk_idx, chunk in enumerate(text_chunks):
                content = chunk.get('content', '').lower()
                chunk_characters = []
                
                # æ‰¾å‡ºåœ¨å½“å‰chunkä¸­å‡ºç°çš„äººç‰©
                for char_name in frequent_characters.keys():
                    if char_name.lower() in content:
                        chunk_characters.append(char_name)
                
                # è®°å½•å…±ç°å…³ç³»
                if len(chunk_characters) >= 2:
                    for char1, char2 in combinations(chunk_characters, 2):
                        # ç¡®ä¿ä¸€è‡´çš„æ’åº
                        if char1 > char2:
                            char1, char2 = char2, char1
                        
                        cooccurrence_matrix[(char1, char2)] += 1
                        # é™åˆ¶è¯¦æƒ…è®°å½•æ•°é‡
                        if len(cooccurrence_details[(char1, char2)]) < 5:  # æœ€å¤šè®°å½•5ä¸ªç¤ºä¾‹
                            cooccurrence_details[(char1, char2)].append({
                                'chunk_id': chunk.get('id', f'chunk_{chunk_idx}'),
                                'page_num': chunk.get('page_num', 1),
                                'content_preview': content[:100] + '...' if len(content) > 100 else content
                            })
            
            # å°†tupleé”®è½¬æ¢ä¸ºå­—ç¬¦ä¸²é”®ä»¥ä¾¿JSONåºåˆ—åŒ–
            cooccurrence_dict = {f"{k[0]}-{k[1]}": v for k, v in cooccurrence_matrix.items()}
            cooccurrence_details_dict = {f"{k[0]}-{k[1]}": v for k, v in cooccurrence_details.items()}
            
            return cooccurrence_dict, frequent_characters, cooccurrence_details_dict
            
        except Exception as e:
            print(f"âš ï¸ äººç‰©å…±ç°åˆ†æå‡ºé”™: {e}")
            return {}, {}, {}
    
    def analyze_themes(self, text_chunks):
        """åˆ†æä¸»é¢˜é¢‘ç‡"""
        theme_counts = defaultdict(int)
        theme_details = defaultdict(list)
        
        for chunk_idx, chunk in enumerate(text_chunks):
            content = chunk.get('content', '').lower()
            words = word_tokenize(content)
            
            for theme, keywords in self.theme_keywords.items():
                theme_score = 0
                found_keywords = []
                
                for keyword in keywords:
                    keyword_count = content.count(keyword.lower())
                    if keyword_count > 0:
                        theme_score += keyword_count
                        found_keywords.extend([keyword] * keyword_count)
                
                if theme_score > 0:
                    theme_counts[theme] += theme_score
                    theme_details[theme].append({
                        'chunk_id': chunk.get('id', f'chunk_{chunk_idx}'),
                        'page_num': chunk.get('page_num', 1),
                        'score': theme_score,
                        'keywords_found': found_keywords,
                        'content_preview': chunk.get('content', '')[:200] + '...' 
                            if len(chunk.get('content', '')) > 200 else chunk.get('content', '')
                    })
        
        return dict(theme_counts), dict(theme_details)
    
    def analyze_emotions(self, text_chunks):
        """åˆ†ææƒ…æ„Ÿå€¾å‘"""
        emotion_counts = defaultdict(int)
        emotion_details = defaultdict(list)
        
        for chunk_idx, chunk in enumerate(text_chunks):
            content = chunk.get('content', '').lower()
            
            for emotion, keywords in self.emotion_keywords.items():
                emotion_score = 0
                found_keywords = []
                
                for keyword in keywords:
                    keyword_count = content.count(keyword.lower())
                    if keyword_count > 0:
                        emotion_score += keyword_count
                        found_keywords.extend([keyword] * keyword_count)
                
                if emotion_score > 0:
                    emotion_counts[emotion] += emotion_score
                    emotion_details[emotion].append({
                        'chunk_id': chunk.get('id', f'chunk_{chunk_idx}'),
                        'page_num': chunk.get('page_num', 1),
                        'score': emotion_score,
                        'keywords_found': found_keywords
                    })
        
        return dict(emotion_counts), dict(emotion_details)
    
    def analyze_narrative_structure(self, text_chunks):
        """åˆ†æå™äº‹ç»“æ„ï¼ˆåœºæ™¯åˆ†å¸ƒï¼‰"""
        # è¯†åˆ«åœºæ™¯æ ‡è®°
        scene_markers = [
            r'act\s+[ivx]+',
            r'scene\s+[ivx]+',
            r'chapter\s+\d+',
            r'part\s+[ivx]+',
            r'enter\s+\w+',
            r'exit\s+\w+'
        ]
        
        scenes = []
        current_scene = None
        scene_content_length = []
        
        for chunk_idx, chunk in enumerate(text_chunks):
            content = chunk.get('content', '').lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«åœºæ™¯æ ‡è®°
            for pattern in scene_markers:
                if re.search(pattern, content, re.IGNORECASE):
                    if current_scene is not None:
                        # ç»“æŸå½“å‰åœºæ™¯
                        scenes.append(current_scene)
                    
                    # å¼€å§‹æ–°åœºæ™¯
                    current_scene = {
                        'start_chunk': chunk_idx,
                        'start_page': chunk.get('page_num', 1),
                        'marker': re.search(pattern, content, re.IGNORECASE).group(),
                        'length': 0
                    }
                    break
            
            # å¦‚æœåœ¨åœºæ™¯ä¸­ï¼Œç´¯è®¡é•¿åº¦
            if current_scene is not None:
                current_scene['length'] += len(chunk.get('content', ''))
        
        # æ·»åŠ æœ€åä¸€ä¸ªåœºæ™¯
        if current_scene is not None:
            scenes.append(current_scene)
        
        return scenes
    
    def generate_comprehensive_analysis(self, text_chunks, output_dir='outputs', min_occurrences=2, progress_callback=None):
        """ç”Ÿæˆç»¼åˆæ–‡å­¦åˆ†ææŠ¥å‘Š"""
        def update_progress(msg):
            try:
                if progress_callback:
                    progress_callback(msg)
                else:
                    print(msg)
            except Exception as e:
                print(f"è¿›åº¦æ›´æ–°å¤±è´¥: {e}")
                print(msg)
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        analysis_results = {
            'characters': {'frequency': {}, 'cooccurrence': {}, 'cooccurrence_details': {}},
            'themes': {'frequency': {}, 'details': {}},
            'emotions': {'frequency': {}, 'details': {}},
            'narrative': {'scenes': []},
            'metadata': {
                'total_chunks': len(text_chunks),
                'analysis_time': datetime.now().isoformat(),
                'errors': []
            }
        }
        
        try:
            update_progress("ğŸ” å¼€å§‹æ–‡å­¦åˆ†æ...")
            
            # 1. äººç‰©åˆ†æ
            try:
                update_progress("ğŸ“ åˆ†æäººç‰©å…³ç³»...")
                
                # æ·»åŠ æ€§èƒ½ç›‘æ§
                import time
                start_time = time.time()
                
                # é™åˆ¶æ–‡æœ¬å—æ•°é‡ä»¥æé«˜æ€§èƒ½
                max_chunks_for_analysis = 50  # æœ€å¤šåˆ†æ50ä¸ªæ–‡æœ¬å—
                chunks_to_analyze = text_chunks[:max_chunks_for_analysis] if len(text_chunks) > max_chunks_for_analysis else text_chunks
                
                if len(text_chunks) > max_chunks_for_analysis:
                    update_progress(f"ğŸ“Š ä¸ºæé«˜æ€§èƒ½ï¼Œå°†åˆ†æå‰{max_chunks_for_analysis}ä¸ªæ–‡æœ¬å—ï¼ˆå…±{len(text_chunks)}ä¸ªï¼‰")
                
                cooccurrence, characters, cooccurrence_details = self.analyze_character_cooccurrence(chunks_to_analyze, min_occurrences=min_occurrences)
                
                elapsed_time = time.time() - start_time
                update_progress(f"âœ“ äººç‰©åˆ†æå®Œæˆ - å‘ç° {len(characters)} ä¸ªäººç‰© (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                
                analysis_results['characters'] = {
                    'frequency': characters,
                    'cooccurrence': cooccurrence,
                    'cooccurrence_details': cooccurrence_details
                }
                
            except Exception as e:
                error_msg = f"äººç‰©åˆ†æå¤±è´¥: {str(e)}"
                analysis_results['metadata']['errors'].append(error_msg)
                update_progress(f"âš ï¸ {error_msg}")
            
            # 2. ä¸»é¢˜åˆ†æ
            try:
                update_progress("ğŸ­ åˆ†æä¸»é¢˜åˆ†å¸ƒ...")
                themes, theme_details = self.analyze_themes(text_chunks)
                analysis_results['themes'] = {
                    'frequency': themes,
                    'details': theme_details
                }
                update_progress(f"âœ“ ä¸»é¢˜åˆ†æå®Œæˆ - å‘ç° {len(themes)} ä¸ªä¸»é¢˜")
            except Exception as e:
                error_msg = f"ä¸»é¢˜åˆ†æå¤±è´¥: {str(e)}"
                analysis_results['metadata']['errors'].append(error_msg)
                update_progress(f"âš ï¸ {error_msg}")
            
            # 3. æƒ…æ„Ÿåˆ†æ
            try:
                update_progress("ğŸ˜Š åˆ†ææƒ…æ„Ÿå€¾å‘...")
                emotions, emotion_details = self.analyze_emotions(text_chunks)
                analysis_results['emotions'] = {
                    'frequency': emotions,
                    'details': emotion_details
                }
                update_progress(f"âœ“ æƒ…æ„Ÿåˆ†æå®Œæˆ - æ£€æµ‹åˆ° {len(emotions)} ç§æƒ…æ„Ÿ")
            except Exception as e:
                error_msg = f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}"
                analysis_results['metadata']['errors'].append(error_msg)
                update_progress(f"âš ï¸ {error_msg}")
            
            # 4. å™äº‹ç»“æ„åˆ†æ
            try:
                update_progress("ğŸ“– åˆ†æå™äº‹ç»“æ„...")
                scenes = self.analyze_narrative_structure(text_chunks)
                analysis_results['narrative']['scenes'] = scenes
                update_progress(f"âœ“ å™äº‹ç»“æ„åˆ†æå®Œæˆ - å‘ç° {len(scenes)} ä¸ªåœºæ™¯")
            except Exception as e:
                error_msg = f"å™äº‹ç»“æ„åˆ†æå¤±è´¥: {str(e)}"
                analysis_results['metadata']['errors'].append(error_msg)
                update_progress(f"âš ï¸ {error_msg}")
            
            # 5. ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
            try:
                update_progress("ğŸ’¾ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                report_path = os.path.join(output_dir, f'literary_analysis_{timestamp}.txt')
                
                report_content = self._generate_text_report(analysis_results)
                
                os.makedirs(output_dir, exist_ok=True)
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(report_content)
                
                update_progress(f"âœ… æ–‡å­¦åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
                
            except Exception as e:
                error_msg = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
                analysis_results['metadata']['errors'].append(error_msg)
                update_progress(f"âš ï¸ {error_msg}")
        
        except Exception as e:
            error_msg = f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}"
            analysis_results['metadata']['errors'].append(error_msg)
            update_progress(f"âŒ {error_msg}")
        
        # ç¡®ä¿è¿”å›æœ‰æ•ˆç»“æœ
        return analysis_results
    
    def _generate_text_report(self, analysis_results):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("æ–‡å­¦åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"åˆ†ææ—¶é—´: {analysis_results['metadata']['analysis_time']}")
        report.append(f"åˆ†ææ–‡æœ¬å—æ•°é‡: {analysis_results['metadata']['total_chunks']}")
        report.append("")
        
        # äººç‰©åˆ†æ
        report.append("ğŸ“š äººç‰©åˆ†æ")
        report.append("-" * 30)
        characters = analysis_results['characters']['frequency']
        if characters:
            report.append("ä¸»è¦äººç‰©å‡ºç°é¢‘ç‡:")
            for char, count in sorted(characters.items(), key=lambda x: x[1], reverse=True)[:10]:
                report.append(f"  {char}: {count} æ¬¡")
            report.append("")
            
            cooccurrence = analysis_results['characters']['cooccurrence']
            if cooccurrence:
                report.append("äººç‰©å…±ç°å…³ç³» (å‰10å¯¹):")
                for char_pair, count in sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)[:10]:
                    char1, char2 = char_pair.split('-', 1)  # æ‹†åˆ†å­—ç¬¦ä¸²é”®
                    report.append(f"  {char1} â†” {char2}: å…±åŒå‡ºç° {count} æ¬¡")
        else:
            report.append("  æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„äººç‰©åç§°")
        report.append("")
        
        # ä¸»é¢˜åˆ†æ
        report.append("ğŸ­ ä¸»é¢˜åˆ†æ")
        report.append("-" * 30)
        themes = analysis_results['themes']['frequency']
        if themes:
            report.append("ä¸»è¦ä¸»é¢˜é¢‘ç‡:")
            for theme, count in sorted(themes.items(), key=lambda x: x[1], reverse=True):
                report.append(f"  {theme.title()}: {count} æ¬¡æåŠ")
        else:
            report.append("  æœªæ£€æµ‹åˆ°é¢„å®šä¹‰ä¸»é¢˜")
        report.append("")
        
        # æƒ…æ„Ÿåˆ†æ
        report.append("ğŸ˜Š æƒ…æ„Ÿå€¾å‘åˆ†æ")
        report.append("-" * 30)
        emotions = analysis_results['emotions']['frequency']
        if emotions:
            total_emotions = sum(emotions.values())
            for emotion, count in sorted(emotions.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_emotions) * 100 if total_emotions > 0 else 0
                report.append(f"  {emotion.title()}: {count} æ¬¡ ({percentage:.1f}%)")
        else:
            report.append("  æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æƒ…æ„Ÿå€¾å‘")
        report.append("")
        
        # å™äº‹ç»“æ„
        report.append("ğŸ“– å™äº‹ç»“æ„åˆ†æ")
        report.append("-" * 30)
        scenes = analysis_results['narrative']['scenes']
        if scenes:
            report.append(f"æ£€æµ‹åˆ° {len(scenes)} ä¸ªåœºæ™¯/ç« èŠ‚:")
            for i, scene in enumerate(scenes, 1):
                report.append(f"  {i}. {scene['marker']} (é¡µé¢ {scene['start_page']}, é•¿åº¦: {scene['length']} å­—ç¬¦)")
        else:
            report.append("  æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„åœºæ™¯åˆ†éš”")
        
        return "\n".join(report)
    
    def create_character_network_data(self, cooccurrence, characters):
        """åˆ›å»ºäººç‰©å…³ç³»ç½‘ç»œæ•°æ®ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
        nodes = []
        edges = []
        
        # åˆ›å»ºèŠ‚ç‚¹
        for char, freq in characters.items():
            nodes.append({
                'id': char,
                'label': char,
                'size': min(freq * 5, 50),  # é™åˆ¶èŠ‚ç‚¹å¤§å°
                'frequency': freq
            })
        
        # åˆ›å»ºè¾¹
        for char_pair, weight in cooccurrence.items():
            char1, char2 = char_pair.split('-', 1)  # æ‹†åˆ†å­—ç¬¦ä¸²é”®
            edges.append({
                'source': char1,
                'target': char2,
                'weight': weight,
                'label': f"{weight} æ¬¡å…±ç°"
            })
        
        return {'nodes': nodes, 'edges': edges}


# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
literary_analyzer = LiteraryAnalyzer()
