#!/usr/bin/env python3
"""
å¯è§†åŒ–æ¨¡å— - å¢å¼ºç‰ˆ
æ”¯æŒé¡µç åˆ†å¸ƒã€ä¸»é¢˜çƒ­åŠ›å›¾ç­‰é«˜çº§å¯è§†åŒ–åŠŸèƒ½
"""

import matplotlib.pyplot as plt
import numpy as np
from collections import Counter
from datetime import datetime
import os

# å°è¯•å¯¼å…¥seabornï¼Œå¦‚æœæ²¡æœ‰å°±è·³è¿‡
try:
    import seaborn as sns
    sns.set_style("whitegrid")
    SEABORN_AVAILABLE = True
except ImportError:
    SEABORN_AVAILABLE = False
# å°è¯•å¯¼å…¥è¯äº‘åº“
try:
    from wordcloud import WordCloud
    WORDCLOUD_AVAILABLE = True
except ImportError:
    WORDCLOUD_AVAILABLE = False


class AdvancedVisualizer:
    """é«˜çº§å¯è§†åŒ–å™¨ - æ”¯æŒé¡µç åˆ†å¸ƒå’Œä¸»é¢˜åˆ†æ"""
    
    def __init__(self, output_dir='outputs', language='zh'):
        self.output_dir = output_dir
        self.language = language  # 'zh' for Chinese, 'en' for English
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # è®¾ç½®å­—ä½“ä»¥æ”¯æŒä¸­æ–‡æ˜¾ç¤º
        self._setup_chinese_fonts()
        
        # åˆå§‹åŒ–æ ‡ç­¾å­—å…¸
        self._init_labels()
            
        if SEABORN_AVAILABLE:
            sns.set_style("whitegrid")
    
    def _setup_chinese_fonts(self):
        """è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè§£å†³æ–¹å—å­—é—®é¢˜"""
        import matplotlib.font_manager as fm
        import platform
        
        # æ ¹æ®æ“ä½œç³»ç»Ÿè®¾ç½®ä¸­æ–‡å­—ä½“
        if platform.system() == 'Darwin':  # macOS
            chinese_fonts = [
                'PingFang SC', 'Pingfang SC', 'PingFang SC Regular',
                'Songti SC', 'STSong', 'STHeiti', 'STXihei', 
                'Arial Unicode MS', 'Helvetica Neue'
            ]
        elif platform.system() == 'Windows':
            chinese_fonts = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'FangSong', 'NSimSun']
        else:  # Linux
            chinese_fonts = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'Source Han Sans CN', 'DejaVu Sans']
        
        # æŸ¥æ‰¾ç³»ç»Ÿä¸­å¯ç”¨çš„ä¸­æ–‡å­—ä½“
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        print(f"DEBUG: ç³»ç»Ÿå¯ç”¨å­—ä½“æ€»æ•°: {len(available_fonts)}")
        
        # ç­›é€‰å‡ºå¯èƒ½çš„ä¸­æ–‡å­—ä½“
        chinese_available = []
        for font in available_fonts:
            font_lower = font.lower()
            if any(keyword in font_lower for keyword in ['pingfang', 'song', 'hei', 'kai', 'fang', 'unicode', 'cjk', 'han']):
                chinese_available.append(font)
        
        print(f"DEBUG: å‘ç°å¯èƒ½çš„ä¸­æ–‡å­—ä½“: {chinese_available[:10]}")  # åªæ˜¾ç¤ºå‰10ä¸ª
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
        selected_font = None
        for font in chinese_fonts:
            if font in available_fonts:
                selected_font = font
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        if not selected_font and chinese_available:
            # ä¼˜å…ˆé€‰æ‹©PingFangç±»å­—ä½“ï¼ˆmacOSæœ€ä½³ä¸­æ–‡æ˜¾ç¤ºï¼‰
            for font in chinese_available:
                if 'pingfang' in font.lower() or 'ping fang' in font.lower():
                    selected_font = font
                    break
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
            if not selected_font:
                selected_font = chinese_available[0]
        
        # è®¾ç½®å­—ä½“å‚æ•° - å¼ºåˆ¶ä½¿ç”¨Unicodeå­—ä½“ç¡®ä¿ä¸­æ–‡æ˜¾ç¤º
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans', 'SimHei', 'Microsoft YaHei']
        if selected_font:
            plt.rcParams['font.sans-serif'].insert(0, selected_font)
        
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['font.size'] = 10  # ç•¥å°ä¸€ç‚¹é¿å…é‡å 
        
        # è®¾ç½®å›¾è¡¨çš„é»˜è®¤å‚æ•°ä»¥é¿å…é‡å 
        plt.rcParams['figure.constrained_layout.use'] = True
        plt.rcParams['figure.autolayout'] = True
        plt.rcParams['axes.labelpad'] = 8
        plt.rcParams['xtick.labelsize'] = 8
        plt.rcParams['ytick.labelsize'] = 8
        plt.rcParams['axes.titlepad'] = 15
        
        # å¼ºåˆ¶æ¸…é™¤å­—ä½“ç¼“å­˜ï¼Œç¡®ä¿æ–°è®¾ç½®ç”Ÿæ•ˆ
        import matplotlib.font_manager
        try:
            matplotlib.font_manager.fontManager.__init__()
        except Exception:
            try:
                import matplotlib
                if hasattr(matplotlib.font_manager, '_get_fontconfig_fonts'):
                    matplotlib.font_manager._get_fontconfig_fonts.cache_clear()
            except Exception:
                print("âš ï¸ å­—ä½“ç¼“å­˜åˆ·æ–°å¤±è´¥ï¼Œä½†ä¸å½±å“åŠŸèƒ½")
        
        # ä¿å­˜é€‰æ‹©çš„å­—ä½“ä¾›è¯äº‘ä½¿ç”¨
        self.chinese_font = selected_font if selected_font else 'Arial Unicode MS'
        print(f"âœ… è®¾ç½®ä¸­æ–‡å­—ä½“: {self.chinese_font}")
        
        # æµ‹è¯•ä¸­æ–‡æ˜¾ç¤º
        try:
            fig, ax = plt.subplots(figsize=(1, 1))
            ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡', ha='center', va='center', fontfamily=self.chinese_font)
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            plt.close(fig)
            print("âœ… ä¸­æ–‡å­—ä½“æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"âš ï¸ ä¸­æ–‡å­—ä½“æµ‹è¯•å¤±è´¥: {e}")
    
    def _init_labels(self):
        """åˆå§‹åŒ–ä¸­è‹±æ–‡æ ‡ç­¾å­—å…¸"""
        self.labels = {
            'zh': {
                # é€šç”¨æ ‡ç­¾
                'page_num': 'PDFé¡µç ',
                'frequency': 'é¢‘ç‡',
                'similarity_score': 'ç›¸ä¼¼åº¦åˆ†æ•°',
                'page_range': 'é¡µç ',
                'character_count': 'å‡ºç°æ¬¡æ•°',
                'character_name': 'äººç‰©åç§°',
                'theme_type': 'ä¸»é¢˜ç±»å‹',
                'emotion_type': 'æƒ…æ„Ÿç±»å‹',
                'scene_length': 'åœºæ™¯é•¿åº¦',
                'cooccurrence_count': 'å…±ç°æ¬¡æ•°',
                'keyword_theme': 'å…³é”®è¯/ä¸»é¢˜',
                'occurrence_frequency': 'å‡ºç°é¢‘ç‡',
                
                # å›¾è¡¨æ ‡é¢˜
                'page_distribution': 'PDFé¡µé¢ä¸­ç›¸å…³å†…å®¹åˆ†å¸ƒ',
                'keyword_frequency': 'å…³é”®è¯é¢‘ç‡åˆ†å¸ƒ',
                'similarity_distribution': 'ç›¸ä¼¼åº¦åˆ†æ•°åˆ†å¸ƒ',
                'page_similarity_relation': 'ç›¸å…³æ€§åˆ†æ•°ä¸é¡µç å…³ç³»',
                'page_density_heatmap': 'é¡µé¢èŒƒå›´å†…å®¹å¯†åº¦çƒ­åŠ›å›¾',
                'keyword_cooccurrence': 'å…³é”®è¯å…±ç°çƒ­åŠ›å›¾',
                'theme_frequency_distribution': 'å…³é”®è¯/ä¸»é¢˜é¢‘ç‡åˆ†å¸ƒ',
                'wordcloud_title': 'å…³é”®è¯äº‘å›¾',
                'character_frequency_title': 'ä¸»è¦äººç‰©å‡ºç°é¢‘ç‡',
                'theme_distribution_title': 'ä¸»è¦ä¸»é¢˜åˆ†å¸ƒ',
                'emotion_analysis_title': 'æƒ…æ„Ÿå€¾å‘åˆ†æ',
                'character_cooccurrence_title': 'äººç‰©å…±ç°å…³ç³»',
                'content_distribution_title': 'åœºæ™¯åœ¨å„é¡µé¢çš„åˆ†å¸ƒ',
                'summary_stats_title': 'å…³é”®ç»Ÿè®¡ä¿¡æ¯',
                'literary_analysis_title': 'æ–‡å­¦åˆ†æç»¼åˆæŠ¥å‘Š',
                
                # ç»Ÿè®¡ä¿¡æ¯
                'total_passages': 'æ€»æ®µè½æ•°',
                'pages_involved': 'æ¶‰åŠé¡µæ•°',
                'max_page': 'æœ€å¤§é¡µç ',
                'avg_per_page': 'å¹³å‡æ¯é¡µæ®µè½æ•°',
                'avg_value': 'å¹³å‡å€¼',
                'trend_line': 'è¶‹åŠ¿çº¿',
                'analysis_summary': 'åˆ†ææ‘˜è¦',
                'characters_detected': 'æ£€æµ‹åˆ°äººç‰©',
                'main_character': 'ä¸»è¦äººç‰©',
                'active_themes': 'æ´»è·ƒä¸»é¢˜',
                'dominant_theme': 'ä¸»å¯¼ä¸»é¢˜',
                'emotion_total': 'æƒ…æ„Ÿè¡¨è¾¾æ€»è®¡',
                'dominant_emotion': 'ä¸»å¯¼æƒ…æ„Ÿ',
                'scenes_detected': 'æ£€æµ‹åœºæ™¯',
                'longest_scene': 'æœ€é•¿åœºæ™¯',
                'other': 'å…¶ä»–',
                
                # æç¤ºä¿¡æ¯
                'no_data': 'æ²¡æœ‰ç»“æœæ•°æ®\næ— æ³•ç”Ÿæˆé¡µç åˆ†å¸ƒå›¾',
                'no_valid_pages': 'æ²¡æœ‰æœ‰æ•ˆçš„é¡µç ä¿¡æ¯\næ— æ³•ç”Ÿæˆé¡µç åˆ†å¸ƒå›¾',
                'no_keywords_found': 'æœªæ‰¾åˆ°å…³é”®è¯',
                'no_similarity_scores': 'æ— ç›¸ä¼¼åº¦åˆ†æ•°',
                'no_page_score_data': 'æ— é¡µç /åˆ†æ•°æ•°æ®',
                'no_keyword_data': 'æ— å…³é”®è¯æ•°æ®\næ— æ³•ç”Ÿæˆå…±ç°çƒ­åŠ›å›¾',
                'insufficient_keywords': 'å…³é”®è¯æ•°é‡ä¸è¶³\néœ€è¦è‡³å°‘2ä¸ªå…³é”®è¯',
                'no_character_info': 'æœªæ£€æµ‹åˆ°äººç‰©ä¿¡æ¯',
                'no_theme_info': 'æœªæ£€æµ‹åˆ°ä¸»é¢˜ä¿¡æ¯',
                'no_emotion_info': 'æœªæ£€æµ‹åˆ°æƒ…æ„Ÿä¿¡æ¯',
                'no_character_relations': 'æœªæ£€æµ‹åˆ°äººç‰©å…³ç³»',
                'no_page_distribution': 'æœªæ£€æµ‹åˆ°é¡µé¢åˆ†å¸ƒä¿¡æ¯',
                'no_stats_data': 'æš‚æ— ç»Ÿè®¡æ•°æ®'
            },
            'en': {
                # é€šç”¨æ ‡ç­¾
                'page_num': 'PDF Page',
                'frequency': 'Frequency',
                'similarity_score': 'Similarity Score',
                'page_range': 'Page Number',
                'character_count': 'Occurrences',
                'character_name': 'Character Name',
                'theme_type': 'Theme Type',
                'emotion_type': 'Emotion Type',
                'scene_length': 'Scene Length',
                'cooccurrence_count': 'Co-occurrence Count',
                'keyword_theme': 'Keywords/Themes',
                'occurrence_frequency': 'Occurrence Frequency',
                
                # å›¾è¡¨æ ‡é¢˜
                'page_distribution': 'Content Distribution Across PDF Pages',
                'keyword_frequency': 'Keyword Frequency Distribution',
                'similarity_distribution': 'Similarity Score Distribution',
                'page_similarity_relation': 'Similarity Score vs Page Number',
                'page_density_heatmap': 'Page Range Content Density Heatmap',
                'keyword_cooccurrence': 'Keyword Co-occurrence Heatmap',
                'theme_frequency_distribution': 'Keywords/Themes Frequency Distribution',
                'wordcloud_title': 'Keywords Word Cloud',
                'character_frequency_title': 'Main Characters Frequency',
                'theme_distribution_title': 'Main Themes Distribution',
                'emotion_analysis_title': 'Emotion Analysis',
                'character_cooccurrence_title': 'Character Co-occurrence',
                'content_distribution_title': 'Scene Distribution Across Pages',
                'summary_stats_title': 'Key Statistics',
                'literary_analysis_title': 'Literary Analysis Report',
                
                # ç»Ÿè®¡ä¿¡æ¯
                'total_passages': 'Total Passages',
                'pages_involved': 'Pages Involved',
                'max_page': 'Max Page',
                'avg_per_page': 'Avg Passages per Page',
                'avg_value': 'Average',
                'trend_line': 'Trend Line',
                'analysis_summary': 'Analysis Summary',
                'characters_detected': 'Characters Detected',
                'main_character': 'Main Character',
                'active_themes': 'Active Themes',
                'dominant_theme': 'Dominant Theme',
                'emotion_total': 'Total Emotions',
                'dominant_emotion': 'Dominant Emotion',
                'scenes_detected': 'Scenes Detected',
                'longest_scene': 'Longest Scene',
                'other': '',  # è‹±æ–‡ä¸­ä¸éœ€è¦é‡è¯
                
                # æç¤ºä¿¡æ¯
                'no_data': 'No result data\nCannot generate page distribution chart',
                'no_valid_pages': 'No valid page information\nCannot generate page distribution chart',
                'no_keywords_found': 'No keywords found',
                'no_similarity_scores': 'No similarity scores',
                'no_page_score_data': 'No page/score data',
                'no_keyword_data': 'No keyword data\nCannot generate co-occurrence heatmap',
                'insufficient_keywords': 'Insufficient keywords\nNeed at least 2 keywords',
                'no_character_info': 'No character information detected',
                'no_theme_info': 'No theme information detected',
                'no_emotion_info': 'No emotion information detected',
                'no_character_relations': 'No character relations detected',
                'no_page_distribution': 'No page distribution information detected',
                'no_stats_data': 'No statistical data available'
            }
        }
    
    def get_label(self, key):
        """è·å–å½“å‰è¯­è¨€çš„æ ‡ç­¾"""
        return self.labels[self.language].get(key, key)
    
    def set_language(self, language):
        """è®¾ç½®å›¾è¡¨è¯­è¨€"""
        if language in ['zh', 'en']:
            self.language = language
        else:
            print(f"âš ï¸ ä¸æ”¯æŒçš„è¯­è¨€: {language}ï¼Œä¿æŒå½“å‰è¯­è¨€: {self.language}")
    
    def get_font_family(self):
        """æ ¹æ®è¯­è¨€è·å–åˆé€‚çš„å­—ä½“"""
        if self.language == 'zh':
            return getattr(self, 'chinese_font', 'Arial Unicode MS')
        else:
            return 'Arial'
    
    def _get_wordcloud_font_path(self):
        """è·å–è¯äº‘ä¸“ç”¨çš„å­—ä½“è·¯å¾„"""
        import matplotlib.font_manager as fm
        import platform
        
        # å¦‚æœå·²ç»æœ‰æ£€æµ‹åˆ°çš„ä¸­æ–‡å­—ä½“ï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„å­—ä½“æ–‡ä»¶
        if hasattr(self, 'chinese_font') and self.chinese_font:
            try:
                # é€šè¿‡å­—ä½“ç®¡ç†å™¨æŸ¥æ‰¾å­—ä½“æ–‡ä»¶è·¯å¾„
                font_files = fm.findSystemFonts()
                font_props = fm.FontProperties(family=self.chinese_font)
                
                # å°è¯•é€šè¿‡åç§°åŒ¹é…æ‰¾åˆ°å­—ä½“æ–‡ä»¶
                for font_file in font_files:
                    try:
                        font_prop = fm.FontProperties(fname=font_file)
                        if font_prop.get_name() == self.chinese_font:
                            print(f"ğŸ¨ æ‰¾åˆ°è¯äº‘å­—ä½“æ–‡ä»¶: {font_file}")
                            return font_file
                    except:
                        continue
                        
            except Exception as e:
                print(f"âš ï¸ åŠ¨æ€å­—ä½“è·¯å¾„æŸ¥æ‰¾å¤±è´¥: {e}")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šé¢„è®¾è·¯å¾„
        import platform
        
        if platform.system() == 'Darwin':  # macOS
            possible_paths = [
                '/System/Library/Fonts/Arial Unicode MS.ttf',
                '/System/Library/Fonts/Supplemental/Arial Unicode MS.ttf',
                '/Library/Fonts/Arial Unicode MS.ttf',
                '/System/Library/Fonts/STHeiti Light.ttc',
                '/System/Library/Fonts/STHeiti Medium.ttc',
                '/System/Library/Fonts/PingFang.ttc',
                '/System/Library/Fonts/Helvetica.ttc',
            ]
        elif platform.system() == 'Windows':
            possible_paths = [
                'C:/Windows/Fonts/simhei.ttf',
                'C:/Windows/Fonts/msyh.ttc',
                'C:/Windows/Fonts/simsun.ttc',
                'C:/Windows/Fonts/arial.ttf',
            ]
        else:  # Linux
            possible_paths = [
                '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
                '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',
                '/usr/share/fonts/TTF/DejaVuSans.ttf',
            ]
        
        # æ£€æŸ¥å“ªä¸ªè·¯å¾„å­˜åœ¨
        for path in possible_paths:
            if os.path.exists(path):
                print(f"ğŸ¨ ä½¿ç”¨è¯äº‘å­—ä½“è·¯å¾„: {path}")
                return path
        
        print("âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„è¯äº‘å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
        return None
    
    def plot_page_distribution(self, results, keywords, save_path=None):
        """ç»˜åˆ¶é¡µç åˆ†å¸ƒå›¾"""
        if not results:
            print("æ²¡æœ‰ç»“æœæ•°æ®ï¼Œè·³è¿‡é¡µç åˆ†å¸ƒå›¾")
            # è¿”å›ä¸€ä¸ªæç¤ºå›¾
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, self.get_label('no_data'), 
                   ha='center', va='center', fontsize=16,
                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),
                   fontfamily=self.get_font_family())
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            plt.tight_layout()
            return fig
        
        # æå–é¡µç ä¿¡æ¯
        page_numbers = []
        for result in results:
            page_num = result.get('page_num', 1)
            if isinstance(page_num, (int, float)) and page_num > 0:
                page_numbers.append(int(page_num))
        
        if not page_numbers:
            print("æ²¡æœ‰æœ‰æ•ˆçš„é¡µç ä¿¡æ¯ï¼Œè·³è¿‡é¡µç åˆ†å¸ƒå›¾")
            # è¿”å›ä¸€ä¸ªæç¤ºå›¾
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, self.get_label('no_valid_pages'), 
                   ha='center', va='center', fontsize=16,
                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),
                   fontfamily=self.get_font_family())
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            plt.tight_layout()
            return fig
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        font_family = self.get_font_family()
        
        # 1. é¡µç ç›´æ–¹å›¾
        page_counts = Counter(page_numbers)
        pages = sorted(page_counts.keys())
        counts = [page_counts[p] for p in pages]
        
        ax1.bar(pages, counts, alpha=0.7, color='skyblue', edgecolor='navy')
        ax1.set_xlabel(self.get_label('page_num'), fontfamily=font_family)
        ax1.set_ylabel(self.get_label('frequency'), fontfamily=font_family)
        
        # æ„å»ºæ ‡é¢˜
        if self.language == 'zh':
            title = f"{self.get_label('page_distribution')}\nå…³é”®è¯: {', '.join(keywords)}"
        else:
            title = f"{self.get_label('page_distribution')}\nKeywords: {', '.join(keywords)}"
        ax1.set_title(title, fontfamily=font_family)
        ax1.grid(True, alpha=0.3)
        
        # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
        for label in ax1.get_xticklabels():
            label.set_fontfamily(font_family)
        for label in ax1.get_yticklabels():
            label.set_fontfamily(font_family)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total_passages = len(page_numbers)
        unique_pages = len(set(page_numbers))
        max_page = max(page_numbers)
        
        if self.language == 'zh':
            stats_text = f'{self.get_label("total_passages")}: {total_passages}\n{self.get_label("pages_involved")}: {unique_pages}\n{self.get_label("max_page")}: {max_page}'
        else:
            stats_text = f'{self.get_label("total_passages")}: {total_passages}\n{self.get_label("pages_involved")}: {unique_pages}\n{self.get_label("max_page")}: {max_page}'
        
        ax1.text(0.02, 0.98, stats_text, 
                transform=ax1.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                fontfamily=font_family)
        
        # 2. é¡µç å¯†åº¦çƒ­åŠ›å›¾
        # å°†é¡µç æŒ‰åŒºé—´åˆ†ç»„ï¼Œæ˜¾ç¤ºå†…å®¹å¯†åº¦
        max_page = max(page_numbers)
        page_ranges = []
        densities = []
        
        # åˆ†æˆ10ä¸ªåŒºé—´
        range_size = max(1, max_page // 10)
        for i in range(0, max_page, range_size):
            start_page = i + 1
            end_page = min(i + range_size, max_page)
            range_name = f"{start_page}-{end_page}"
            
            # è®¡ç®—è¯¥åŒºé—´çš„å†…å®¹å¯†åº¦
            count_in_range = sum(1 for p in page_numbers if start_page <= p <= end_page)
            density = count_in_range / range_size if range_size > 0 else 0
            
            page_ranges.append(range_name)
            densities.append(density)
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        density_matrix = np.array(densities).reshape(1, -1)
        im = ax2.imshow(density_matrix, cmap='YlOrRd', aspect='auto')
        ax2.set_xticks(range(len(page_ranges)))
        ax2.set_xticklabels(page_ranges, rotation=45, fontfamily=font_family)
        ax2.set_yticks([])
        ax2.set_title(self.get_label('page_density_heatmap'), fontfamily=font_family)
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax2, orientation='horizontal', pad=0.1)
        cbar.set_label(self.get_label('avg_per_page'), fontfamily=font_family)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨å¹¶è¿”å›Figureå¯¹è±¡ï¼Œæ–¹ä¾¿åœ¨Streamlitä¸­å±•ç¤º
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(self.output_dir, f'page_distribution_{timestamp}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        # è¿”å›å›¾å½¢å¯¹è±¡
        return fig
        return fig
    
    def plot_theme_analysis(self, results, keywords, save_path=None):
        """ç»˜åˆ¶ä¸»é¢˜åˆ†æå›¾"""
        if not results:
            print("æ²¡æœ‰ç»“æœæ•°æ®ï¼Œè·³è¿‡ä¸»é¢˜åˆ†æå›¾")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        font_family = self.get_font_family()
        
        # 1. å…³é”®è¯é¢‘ç‡åˆ†å¸ƒ
        all_keywords = []
        for result in results:
            found_keywords = result.get('found_keywords', [])
            all_keywords.extend(found_keywords)
        
        if all_keywords:
            keyword_counts = Counter(all_keywords)
            keywords_list = list(keyword_counts.keys())
            counts = list(keyword_counts.values())
            
            ax1.barh(keywords_list, counts, color='lightcoral')
            ax1.set_xlabel(self.get_label('frequency'), fontfamily=font_family)
            ax1.set_title(self.get_label('keyword_frequency'), fontfamily=font_family)
            ax1.grid(True, alpha=0.3)
            
            # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
            for label in ax1.get_xticklabels():
                label.set_fontfamily(font_family)
            for label in ax1.get_yticklabels():
                label.set_fontfamily(font_family)
        else:
            ax1.text(0.5, 0.5, self.get_label('no_keywords_found'), ha='center', va='center', transform=ax1.transAxes, fontfamily=font_family)
            ax1.set_title(self.get_label('keyword_frequency'), fontfamily=font_family)
        
        # 2. ç›¸å…³æ€§åˆ†æ•°åˆ†å¸ƒ
        similarity_scores = []
        for result in results:
            score = result.get('similarity_score') or result.get('hybrid_score') or result.get('bm25_score', 0)
            if score > 0:
                similarity_scores.append(score)
        
        if similarity_scores:
            ax2.hist(similarity_scores, bins=20, alpha=0.7, color='lightgreen', edgecolor='darkgreen')
            ax2.set_xlabel(self.get_label('similarity_score'), fontfamily=font_family)
            ax2.set_ylabel(self.get_label('frequency'), fontfamily=font_family)
            ax2.set_title(self.get_label('similarity_distribution'), fontfamily=font_family)
            ax2.grid(True, alpha=0.3)
            
            # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
            for label in ax2.get_xticklabels():
                label.set_fontfamily(font_family)
            for label in ax2.get_yticklabels():
                label.set_fontfamily(font_family)
            
            # æ·»åŠ ç»Ÿè®¡çº¿
            mean_score = np.mean(similarity_scores)
            avg_label = f'{self.get_label("avg_value")}: {mean_score:.3f}'
            ax2.axvline(mean_score, color='red', linestyle='--', label=avg_label)
            legend = ax2.legend()
            legend.get_texts()[0].set_fontfamily(font_family)
        else:
            ax2.text(0.5, 0.5, self.get_label('no_similarity_scores'), ha='center', va='center', transform=ax2.transAxes, fontfamily=font_family)
            ax2.set_title(self.get_label('similarity_distribution'), fontfamily=font_family)
        
        # 3. æ®µè½é•¿åº¦åˆ†å¸ƒ - å·²ç§»é™¤
        ax3.axis('off')
        if self.language == 'zh':
            ax3.set_title('æ®µè½é•¿åº¦åˆ†å¸ƒå·²ç§»é™¤', fontfamily=font_family)
        else:
            ax3.set_title('Paragraph Length Distribution Removed', fontfamily=font_family)
        
        # 4. é¡µç vsç›¸å…³æ€§æ•£ç‚¹å›¾
        pages = []
        scores = []
        for result in results:
            page_num = result.get('page_num', 1)
            score = result.get('similarity_score') or result.get('hybrid_score') or result.get('bm25_score', 0)
            if page_num > 0 and score > 0:
                pages.append(page_num)
                scores.append(score)
        
        if pages and scores:
            ax4.scatter(pages, scores, alpha=0.6, color='purple')
            ax4.set_xlabel(self.get_label('page_range'), fontfamily=font_family)
            ax4.set_ylabel(self.get_label('similarity_score'), fontfamily=font_family)
            ax4.set_title(self.get_label('page_similarity_relation'), fontfamily=font_family)
            ax4.grid(True, alpha=0.3)
            
            # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
            for label in ax4.get_xticklabels():
                label.set_fontfamily(font_family)
            for label in ax4.get_yticklabels():
                label.set_fontfamily(font_family)
            
            # æ·»åŠ è¶‹åŠ¿çº¿
            if len(pages) > 1:
                z = np.polyfit(pages, scores, 1)
                p = np.poly1d(z)
                ax4.plot(pages, p(pages), "r--", alpha=0.8, label=self.get_label('trend_line'))
                legend = ax4.legend()
                legend.get_texts()[0].set_fontfamily(font_family)
        else:
            ax4.text(0.5, 0.5, self.get_label('no_page_score_data'), ha='center', va='center', transform=ax4.transAxes, fontfamily=font_family)
            ax4.set_title(self.get_label('page_similarity_relation'), fontfamily=font_family)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨å¹¶è¿”å›Figureå¯¹è±¡ï¼Œæ–¹ä¾¿åœ¨Streamlitä¸­å±•ç¤º
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(self.output_dir, f'theme_analysis_{timestamp}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        return fig
    
    def plot_word_cloud(self, results, keywords, max_words=200):
        """ç»˜åˆ¶å…³é”®è¯è¯äº‘å›¾"""
        if not WORDCLOUD_AVAILABLE:
            print("è¯äº‘åº“æœªå®‰è£…ï¼Œè·³è¿‡è¯äº‘å›¾")
            # è¿”å›ä¸€ä¸ªæç¤ºå›¾
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.text(0.5, 0.5, 'è¯äº‘åº“æœªå®‰è£…\nè¯·å®‰è£… wordcloud åŒ…', 
                   ha='center', va='center', fontsize=16,
                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8),
                   fontfamily=self.get_font_family())
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            plt.tight_layout()
            return fig
            
        # æ”¶é›†æ‰€æœ‰åŒ¹é…å…³é”®è¯å’Œæ–‡æœ¬å†…å®¹
        all_text_words = []
        all_keywords = []
        
        # ä»ç»“æœä¸­æå–æ›´å¤šè¯æ±‡
        for r in results:
            # è·å–found_keywords
            found_kw = r.get('found_keywords', [])
            if found_kw:
                all_keywords.extend(found_kw)
            
            # ä»æ–‡æœ¬å†…å®¹ä¸­æå–æ›´å¤šè¯æ±‡
            text_content = r.get('text', r.get('content', ''))
            if text_content:
                # ç®€å•çš„è¯æ±‡æå– - æ”¯æŒä¸­è‹±æ–‡
                import re
                # æå–è‹±æ–‡å•è¯
                english_words = re.findall(r'\b[a-zA-Z]{3,}\b', text_content.lower())
                # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2ä¸ªå­—ç¬¦ä»¥ä¸Šï¼‰
                chinese_words = re.findall(r'[\u4e00-\u9fff]{2,}', text_content)
                
                # è¿‡æ»¤å¸¸è§åœç”¨è¯
                stop_words = {
                    'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 
                    'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 
                    'above', 'below', 'between', 'among', 'this', 'that', 'these', 'those',
                    'his', 'her', 'him', 'she', 'was', 'were', 'been', 'have', 'has', 'had',
                    'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must',
                    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¹Ÿ', 
                    'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£'
                }
                
                # è¿‡æ»¤è‹±æ–‡åœç”¨è¯
                filtered_english = [w for w in english_words if w not in stop_words and len(w) > 3]
                # è¿‡æ»¤ä¸­æ–‡åœç”¨è¯
                filtered_chinese = [w for w in chinese_words if w not in stop_words and len(w) >= 2]
                
                all_text_words.extend(filtered_english + filtered_chinese)
        
        # åˆå¹¶å…³é”®è¯å’Œæ–‡æœ¬è¯æ±‡
        all_words = all_keywords + all_text_words
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯æ±‡ï¼Œä½¿ç”¨åŸå§‹æœç´¢å…³é”®è¯
        if not all_words and keywords:
            all_words = keywords * 5  # é‡å¤å…³é”®è¯ä»¥å¢åŠ æƒé‡
            
        if not all_words:
            print("æ— å…³é”®è¯æ•°æ®ï¼Œè·³è¿‡è¯äº‘å›¾")
            # è¿”å›ä¸€ä¸ªæç¤ºå›¾
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.text(0.5, 0.5, self.get_label('no_keywords_found') + '\næ— æ³•ç”Ÿæˆè¯äº‘å›¾', 
                   ha='center', va='center', fontsize=16,
                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),
                   fontfamily=self.get_font_family())
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            plt.tight_layout()
            return fig
            
        # åˆ›å»ºè¯é¢‘ç»Ÿè®¡
        word_freq = Counter(all_words)
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›è¯æ±‡
        if len(word_freq) < 5:
            # å¦‚æœè¯æ±‡å¤ªå°‘ï¼Œæ·»åŠ ä¸€äº›é€šç”¨è¯æ±‡
            if self.language == 'zh':
                common_words = ['åˆ†æ', 'æœç´¢', 'æ–‡æœ¬', 'å†…å®¹', 'å…³é”®è¯', 'ä¸»é¢˜', 'æƒ…æ„Ÿ', 'äººç‰©', 'æ•…äº‹', 'å™è¿°']
            else:
                common_words = ['analysis', 'text', 'search', 'literature', 'character', 'theme', 'emotion', 'story', 'narrative', 'content']
            
            for word in common_words:
                if word not in word_freq:
                    word_freq[word] = 1
        
        # ç”Ÿæˆè¯äº‘æ–‡æœ¬ - æ ¹æ®é¢‘ç‡é‡å¤è¯æ±‡
        text_for_wordcloud = []
        for word, count in word_freq.most_common(max_words):
            # æ ¹æ®é¢‘ç‡é‡å¤å•è¯ï¼Œä½†é™åˆ¶æœ€å¤§é‡å¤æ¬¡æ•°ä»¥ä¿æŒå¹³è¡¡
            repeat_count = min(count, 10)  # æœ€å¤šé‡å¤10æ¬¡
            text_for_wordcloud.extend([word] * repeat_count)
        
        text = ' '.join(text_for_wordcloud)
        
        # å¦‚æœæ–‡æœ¬ä»ç„¶å¤ªçŸ­ï¼Œè¡¥å……å†…å®¹
        if len(text.split()) < 20:
            text += ' ' + ' '.join(keywords * 3) if keywords else ''
        
        try:
            # ä½¿ç”¨åŠ¨æ€æ£€æµ‹åˆ°çš„ä¸­æ–‡å­—ä½“
            font_path = self._get_wordcloud_font_path()
            
            # åˆ›å»ºè¯äº‘é…ç½®
            wordcloud_config = {
                'width': 800, 
                'height': 400, 
                'max_words': max_words,
                'background_color': 'white', 
                'collocations': False,
                'relative_scaling': 0.5,
                'min_font_size': 12,
                'max_font_size': 80,
                'colormap': 'viridis',
                'prefer_horizontal': 0.7,
                'min_word_length': 2
            }
            
            # åªæœ‰åœ¨æ‰¾åˆ°å­—ä½“æ–‡ä»¶æ—¶æ‰è®¾ç½®font_path
            if font_path:
                wordcloud_config['font_path'] = font_path
            
            wc = WordCloud(**wordcloud_config)
            
            # ç”Ÿæˆè¯äº‘
            if text.strip():
                wc.generate(text)
            else:
                # å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œä½¿ç”¨å…³é”®è¯åˆ›å»ºåŸºæœ¬è¯äº‘
                fallback_text = ' '.join(keywords * 3) if keywords else 'text analysis search'
                wc.generate(fallback_text)
            
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wc, interpolation='bilinear')
            ax.axis('off')
            ax.set_title(self.get_label('wordcloud_title'), fontfamily=self.get_font_family(), pad=20)
            plt.tight_layout()
            return fig
            
        except Exception as e:
            print(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›é”™è¯¯æç¤ºå›¾
            fig, ax = plt.subplots(figsize=(10, 5))
            error_msg = f'è¯äº‘ç”Ÿæˆå¤±è´¥\n{str(e)[:100]}...' if len(str(e)) > 100 else f'è¯äº‘ç”Ÿæˆå¤±è´¥\n{str(e)}'
            ax.text(0.5, 0.5, error_msg, 
                   ha='center', va='center', fontsize=12,
                   bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8),
                   fontfamily=self.get_font_family())
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            plt.tight_layout()
            return fig
    
    def plot_cooccurrence_heatmap(self, results, keywords, top_n=10):
        """ç»˜åˆ¶å…³é”®è¯å…±ç°çƒ­åŠ›å›¾ - æ”¹è¿›ç‰ˆ"""
        print(f"DEBUG: å¼€å§‹ç”Ÿæˆå…±ç°çƒ­åŠ›å›¾ï¼Œè¾“å…¥ç»“æœæ•°: {len(results) if results else 0}, å…³é”®è¯: {keywords}")
        
        try:
            # æ”¶é›†å…±ç°è®¡æ•°
            from itertools import combinations
            co_counts = Counter()
            
            # å…ˆå°è¯•ä»found_keywordsè·å–
            all_keywords = []
            for r in results:
                found_kw = r.get('found_keywords', [])
                if found_kw:
                    all_keywords.extend(found_kw)
            
            print(f"DEBUG: ä»found_keywordsè·å–åˆ° {len(all_keywords)} ä¸ªå…³é”®è¯")
            
            # å¦‚æœæ²¡æœ‰found_keywordsï¼Œä½¿ç”¨åŸå§‹æœç´¢å…³é”®è¯
            if not all_keywords and keywords:
                # åˆ›å»ºåŸºäºæ–‡æœ¬å†…å®¹çš„å…³é”®è¯åŒ¹é…
                for r in results:
                    text_content = r.get('text', '').lower()
                    for keyword in keywords:
                        if keyword.lower() in text_content:
                            count = text_content.count(keyword.lower())
                            all_keywords.extend([keyword] * count)
                print(f"DEBUG: åŸºäºæ–‡æœ¬åŒ¹é…è·å–åˆ° {len(all_keywords)} ä¸ªå…³é”®è¯")
            
            # æœ€åçš„å›é€€ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®
            if not all_keywords:
                if keywords and len(keywords) >= 2:
                    # ä½¿ç”¨æœç´¢å…³é”®è¯åˆ›å»ºåŸºæœ¬å…±ç°
                    all_keywords = keywords * 3  # ç»™æ¯ä¸ªå…³é”®è¯ä¸€äº›æƒé‡
                    print(f"DEBUG: ä½¿ç”¨æœç´¢å…³é”®è¯åˆ›å»ºç¤ºä¾‹æ•°æ®: {len(all_keywords)} ä¸ª")
                else:
                    print('DEBUG: æ— æ³•ç”Ÿæˆå…±ç°çƒ­åŠ›å›¾ - ç¼ºå°‘å…³é”®è¯æ•°æ®')
                    return self._create_placeholder_heatmap("æ— å…³é”®è¯æ•°æ®\næ— æ³•ç”Ÿæˆå…±ç°çƒ­åŠ›å›¾")
            
            # è®¡ç®—å…±ç°
            for r in results:
                kws = r.get('found_keywords', [])
                if not kws and keywords:
                    # å¦‚æœæ²¡æœ‰found_keywordsï¼ŒåŸºäºæ–‡æœ¬å†…å®¹åŒ¹é…
                    text_content = r.get('text', '').lower()
                    kws = [k for k in keywords if k.lower() in text_content]
                
                unique_kws = list(set(kws))
                if len(unique_kws) >= 2:
                    for a, b in combinations(sorted(unique_kws), 2):
                        co_counts[(a, b)] += 1
            
            print(f"DEBUG: è®¡ç®—å¾—åˆ° {len(co_counts)} ä¸ªå…±ç°å¯¹")
            
            # é€‰å–æœ€å¸¸è§å…³é”®è¯
            keyword_counts = Counter(all_keywords)
            top_keywords = [kw for kw, _ in keyword_counts.most_common(min(top_n, len(keyword_counts)))]
            
            print(f"DEBUG: é€‰æ‹©äº† {len(top_keywords)} ä¸ªé¡¶çº§å…³é”®è¯: {top_keywords}")
            
            if len(top_keywords) < 2:
                print(f'DEBUG: å…³é”®è¯æ•°é‡ä¸è¶³ ({len(top_keywords)})ï¼Œåˆ›å»ºå ä½å›¾')
                return self._create_placeholder_heatmap(f"å…³é”®è¯æ•°é‡ä¸è¶³ ({len(top_keywords)})\néœ€è¦è‡³å°‘2ä¸ªå…³é”®è¯")
            
            # æ„é€ çŸ©é˜µ
            size = len(top_keywords)
            matrix = []
            for x in top_keywords:
                row = []
                for y in top_keywords:
                    if x == y:
                        # å¯¹è§’çº¿æ˜¾ç¤ºè¯¥å…³é”®è¯çš„é¢‘ç‡
                        row.append(keyword_counts.get(x, 0))
                    else:
                        # æŸ¥æ‰¾å…±ç°æ¬¡æ•°
                        cooccur_count = co_counts.get((x, y), co_counts.get((y, x), 0))
                        row.append(cooccur_count)
                matrix.append(row)
            
            print(f"DEBUG: æ„å»ºäº† {size}x{size} çš„å…±ç°çŸ©é˜µ")
            
            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(8, 6))
            font_family = self.get_font_family()
            
            try:
                if SEABORN_AVAILABLE:
                    print("DEBUG: ä½¿ç”¨seabornç”Ÿæˆçƒ­åŠ›å›¾")
                    import seaborn as sns
                    sns.heatmap(matrix, xticklabels=top_keywords, yticklabels=top_keywords,
                                cmap='YlGnBu', annot=True, fmt='d', ax=ax, cbar=True)
                else:
                    print("DEBUG: ä½¿ç”¨matplotlibç”Ÿæˆçƒ­åŠ›å›¾")
                    import numpy as np
                    im = ax.imshow(matrix, cmap='YlGnBu', aspect='auto')
                    
                    # è®¾ç½®åˆ»åº¦å’Œæ ‡ç­¾
                    ax.set_xticks(range(size))
                    ax.set_yticks(range(size))
                    ax.set_xticklabels(top_keywords, rotation=45, ha='right')
                    ax.set_yticklabels(top_keywords)
                    
                    # æ·»åŠ æ•°å€¼æ ‡æ³¨
                    for i in range(size):
                        for j in range(size):
                            text = ax.text(j, i, matrix[i][j], ha="center", va="center", color="black")
                    
                    # æ·»åŠ é¢œè‰²æ¡
                    plt.colorbar(im, ax=ax)
                
                ax.set_title(self.get_label('keyword_cooccurrence'), fontfamily=font_family, pad=20)
                
                # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
                for label in ax.get_xticklabels():
                    label.set_fontfamily(font_family)
                for label in ax.get_yticklabels():
                    label.set_fontfamily(font_family)
                
                plt.tight_layout()
                print("DEBUG: å…±ç°çƒ­åŠ›å›¾ç”ŸæˆæˆåŠŸ")
                
                # ç¡®ä¿å›¾å½¢æ­£ç¡®æ˜¾ç¤º
                fig.canvas.draw()  # å¼ºåˆ¶ç»˜åˆ¶
                return fig
                
            except Exception as plot_error:
                print(f"DEBUG: ç»˜å›¾å¤±è´¥: {plot_error}")
                # åˆ›å»ºç®€å•çš„æŸ±çŠ¶å›¾ä½œä¸ºæ›¿ä»£
                return self._create_alternative_cooccurrence_chart(top_keywords, keyword_counts, font_family)
                
        except Exception as e:
            print(f"DEBUG: å…±ç°çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._create_placeholder_heatmap(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def _create_placeholder_heatmap(self, message):
        """åˆ›å»ºå ä½ç¬¦çƒ­åŠ›å›¾"""
        fig, ax = plt.subplots(figsize=(8, 6))
        font_family = self.get_font_family()
        ax.text(0.5, 0.5, message, 
               ha='center', va='center', fontsize=14,
               bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),
               fontfamily=font_family)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title(self.get_label('keyword_cooccurrence'), fontfamily=font_family)
        plt.tight_layout()
        return fig
    
    def _create_alternative_cooccurrence_chart(self, keywords, keyword_counts, font_family):
        """åˆ›å»ºæ›¿ä»£çš„å…³é”®è¯é¢‘ç‡å›¾"""
        fig, ax = plt.subplots(figsize=(8, 6))
        
        counts = [keyword_counts.get(kw, 0) for kw in keywords]
        colors = plt.cm.viridis(np.linspace(0, 1, len(keywords)))
        
        bars = ax.bar(keywords, counts, color=colors, alpha=0.7)
        ax.set_title(f"{self.get_label('keyword_cooccurrence')} (ç®€åŒ–ç‰ˆ)", fontfamily=font_family)
        ax.set_xlabel(self.get_label('keyword_theme'), fontfamily=font_family)
        ax.set_ylabel(self.get_label('occurrence_frequency'), fontfamily=font_family)
        
        # æ—‹è½¬æ ‡ç­¾
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}', ha='center', va='bottom', fontfamily=font_family)
        
        # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
        for label in ax.get_xticklabels():
            label.set_fontfamily(font_family)
        for label in ax.get_yticklabels():
            label.set_fontfamily(font_family)
        
        plt.tight_layout()
        
        # ç¡®ä¿å›¾å½¢æ­£ç¡®æ˜¾ç¤º
        try:
            fig.canvas.draw()
        except Exception as draw_error:
            print(f"âš ï¸ æ›¿ä»£å›¾å½¢ç»˜åˆ¶è­¦å‘Š: {draw_error}")
        
        return fig

    def plot_theme_frequency(self, results, keywords, top_n=10):
        """ç»˜åˆ¶ä¸»é¢˜/å…³é”®è¯å‡ºç°é¢‘ç‡æŸ±çŠ¶å›¾"""
        # æ”¶é›†æ‰€æœ‰å…³é”®è¯å’Œé‡è¦è¯æ±‡
        all_keywords = []
        keyword_counts = Counter()
        
        # 1. å…ˆå°è¯•ä»found_keywordsè·å–
        for r in results:
            found_kw = r.get('found_keywords', [])
            if found_kw:
                all_keywords.extend(found_kw)
        
        # 2. å¦‚æœæ²¡æœ‰found_keywordsï¼Œä»æ–‡æœ¬å†…å®¹ä¸­æå–é‡è¦è¯æ±‡
        if not all_keywords:
            import re
            for r in results:
                text_content = r.get('text', r.get('content', ''))
                if text_content:
                    # æå–è‹±æ–‡å•è¯ï¼ˆ3ä¸ªå­—æ¯ä»¥ä¸Šï¼‰
                    words = re.findall(r'\b[a-zA-Z]{3,}\b', text_content.lower())
                    # è¿‡æ»¤åœç”¨è¯
                    stop_words = {
                        'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 
                        'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 
                        'above', 'below', 'between', 'among', 'this', 'that', 'these', 'those',
                        'his', 'her', 'him', 'she', 'was', 'were', 'been', 'have', 'has', 'had',
                        'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must',
                        'said', 'say', 'says', 'one', 'two', 'now', 'out', 'who', 'get', 'use',
                        'man', 'new', 'way', 'day', 'time', 'year', 'work', 'part', 'take',
                        'place', 'make', 'end', 'first', 'last', 'good', 'great', 'old', 'own'
                    }
                    filtered_words = [w for w in words if w not in stop_words and len(w) > 3]
                    all_keywords.extend(filtered_words)
        
        # 3. å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨åŸå§‹æœç´¢å…³é”®è¯å¹¶ç»Ÿè®¡é¢‘ç‡
        if not all_keywords and keywords:
            for keyword in keywords:
                count = 0
                for r in results:
                    text = r.get('text', r.get('content', '')).lower()
                    if keyword.lower() in text:
                        count += text.lower().count(keyword.lower())
                if count > 0:
                    keyword_counts[keyword] = count
                    all_keywords.extend([keyword] * count)
        
        # ç»Ÿè®¡è¯é¢‘
        if all_keywords:
            keyword_counts = Counter(all_keywords)
        
        # 4. å¦‚æœä»ç„¶æ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºä¸€äº›ç¤ºä¾‹æ•°æ®
        if not keyword_counts:
            print('æ— å…³é”®è¯æ•°æ®ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®')
            # ä½¿ç”¨æœç´¢å…³é”®è¯ä½œä¸ºç¤ºä¾‹
            if keywords:
                for i, kw in enumerate(keywords[:top_n]):
                    keyword_counts[kw] = len(keywords) - i
            else:
                # åˆ›å»ºæ–‡å­¦ä¸»é¢˜ç¤ºä¾‹
                sample_themes = ['love', 'death', 'power', 'betrayal', 'honor', 'fear', 'ambition']
                for i, theme in enumerate(sample_themes):
                    keyword_counts[theme] = len(sample_themes) - i
        
        if not keyword_counts:
            print('æ— å…³é”®è¯æ•°æ®ï¼Œè·³è¿‡ä¸»é¢˜é¢‘ç‡å›¾')
            # è¿”å›ä¸€ä¸ªæç¤ºå›¾
            fig, ax = plt.subplots(figsize=(8, 5))
            ax.text(0.5, 0.5, 'æ— å…³é”®è¯æ•°æ®\næ— æ³•ç”Ÿæˆé¢‘ç‡å›¾', 
                   ha='center', va='center', fontsize=16,
                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            plt.tight_layout()
            return fig
            
        # è·å–æœ€å¸¸è§çš„è¯æ±‡
        most = keyword_counts.most_common(min(top_n, len(keyword_counts)))
        if not most:
            print('æ— æœ‰æ•ˆå…³é”®è¯æ•°æ®ï¼Œè·³è¿‡ä¸»é¢˜é¢‘ç‡å›¾')
            # è¿”å›ä¸€ä¸ªæç¤ºå›¾
            fig, ax = plt.subplots(figsize=(8, 5))
            ax.text(0.5, 0.5, 'æ— æœ‰æ•ˆå…³é”®è¯æ•°æ®\næ— æ³•ç”Ÿæˆé¢‘ç‡å›¾', 
                   ha='center', va='center', fontsize=16,
                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.axis('off')
            plt.tight_layout()
            return fig
            
        labels, counts = zip(*most)
        fig, ax = plt.subplots(figsize=(10, 6))
        font_family = self.get_font_family()
        
        # ä½¿ç”¨æ¸å˜è‰²
        colors = plt.cm.viridis(np.linspace(0, 1, len(labels)))
        bars = ax.bar(labels, counts, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)
        
        ax.set_xlabel(self.get_label('keyword_theme'), fontfamily=self.get_font_family())
        ax.set_ylabel(self.get_label('occurrence_frequency'), fontfamily=self.get_font_family())
        ax.set_title(self.get_label('theme_frequency_distribution'), fontfamily=self.get_font_family())
        
        # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
        font_family = self.get_font_family()
        for label in ax.get_xticklabels():
            label.set_fontfamily(font_family)
        for label in ax.get_yticklabels():
            label.set_fontfamily(font_family)
        
        # æ—‹è½¬æ ‡ç­¾ä»¥é¿å…é‡å 
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
        
        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}', ha='center', va='bottom', fontweight='bold',
                   fontfamily=font_family)
        
        # æ·»åŠ ç½‘æ ¼
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_axisbelow(True)
        
        plt.tight_layout()
        return fig
    
    def plot_comprehensive_analysis(self, results, keywords, save_path=None):
        """ç»˜åˆ¶ç»¼åˆåˆ†æå›¾è¡¨"""
        if not results:
            print("æ²¡æœ‰ç»“æœæ•°æ®ï¼Œè·³è¿‡ç»¼åˆåˆ†æå›¾")
            return
        
        # åˆ†åˆ«ç»˜åˆ¶é¡µç åˆ†å¸ƒå’Œä¸»é¢˜åˆ†æ
        page_dist_path = self.plot_page_distribution(results, keywords)
        theme_analysis_path = self.plot_theme_analysis(results, keywords)
        
        return {
            'page_distribution': page_dist_path,
            'theme_analysis': theme_analysis_path
        }
    
    def generate_summary_report(self, results, keywords, pdf_info=None):
        """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æŠ¥å‘Š"""
        if not results:
            return "æ²¡æœ‰ç»“æœæ•°æ®å¯åˆ†æã€‚"
        
        # åŸºæœ¬ç»Ÿè®¡
        total_results = len(results)
        unique_pages = len(set(r.get('page_num', 1) for r in results))
        
        # å…³é”®è¯ç»Ÿè®¡
        all_keywords = []
        for result in results:
            found_keywords = result.get('found_keywords', [])
            all_keywords.extend(found_keywords)
        
        keyword_counts = Counter(all_keywords)
        
        # ç›¸å…³æ€§ç»Ÿè®¡
        scores = []
        for result in results:
            score = result.get('similarity_score') or result.get('hybrid_score') or result.get('bm25_score', 0)
            if score > 0:
                scores.append(score)
        
        # é¡µç ç»Ÿè®¡
        page_numbers = [r.get('page_num', 1) for r in results if r.get('page_num', 1) > 0]
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
=== PDF å†…å®¹åˆ†ææ‘˜è¦æŠ¥å‘Š ===
åˆ†ææ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
æœç´¢å…³é”®è¯: {", ".join(keywords)}

ğŸ“Š åŸºæœ¬ç»Ÿè®¡:
- æ‰¾åˆ°ç›¸å…³æ®µè½: {total_results} ä¸ª
- æ¶‰åŠé¡µé¢æ•°é‡: {unique_pages} é¡µ
- å¹³å‡æ¯é¡µç›¸å…³æ®µè½: {total_results / unique_pages:.1f} ä¸ª

ğŸ” å…³é”®è¯åˆ†æ:
"""
        
        if keyword_counts:
            for keyword, count in keyword_counts.most_common(5):
                report += f"- '{keyword}': å‡ºç° {count} æ¬¡\n"
        else:
            report += "- æœªæ£€æµ‹åˆ°å…³é”®è¯åŒ¹é…\n"
        
        if scores:
            report += f"""
ğŸ“ˆ ç›¸å…³æ€§åˆ†æ:
- æœ€é«˜ç›¸å…³æ€§: {max(scores):.3f}
- å¹³å‡ç›¸å…³æ€§: {np.mean(scores):.3f}
- ç›¸å…³æ€§æ ‡å‡†å·®: {np.std(scores):.3f}
"""
        
        if page_numbers:
            report += f"""
ğŸ“„ é¡µç åˆ†å¸ƒ:
- æœ€æ—©å‡ºç°é¡µç : ç¬¬ {min(page_numbers)} é¡µ
- æœ€æ™šå‡ºç°é¡µç : ç¬¬ {max(page_numbers)} é¡µ
- å†…å®¹é›†ä¸­åº¦: {len(set(page_numbers)) / (max(page_numbers) - min(page_numbers) + 1):.2%}
"""
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(self.output_dir, f'analysis_summary_{timestamp}.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ“ åˆ†ææ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return report

    def plot_literary_analysis(self, analysis_result, save_path=None):
        """ä¸ºæ–‡å­¦åˆ†æç»“æœç”Ÿæˆç»¼åˆå¯è§†åŒ–å›¾è¡¨"""
        if not analysis_result:
            print("æ²¡æœ‰æ–‡å­¦åˆ†ææ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–")
            return None
            
        # åˆ›å»ºä¸€ä¸ªå¤§çš„å›¾è¡¨åŒ…å«å¤šä¸ªå­å›¾
        fig = plt.figure(figsize=(16, 12))
        
        # 1. äººç‰©å‡ºç°é¢‘ç‡å›¾ (å·¦ä¸Š)
        ax1 = plt.subplot(2, 3, 1)
        self._plot_character_frequency(analysis_result, ax1)
        
        # 2. ä¸»é¢˜åˆ†å¸ƒé¥¼å›¾ (å³ä¸Š)
        ax2 = plt.subplot(2, 3, 2)
        self._plot_theme_distribution(analysis_result, ax2)
        
        # 3. æƒ…æ„Ÿå€¾å‘æŸ±çŠ¶å›¾ (å·¦ä¸­)
        ax3 = plt.subplot(2, 3, 3)
        self._plot_emotion_analysis(analysis_result, ax3)
        
        # 4. äººç‰©å…±ç°ç½‘ç»œå›¾ (å·¦ä¸‹)
        ax4 = plt.subplot(2, 3, 4)
        self._plot_character_cooccurrence(analysis_result, ax4)
        
        # 5. å†…å®¹åœ¨é¡µé¢ä¸­çš„åˆ†å¸ƒ (å³ä¸­)
        ax5 = plt.subplot(2, 3, 5)
        self._plot_content_distribution(analysis_result, ax5)
        
        # 6. å…³é”®ç»Ÿè®¡ä¿¡æ¯ (å³ä¸‹)
        ax6 = plt.subplot(2, 3, 6)
        self._plot_summary_stats(analysis_result, ax6)
        
        # è®¾ç½®æ€»æ ‡é¢˜å­—ä½“
        chinese_font = getattr(self, 'chinese_font', 'Arial Unicode MS')
        plt.suptitle('æ–‡å­¦åˆ†æç»¼åˆæŠ¥å‘Š', fontsize=16, y=0.95, 
                    fontfamily=chinese_font)
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(self.output_dir, f'literary_analysis_{timestamp}.png')
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æ–‡å­¦åˆ†æå¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        
        return fig
    
    def _plot_character_frequency(self, analysis_result, ax):
        """ç»˜åˆ¶äººç‰©å‡ºç°é¢‘ç‡å›¾"""
        # ä»æ­£ç¡®çš„æ•°æ®ç»“æ„ä¸­æå–äººç‰©æ•°æ®
        characters_data = analysis_result.get('characters', {})
        characters = characters_data.get('frequency', {}) if isinstance(characters_data, dict) else characters_data
        
        chinese_font = getattr(self, 'chinese_font', 'Arial Unicode MS')
        
        if not characters:
            ax.text(0.5, 0.5, 'æœªæ£€æµ‹åˆ°äººç‰©ä¿¡æ¯', ha='center', va='center', transform=ax.transAxes,
                   fontfamily=chinese_font)
            ax.set_title('äººç‰©å‡ºç°é¢‘ç‡', fontfamily=chinese_font)
            return
            
        # å–å‰10ä¸ªæœ€å¸¸å‡ºç°çš„äººç‰©
        top_characters = dict(sorted(characters.items(), key=lambda x: x[1], reverse=True)[:10])
        
        names = list(top_characters.keys())
        counts = list(top_characters.values())
        
        bars = ax.bar(names, counts, color='lightblue', edgecolor='navy', alpha=0.7)
        ax.set_title('ä¸»è¦äººç‰©å‡ºç°é¢‘ç‡', fontfamily=chinese_font, fontsize=12)
        ax.set_xlabel('äººç‰©åç§°', fontfamily=chinese_font)
        ax.set_ylabel('å‡ºç°æ¬¡æ•°', fontfamily=chinese_font)
        
        # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
        for label in ax.get_xticklabels():
            label.set_fontfamily(chinese_font)
        for label in ax.get_yticklabels():
            label.set_fontfamily(chinese_font)
        
        # æ—‹è½¬xè½´æ ‡ç­¾ä»¥é¿å…é‡å 
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
        
        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}', ha='center', va='bottom', 
                   fontfamily=chinese_font, fontsize=9)
    
    def _plot_theme_distribution(self, analysis_result, ax):
        """ç»˜åˆ¶ä¸»é¢˜åˆ†å¸ƒé¥¼å›¾"""
        # ä»æ­£ç¡®çš„æ•°æ®ç»“æ„ä¸­æå–ä¸»é¢˜æ•°æ®
        themes_data = analysis_result.get('themes', {})
        themes = themes_data.get('frequency', {}) if isinstance(themes_data, dict) else themes_data
        
        if not themes:
            ax.text(0.5, 0.5, 'æœªæ£€æµ‹åˆ°ä¸»é¢˜ä¿¡æ¯', ha='center', va='center', transform=ax.transAxes,
                   fontfamily=getattr(self, 'chinese_font', 'Arial Unicode MS'))
            ax.set_title('ä¸»é¢˜åˆ†å¸ƒ', fontfamily=getattr(self, 'chinese_font', 'Arial Unicode MS'))
            return
            
        # è¿‡æ»¤æ‰è®¡æ•°ä¸º0çš„ä¸»é¢˜
        filtered_themes = {k: v for k, v in themes.items() if v > 0}
        
        if not filtered_themes:
            ax.text(0.5, 0.5, 'æœªæ£€æµ‹åˆ°æœ‰æ•ˆä¸»é¢˜', ha='center', va='center', transform=ax.transAxes,
                   fontfamily=getattr(self, 'chinese_font', 'Arial Unicode MS'))
            ax.set_title('ä¸»é¢˜åˆ†å¸ƒ', fontfamily=getattr(self, 'chinese_font', 'Arial Unicode MS'))
            return
        
        # åˆå¹¶å°æ¯”ä¾‹é¡¹ç›®ä»¥é¿å…æ ‡ç­¾é‡å 
        total = sum(filtered_themes.values())
        main_themes = {}
        small_themes_count = 0
        
        for theme, count in filtered_themes.items():
            percentage = count / total * 100
            if percentage >= 5.0:  # åªæ˜¾ç¤ºå æ¯”å¤§äº5%çš„ä¸»é¢˜
                main_themes[theme] = count
            else:
                small_themes_count += count
        
        # å¦‚æœæœ‰å°ä¸»é¢˜ï¼Œåˆå¹¶ä¸º"å…¶ä»–"
        if small_themes_count > 0:
            main_themes['å…¶ä»–'] = small_themes_count
            
        labels = list(main_themes.keys())
        sizes = list(main_themes.values())
        colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))
        
        # ä½¿ç”¨æ›´å¥½çš„é¥¼å›¾é…ç½®ï¼Œé¿å…æ ‡ç­¾é‡å 
        wedges, texts, autotexts = ax.pie(sizes, labels=None, autopct='%1.1f%%', 
                                         colors=colors, startangle=90,
                                         pctdistance=0.85, labeldistance=1.1)
        
        # æ‰‹åŠ¨è®¾ç½®å­—ä½“
        chinese_font = getattr(self, 'chinese_font', 'Arial Unicode MS')
        
        # è®¾ç½®æ ‡é¢˜
        ax.set_title('ä¸»è¦ä¸»é¢˜åˆ†å¸ƒ', fontfamily=chinese_font, fontsize=12, pad=20)
        
        # åˆ›å»ºå›¾ä¾‹ä»£æ›¿æ ‡ç­¾ï¼Œé¿å…é‡å 
        ax.legend(wedges, labels, title="ä¸»é¢˜ç±»å‹", loc="center left", 
                 bbox_to_anchor=(1, 0, 0.5, 1), fontsize=10,
                 prop={'family': chinese_font})
        
        # è°ƒæ•´ç™¾åˆ†æ¯”æ–‡å­—
        for autotext in autotexts:
            autotext.set_fontsize(9)
            autotext.set_fontfamily(chinese_font)
            autotext.set_color('white')
            autotext.set_weight('bold')
    
    def _plot_emotion_analysis(self, analysis_result, ax):
        """ç»˜åˆ¶æƒ…æ„Ÿåˆ†ææŸ±çŠ¶å›¾"""
        # ä»æ­£ç¡®çš„æ•°æ®ç»“æ„ä¸­æå–æƒ…æ„Ÿæ•°æ®
        emotions_data = analysis_result.get('emotions', {})
        emotions = emotions_data.get('frequency', {}) if isinstance(emotions_data, dict) else emotions_data
        
        chinese_font = getattr(self, 'chinese_font', 'Arial Unicode MS')
        
        if not emotions:
            ax.text(0.5, 0.5, 'æœªæ£€æµ‹åˆ°æƒ…æ„Ÿä¿¡æ¯', ha='center', va='center', transform=ax.transAxes,
                   fontfamily=chinese_font)
            ax.set_title('æƒ…æ„Ÿå€¾å‘åˆ†æ', fontfamily=chinese_font)
            return
        
        emotion_types = list(emotions.keys())
        emotion_counts = list(emotions.values())
        colors = ['green', 'red', 'gray'][:len(emotion_types)]  # é˜²æ­¢é¢œè‰²ä¸å¤Ÿ
        
        bars = ax.bar(emotion_types, emotion_counts, color=colors, alpha=0.7)
        ax.set_title('æƒ…æ„Ÿå€¾å‘åˆ†æ', fontfamily=chinese_font, fontsize=12)
        ax.set_xlabel('æƒ…æ„Ÿç±»å‹', fontfamily=chinese_font)
        ax.set_ylabel('å‡ºç°é¢‘æ¬¡', fontfamily=chinese_font)
        
        # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
        for label in ax.get_xticklabels():
            label.set_fontfamily(chinese_font)
        for label in ax.get_yticklabels():
            label.set_fontfamily(chinese_font)
        
        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}', ha='center', va='bottom',
                   fontfamily=chinese_font, fontsize=9)
    
    def _plot_character_cooccurrence(self, analysis_result, ax):
        """ç»˜åˆ¶äººç‰©å…±ç°å…³ç³»å›¾"""
        # ä»æ­£ç¡®çš„æ•°æ®ç»“æ„ä¸­æå–äººç‰©å…±ç°æ•°æ®
        characters_data = analysis_result.get('characters', {})
        character_pairs = characters_data.get('cooccurrence', {}) if isinstance(characters_data, dict) else {}
        
        chinese_font = getattr(self, 'chinese_font', 'Arial Unicode MS')
        
        if not character_pairs:
            ax.text(0.5, 0.5, 'æœªæ£€æµ‹åˆ°äººç‰©å…³ç³»', ha='center', va='center', transform=ax.transAxes,
                   fontfamily=chinese_font)
            ax.set_title('äººç‰©å…±ç°å…³ç³»', fontfamily=chinese_font)
            return
        
        # å–å‰10ä¸ªæœ€å¸¸å…±ç°çš„äººç‰©å¯¹
        top_pairs = dict(sorted(character_pairs.items(), key=lambda x: x[1], reverse=True)[:10])
        
        # ç®€åŒ–æ˜¾ç¤ºï¼šåªæ˜¾ç¤ºäººç‰©å¯¹çš„åç§°å’Œå…±ç°æ¬¡æ•°
        pair_labels = list(top_pairs.keys())
        pair_counts = list(top_pairs.values())
        
        if pair_labels:
            bars = ax.barh(pair_labels, pair_counts, color='lightcoral', alpha=0.7)
            ax.set_title('äººç‰©å…±ç°å…³ç³»', fontfamily=chinese_font, fontsize=12)
            ax.set_xlabel('å…±ç°æ¬¡æ•°', fontfamily=chinese_font)
            
            # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
            for label in ax.get_xticklabels():
                label.set_fontfamily(chinese_font)
            for label in ax.get_yticklabels():
                label.set_fontfamily(chinese_font)
            
            # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar in bars:
                width = bar.get_width()
                ax.text(width, bar.get_y() + bar.get_height()/2.,
                       f'{int(width)}', ha='left', va='center',
                       fontfamily=chinese_font, fontsize=9)
        else:
            ax.text(0.5, 0.5, 'æ— æœ‰æ•ˆäººç‰©å…³ç³»æ•°æ®', ha='center', va='center', transform=ax.transAxes,
                   fontfamily=chinese_font)
            ax.set_title('äººç‰©å…±ç°å…³ç³»', fontfamily=chinese_font)
    
    def _plot_content_distribution(self, analysis_result, ax):
        """ç»˜åˆ¶å†…å®¹åœ¨é¡µé¢ä¸­çš„åˆ†å¸ƒ"""
        # ä»å™äº‹åˆ†æä¸­æå–é¡µé¢åˆ†å¸ƒä¿¡æ¯
        narrative_data = analysis_result.get('narrative', {})
        scenes = narrative_data.get('scenes', []) if isinstance(narrative_data, dict) else []
        
        font_family = self.get_font_family()
        
        if not scenes:
            ax.text(0.5, 0.5, self.get_label('no_page_distribution'), ha='center', va='center', transform=ax.transAxes,
                   fontfamily=font_family)
            ax.set_title(self.get_label('content_distribution_title'), fontfamily=font_family)
            return
        
        # ä»åœºæ™¯æ•°æ®ä¸­æå–é¡µç å’Œé•¿åº¦ä¿¡æ¯
        pages = [scene.get('start_page', 1) for scene in scenes]
        content_lengths = [scene.get('length', 0) for scene in scenes]
        
        if pages and content_lengths:
            ax.plot(pages, content_lengths, 'o-', color='purple', linewidth=2, markersize=6)
            ax.set_title(self.get_label('content_distribution_title'), fontfamily=font_family, fontsize=12)
            ax.set_xlabel(self.get_label('page_range'), fontfamily=font_family)
            ax.set_ylabel(self.get_label('scene_length'), fontfamily=font_family)
            ax.grid(True, alpha=0.3)
            
            # è®¾ç½®è½´æ ‡ç­¾å­—ä½“
            for label in ax.get_xticklabels():
                label.set_fontfamily(font_family)
            for label in ax.get_yticklabels():
                label.set_fontfamily(font_family)
            
            # æ ‡æ³¨å³°å€¼ - ä¿®å¤é‡å é—®é¢˜
            if content_lengths:
                max_idx = content_lengths.index(max(content_lengths))
                max_page = pages[max_idx]
                max_length = max(content_lengths)
                
                # ä½¿ç”¨æ›´ç®€æ´çš„æ³¨é‡Šï¼Œé¿å…é‡å 
                if self.language == 'zh':
                    annotation_text = f'å³°å€¼: ç¬¬{max_page}é¡µ\n({max_length}å­—ç¬¦)'
                else:
                    annotation_text = f'Peak: Page {max_page}\n({max_length} chars)'
                
                # è®¡ç®—æ³¨é‡Šä½ç½®ï¼Œé¿å…é‡å 
                x_offset = 20 if max_idx < len(pages) / 2 else -80
                y_offset = 20
                
                ax.annotate(annotation_text,
                           xy=(max_page, max_length),
                           xytext=(x_offset, y_offset), textcoords='offset points',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                           arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.1'),
                           fontfamily=font_family, fontsize=8)
        else:
            ax.text(0.5, 0.5, self.get_label('no_page_distribution'), ha='center', va='center', transform=ax.transAxes,
                   fontfamily=font_family)
            ax.set_title(self.get_label('content_distribution_title'), fontfamily=font_family)
    
    def _plot_summary_stats(self, analysis_result, ax):
        """ç»˜åˆ¶å…³é”®ç»Ÿè®¡ä¿¡æ¯"""
        ax.axis('off')  # éšè—åæ ‡è½´
        
        font_family = self.get_font_family()
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        stats = []
        
        # äººç‰©ç»Ÿè®¡
        characters_data = analysis_result.get('characters', {})
        characters = characters_data.get('frequency', {}) if isinstance(characters_data, dict) else characters_data
        if characters:
            stats.append(f"{self.get_label('characters_detected')}: {len(characters)}{self.get_label('other') if self.language == 'zh' else ''}")
            top_character = max(characters.items(), key=lambda x: x[1])
            if self.language == 'zh':
                stats.append(f"{self.get_label('main_character')}: {top_character[0]} ({top_character[1]}æ¬¡)")
            else:
                stats.append(f"{self.get_label('main_character')}: {top_character[0]} ({top_character[1]} times)")
        
        # ä¸»é¢˜ç»Ÿè®¡
        themes_data = analysis_result.get('themes', {})
        themes = themes_data.get('frequency', {}) if isinstance(themes_data, dict) else themes_data
        if themes:
            active_themes = {k: v for k, v in themes.items() if v > 0}
            stats.append(f"{self.get_label('active_themes')}: {len(active_themes)}{self.get_label('other') if self.language == 'zh' else ''}")
            if active_themes:
                top_theme = max(active_themes.items(), key=lambda x: x[1])
                if self.language == 'zh':
                    stats.append(f"{self.get_label('dominant_theme')}: {top_theme[0]} ({top_theme[1]}æ¬¡)")
                else:
                    stats.append(f"{self.get_label('dominant_theme')}: {top_theme[0]} ({top_theme[1]} times)")
        
        # æƒ…æ„Ÿç»Ÿè®¡
        emotions_data = analysis_result.get('emotions', {})
        emotions = emotions_data.get('frequency', {}) if isinstance(emotions_data, dict) else emotions_data
        if emotions:
            total_emotions = sum(emotions.values())
            if self.language == 'zh':
                stats.append(f"{self.get_label('emotion_total')}: {total_emotions}æ¬¡")
            else:
                stats.append(f"{self.get_label('emotion_total')}: {total_emotions} times")
            
            if emotions:
                dominant_emotion = max(emotions.items(), key=lambda x: x[1])
                if self.language == 'zh':
                    stats.append(f"{self.get_label('dominant_emotion')}: {dominant_emotion[0]} ({dominant_emotion[1]}æ¬¡)")
                else:
                    stats.append(f"{self.get_label('dominant_emotion')}: {dominant_emotion[0]} ({dominant_emotion[1]} times)")
        
        # å™äº‹ç»Ÿè®¡
        narrative_data = analysis_result.get('narrative', {})
        if narrative_data:
            scenes = narrative_data.get('scenes', [])
            if scenes:
                if self.language == 'zh':
                    stats.append(f"{self.get_label('scenes_detected')}: {len(scenes)}ä¸ª")
                else:
                    stats.append(f"{self.get_label('scenes_detected')}: {len(scenes)}")
        
        # è®¾ç½®æ ‡é¢˜
        ax.set_title(self.get_label('summary_stats_title'), fontfamily=font_family, fontsize=12, pad=20)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if not stats:
            ax.text(0.5, 0.5, self.get_label('no_stats_data'), ha='center', va='center', 
                   transform=ax.transAxes, fontfamily=font_family, fontsize=14)
        else:
            # åˆ›å»ºä¸€ä¸ªç¾è§‚çš„ç»Ÿè®¡é¢æ¿ - çº¯æ–‡å­—ç‰ˆæœ¬
            title_emoji = 'ğŸ“Š ' if self.language == 'zh' else 'ğŸ“Š '
            ax.text(0.1, 0.9, f'{title_emoji}{self.get_label("analysis_summary")}', 
                   transform=ax.transAxes, fontfamily=font_family, fontsize=14, fontweight='bold')
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼Œæ¯è¡Œä¸€ä¸ª
            for i, stat in enumerate(stats):
                y_pos = 0.75 - i * 0.1
                if y_pos > 0.1:  # ç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œ
                    ax.text(0.1, y_pos, f"â€¢ {stat}", transform=ax.transAxes,
                           fontfamily=font_family, fontsize=11)
        
        # è®¾ç½®å›¾è¡¨è¾¹ç•Œ
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)


# å…¨å±€å¯è§†åŒ–å®ä¾‹ï¼ˆé»˜è®¤ä¸­æ–‡ï¼‰
visualizer = AdvancedVisualizer(language='zh')
