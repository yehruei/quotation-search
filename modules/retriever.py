from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import warnings
from rank_bm25 import BM25Okapi

def load_sentence_transformer_model(model_path):
    """
    å®‰å…¨åŠ è½½SentenceTransformeræ¨¡åž‹ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜
    """
    # æ£€æŸ¥model_pathæ˜¯å¦ä¸ºNoneæˆ–ç©º
    if model_path is None or model_path == "":
        print("âš ï¸ æ¨¡åž‹è·¯å¾„ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ¨¡åž‹")
        model_path = 'sentence-transformers/all-MiniLM-L6-v2'
    
    # å¦‚æžœä¼ å…¥çš„æ˜¯æ¨¡åž‹åç§°è€Œä¸æ˜¯è·¯å¾„ï¼Œç›´æŽ¥å°è¯•åŠ è½½
    if model_path.startswith('sentence-transformers/') or model_path.startswith('all-'):
        try:
            print(f"ðŸ” ä½¿ç”¨æ¨¡åž‹åç§°åŠ è½½: {model_path}")
            model = SentenceTransformer(model_path, device='cpu', trust_remote_code=False)
            print(f"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸ: {model_path}")
            return model
        except Exception as e:
            print(f"âš ï¸ æ¨¡åž‹ {model_path} åŠ è½½å¤±è´¥: {e}")
            return None
    
    # å¦‚æžœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨
    import os
    if os.path.exists(model_path):
        try:
            print(f"ðŸ“š æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡åž‹: {model_path}")
            model = SentenceTransformer(model_path)
            print(f"âœ… æœ¬åœ°æ¨¡åž‹åŠ è½½æˆåŠŸ")
            return model
        except Exception as e:
            print(f"âš ï¸ æœ¬åœ°æ¨¡åž‹ {model_path} åŠ è½½å¤±è´¥: {e}")
    
    # å¦‚æžœæœ¬åœ°è·¯å¾„ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ¨¡åž‹åç§°
    model_names = [
        'sentence-transformers/all-MiniLM-L6-v2',
        'all-MiniLM-L6-v2'
    ]
    
    for model_name in model_names:
        try:
            print(f"ðŸ” å°è¯•é€šè¿‡æ¨¡åž‹åç§°åŠ è½½: {model_name}")
            model = SentenceTransformer(model_name, device='cpu', trust_remote_code=False)
            print(f"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸ: {model_name}")
            return model
        except Exception as e:
            print(f"âš ï¸ æ¨¡åž‹ {model_name} åŠ è½½å¤±è´¥: {e}")
            
            # å°è¯•æ¸…ç†ç¼“å­˜åŽé‡æ–°åŠ è½½
            try:
                import shutil
                cache_dirs = [
                    os.path.expanduser("~/.cache/torch/sentence_transformers"),
                    os.path.expanduser("~/.cache/huggingface/transformers"),
                    os.path.expanduser("~/.cache/huggingface/hub")
                ]
                
                for cache_dir in cache_dirs:
                    if os.path.exists(cache_dir):
                        print(f"ðŸ§¹ æ¸…ç†ç¼“å­˜ç›®å½•: {cache_dir}")
                        try:
                            shutil.rmtree(cache_dir)
                        except:
                            pass
                
                print(f"ðŸ”„ æ¸…ç†ç¼“å­˜åŽé‡è¯•åŠ è½½: {model_name}")
                model = SentenceTransformer(model_name, device='cpu', trust_remote_code=False)
                print(f"âœ… æ¸…ç†ç¼“å­˜åŽæ¨¡åž‹åŠ è½½æˆåŠŸ: {model_name}")
                return model
                
            except Exception as e2:
                print(f"âš ï¸ æ¸…ç†ç¼“å­˜åŽä»ç„¶å¤±è´¥: {e2}")
                continue
    
    print("ðŸŒ æœ¬åœ°ç¼“å­˜ä¸å¯ç”¨ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½...")
    
    # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä¸‹è½½åœ¨çº¿æ¨¡åž‹ï¼ˆå¸¦SSLå¤„ç†ï¼‰
    try:
        # ç¦ç”¨SSLéªŒè¯ä½œä¸ºä¸´æ—¶è§£å†³æ–¹æ¡ˆ
        import ssl
        ssl._create_default_https_context = ssl._create_unverified_context
        
        model_name = 'sentence-transformers/all-MiniLM-L6-v2'
        print(f"ðŸŒ å°è¯•åœ¨çº¿ä¸‹è½½: {model_name}")
        model = SentenceTransformer(model_name)
        print(f"âœ… åœ¨çº¿æ¨¡åž‹ä¸‹è½½å’ŒåŠ è½½æˆåŠŸ: {model_name}")
        return model
        
    except Exception as e:
        print(f"âš ï¸ åœ¨çº¿ä¸‹è½½å¤±è´¥: {e}")
    
    # æœ€åŽçš„å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•è¯æ±‡æ¨¡åž‹
    print("ðŸš¨ æ‰€æœ‰æ¨¡åž‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºäºŽè¯æ±‡çš„ç®€å•æ¨¡åž‹")
    return None

class SimpleVocabModel:
    """
    ç®€å•çš„åŸºäºŽè¯æ±‡çš„è¯­ä¹‰æ¨¡åž‹ï¼Œä½œä¸ºSentenceTransformerçš„å¤‡ç”¨æ–¹æ¡ˆ
    """
    def __init__(self):
        self.vocab = {}
        
    def encode(self, texts):
        """ç®€å•çš„è¯æ±‡ç¼–ç """
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = []
        for text in texts:
            words = text.lower().split()
            # åˆ›å»ºç®€å•çš„è¯é¢‘å‘é‡
            word_counts = {}
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
            
            # è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å‘é‡
            # ä½¿ç”¨å¸¸è§çš„1000ä¸ªè¯æ±‡ç»´åº¦
            vocab_size = 1000
            embedding = np.zeros(vocab_size)
            
            for word, count in word_counts.items():
                # ç®€å•çš„å“ˆå¸Œå‡½æ•°æ˜ å°„è¯æ±‡åˆ°ç»´åº¦
                dim = hash(word) % vocab_size
                embedding[dim] = count
                
            # å½’ä¸€åŒ–
            if np.linalg.norm(embedding) > 0:
                embedding = embedding / np.linalg.norm(embedding)
                
            embeddings.append(embedding)
        
        return np.array(embeddings)

class EmbeddingRetriever:
    def __init__(self, model_path):
        self.model = load_sentence_transformer_model(model_path)
        self.use_simple_model = self.model is None
        if self.use_simple_model:
            self.model = SimpleVocabModel()
            print("ðŸ”„ ä½¿ç”¨ç®€å•è¯æ±‡æ¨¡åž‹è¿›è¡Œè¯­ä¹‰æœç´¢")
        self.keywords = []

    def retrieve(self, text_chunks, k):
        query = " ".join(self.keywords)
        
        try:
            query_embedding = self.model.encode([query])
            
            texts = [chunk['content'] for chunk in text_chunks]
            text_embeddings = self.model.encode(texts)
            
            similarities = cosine_similarity(query_embedding, text_embeddings)[0]
            
            results = []
            for i, score in enumerate(similarities):
                results.append({
                    'chunk_id': text_chunks[i].get('id', f'chunk_{i}'),
                    'content': text_chunks[i]['content'],
                    'page_num': text_chunks[i].get('page_num', 1),
                    'word_count': len(text_chunks[i]['content'].split()),
                    'index': i,
                    'similarity_score': score,
                    'found_keywords': self.keywords,
                    'model_type': 'simple_vocab' if self.use_simple_model else 'sentence_transformer'
                })
                
            return sorted(results, key=lambda x: x['similarity_score'], reverse=True)[:k]
            
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°BM25æœç´¢
            print("ðŸ”„ é™çº§åˆ°BM25æœç´¢")
            bm25_retriever = BM25Retriever()
            bm25_retriever.keywords = self.keywords
            return bm25_retriever.retrieve(text_chunks, k)

class BM25Retriever:
    def __init__(self):
        self.keywords = []
        self.bm25 = None

    def retrieve(self, text_chunks, k):
        tokenized_corpus = [chunk['content'].split(" ") for chunk in text_chunks]
        self.bm25 = BM25Okapi(tokenized_corpus)
        
        query = " ".join(self.keywords)
        tokenized_query = query.split(" ")
        
        doc_scores = self.bm25.get_scores(tokenized_query)
        
        top_n_indices = np.argsort(doc_scores)[::-1][:k]
        
        results = []
        for i in top_n_indices:
            results.append({
                'chunk_id': text_chunks[i].get('id', f'chunk_{i}'),
                'content': text_chunks[i]['content'],
                'page_num': text_chunks[i].get('page_num', 1),
                'word_count': len(text_chunks[i]['content'].split()),
                'index': i,
                'bm25_score': doc_scores[i],
                'found_keywords': self.keywords
            })
            
        return results

def search(query, texts, model_path):
    """Performs a semantic search for a query within a list of texts."""
    model = load_sentence_transformer_model(model_path)
    
    if model is None:
        print("ðŸ”„ ä½¿ç”¨ç®€å•è¯æ±‡æ¨¡åž‹è¿›è¡Œæœç´¢")
        model = SimpleVocabModel()
    
    try:
        query_embedding = model.encode([query])
        text_embeddings = model.encode(texts)

        similarities = cosine_similarity(query_embedding, text_embeddings)[0]

        results = []
        for i, score in enumerate(similarities):
            results.append({
                'score': score, 
                'text': texts[i],
                'page_num': i + 1,  # ç®€å•çš„é¡µç åˆ†é…
                'index': i
            })

        return sorted(results, key=lambda x: x['score'], reverse=True)
        
    except Exception as e:
        print(f"âš ï¸ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
        # é™çº§åˆ°ç®€å•çš„æ–‡æœ¬åŒ¹é…
        print("ðŸ”„ é™çº§åˆ°ç®€å•æ–‡æœ¬åŒ¹é…")
        results = []
        query_words = query.lower().split()
        
        for i, text in enumerate(texts):
            text_lower = text.lower()
            score = 0
            for word in query_words:
                score += text_lower.count(word)
            
            # å½’ä¸€åŒ–åˆ†æ•°
            max_possible_score = len(query_words) * max(1, len(text.split()) // 10)
            normalized_score = score / max_possible_score if max_possible_score > 0 else 0
            
            results.append({
                'score': normalized_score,
                'text': text,
                'page_num': i + 1,
                'index': i
            })
        
        return sorted(results, key=lambda x: x['score'], reverse=True)
