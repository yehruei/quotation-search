#!/usr/bin/env python3
"""
Quotation Search App - Production Ready Edition
ä¸ºå¤šç”¨æˆ·æœåŠ¡å™¨éƒ¨ç½²ä¼˜åŒ–çš„ç‰ˆæœ¬
"""

import streamlit as st
import os
import sys
import pandas as pd
import tempfile
import traceback
import random
import re
import time
import hashlib
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
import threading

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

from modules.text_processor import process_pdf
from modules.retriever import search as standard_search
from modules.reranker import create_reranker
from modules.keyword_expander import KeywordExpander
from modules.literary_analyzer import LiteraryAnalyzer
from modules.theme_analyzer import LiteraryThemeAnalyzer
from modules.visualizer import AdvancedVisualizer

# å…¨å±€é…ç½®
CONFIG = {
    'MAX_FILE_SIZE': 50 * 1024 * 1024,  # 50MB
    'MAX_CONCURRENT_USERS': 10,
    'CACHE_TTL': 3600,  # 1å°æ—¶
    'MAX_RESULTS_PER_USER': 50,
    'RATE_LIMIT_PER_HOUR': 10,
    'UPLOAD_DIR': 'uploads',
    'CACHE_DIR': 'cache',
    'TEMP_DIR': 'temp'
}

# å…¨å±€å˜é‡ç®¡ç†
class GlobalState:
    def __init__(self):
        self._model_path = None
        self._model_loaded = False
        self._loading_lock = threading.Lock()
        self._user_sessions = {}
        self._rate_limits = {}
        self._executor = ThreadPoolExecutor(max_workers=CONFIG['MAX_CONCURRENT_USERS'])
    
    def get_model_path(self):
        if not self._model_loaded:
            with self._loading_lock:
                if not self._model_loaded:
                    self._model_path = self._load_model_path()
                    self._model_loaded = True
        return self._model_path
    
    def _load_model_path(self):
        """åŠ è½½æ¨¡å‹è·¯å¾„ - ä¼˜åŒ–ç‰ˆ"""
        try:
            return get_model_path()
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
    
    def check_rate_limit(self, user_id):
        """æ£€æŸ¥ç”¨æˆ·é€Ÿç‡é™åˆ¶"""
        now = datetime.now()
        hour_key = now.strftime("%Y%m%d%H")
        
        if user_id not in self._rate_limits:
            self._rate_limits[user_id] = {}
        
        if hour_key not in self._rate_limits[user_id]:
            self._rate_limits[user_id][hour_key] = 0
        
        # æ¸…ç†æ—§æ•°æ®
        old_keys = [k for k in self._rate_limits[user_id].keys() if k < (now - timedelta(hours=2)).strftime("%Y%m%d%H")]
        for key in old_keys:
            del self._rate_limits[user_id][key]
        
        current_requests = self._rate_limits[user_id][hour_key]
        if current_requests >= CONFIG['RATE_LIMIT_PER_HOUR']:
            return False
        
        self._rate_limits[user_id][hour_key] += 1
        return True

# å…¨å±€çŠ¶æ€å®ä¾‹
global_state = GlobalState()

# ç¼“å­˜ç®¡ç†
@st.cache_data(ttl=CONFIG['CACHE_TTL'], max_entries=100)
def cached_process_pdf(file_hash, chunk_size, chunk_overlap, remove_stopwords):
    """ç¼“å­˜çš„PDFå¤„ç†"""
    cache_file = os.path.join(CONFIG['CACHE_DIR'], f"pdf_{file_hash}.cache")
    
    if os.path.exists(cache_file):
        try:
            import pickle
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
    
    return None

def save_pdf_cache(file_hash, texts, chunk_size, chunk_overlap, remove_stopwords):
    """ä¿å­˜PDFå¤„ç†ç»“æœåˆ°ç¼“å­˜"""
    try:
        import pickle
        cache_file = os.path.join(CONFIG['CACHE_DIR'], f"pdf_{file_hash}.cache")
        with open(cache_file, 'wb') as f:
            pickle.dump(texts, f)
        logger.info(f"PDFç¼“å­˜å·²ä¿å­˜: {file_hash}")
    except Exception as e:
        logger.error(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

def get_file_hash(file_bytes):
    """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
    return hashlib.md5(file_bytes).hexdigest()

def get_user_session_id():
    """è·å–ç”¨æˆ·ä¼šè¯ID"""
    if 'session_id' not in st.session_state:
        st.session_state.session_id = hashlib.md5(
            f"{time.time()}_{random.randint(1000, 9999)}".encode()
        ).hexdigest()
    return st.session_state.session_id

@st.cache_resource(ttl=CONFIG['CACHE_TTL'])
def get_model_path():
    """
    è·å–æ¨¡å‹è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›Noneä½¿ç”¨ç®€å•æ¨¡å‹
    """
    import os
    
    # æ£€æŸ¥é¡¹ç›®å†…çš„æœ¬åœ°æ¨¡å‹ä½ç½®
    base_path = os.getcwd()
    local_paths = [
        os.path.join(base_path, 'final_model_data/sentence_transformer_model'),
        os.path.join(base_path, 'models_cache/models--sentence-transformers--all-MiniLM-L6-v2'),
        os.path.join(base_path, 'models_cache/sentence-transformers--all-MiniLM-L6-v2'),
    ]
    
    # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å¿…è¦æ–‡ä»¶
    for local_path in local_paths:
        if os.path.exists(local_path):
            # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
            required_files = ['config.json']
            config_file = os.path.join(local_path, 'config.json')
            if os.path.exists(config_file):
                print(f"âœ… æ‰¾åˆ°æœ¬åœ°æ¨¡å‹: {local_path}")
                return local_path
            else:
                print(f"âš ï¸ è·¯å¾„å­˜åœ¨ä½†ç¼ºå°‘config.json: {local_path}")
    
    # æ£€æŸ¥ç”¨æˆ·çš„sentence-transformersç¼“å­˜ç›®å½•
    import os
    home_cache_paths = [
        os.path.expanduser("~/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2"),
        os.path.expanduser("~/.cache/huggingface/transformers/models--sentence-transformers--all-MiniLM-L6-v2"),
    ]
    
    for cache_path in home_cache_paths:
        if os.path.exists(cache_path):
            config_file = os.path.join(cache_path, 'config.json')
            if os.path.exists(config_file):
                print(f"âœ… æ‰¾åˆ°ç³»ç»Ÿç¼“å­˜æ¨¡å‹: {cache_path}")
                return cache_path
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œè¿”å›Noneä½¿ç”¨ç®€å•æ¨¡å‹
    print("ğŸ”„ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†ä½¿ç”¨ç®€å•è¯æ±‡æ¨¡å‹ï¼ˆæ— éœ€ç½‘ç»œè¿æ¥ï¼‰")
    return None

def display_results(results):
    print(f"DEBUG: display_results è¢«è°ƒç”¨ï¼Œresults é•¿åº¦: {len(results) if results else 'ç©ºæˆ–None'}")
    if results:
        print(f"DEBUG: ç¬¬ä¸€ä¸ªç»“æœå†…å®¹: {results[0]}")
    
    if not results:
        st.warning("æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")
        print("DEBUG: æ˜¾ç¤º'æœªæ‰¾åˆ°ç›¸å…³ç»“æœ'è­¦å‘Š")
        return
    
    # Display summary
    if len(results) > 0:
        st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
        print(f"DEBUG: æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯ï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
    
    for i, result in enumerate(results, 1):
        st.markdown("---")
        score = result.get('score', 0.0)
        is_fallback = result.get('is_fallback', False)
        search_method = result.get('method', '')
        
        # Score display with user-friendly labels (using 30-100 range)
        if score >= 80:
            score_color = "ğŸŸ¢"
            score_label = "é«˜åº¦ç›¸å…³"
            score_class = "score-high"
        elif score >= 65:
            score_color = "ğŸŸ¡"
            score_label = "è¾ƒä¸ºç›¸å…³"
            score_class = "score-medium"
        elif score >= 45:
            score_color = "ğŸŸ "
            score_label = "ä¸€èˆ¬ç›¸å…³"
            score_class = "score-medium"
        else:
            score_color = "ğŸ”´"
            score_label = "ç›¸å…³æ€§è¾ƒä½"
            score_class = "score-low"
        
        # ä¸ºè¡¥å……ç»“æœå’Œä¸åŒæœç´¢æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è¯†
        if is_fallback:
            fallback_indicator = " ğŸ“ è¡¥å……ç»“æœ"
            score_label = "è¡¥å……å†…å®¹"
            score_color = "ğŸ”"
        else:
            fallback_indicator = ""
        
        # æœç´¢æ–¹æ³•æ ‡è¯†
        if search_method:
            method_indicator = f" [{search_method}]"
        else:
            method_indicator = ""
        
        # Get page information 
        page_num = result.get('page_num', 'Unknown')
        
        # Use simpler, more reliable display approach
        st.markdown("---")
        
        # Result header with score badge
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown(f"**ç»“æœ {i}**")
            st.caption(f"ğŸ“„ ç¬¬ {page_num} é¡µ")
        with col2:
            if score >= 80:
                st.markdown(f"ğŸŸ¢ **{score:.0f}åˆ†** (é«˜åº¦ç›¸å…³)")
            elif score >= 65:
                st.markdown(f"ğŸŸ¡ **{score:.0f}åˆ†** (è¾ƒä¸ºç›¸å…³)")
            elif score >= 45:
                st.markdown(f"ğŸŸ  **{score:.0f}åˆ†** (ä¸€èˆ¬ç›¸å…³)")
            else:
                st.markdown(f"ğŸ”´ **{score:.0f}åˆ†** (ç›¸å…³æ€§è¾ƒä½)")
            
            # Add fallback and method indicators
            if fallback_indicator:
                st.caption(f"ğŸ“ {fallback_indicator}")
            if method_indicator:
                st.caption(f"ğŸ”§ {method_indicator}")
        
        # Enhanced content display with highlighting
        content = result.get('text', '')
        found_keywords = result.get('found_keywords', [])
        
        # æ™ºèƒ½åˆ†å‰²å†…å®¹ä»¥çªå‡ºæ˜¾ç¤ºä¸»è¦åŒ¹é…éƒ¨åˆ†
        highlighted_content = highlight_main_content(content, found_keywords)
        
        # è°ƒè¯•HTMLå†…å®¹
        print(f"DEBUG: highlighted_content å‰100å­—ç¬¦: {highlighted_content[:100]}")
        
        # æ˜¾ç¤ºå¸¦æœ‰é«˜äº®çš„å†…å®¹ - æ”¹è¿›HTMLæ¸²æŸ“ç¨³å®šæ€§
        try:
            # éªŒè¯HTMLå†…å®¹å®Œæ•´æ€§
            if is_valid_html_content(highlighted_content):
                st.markdown(highlighted_content, unsafe_allow_html=True)
                print("âœ… HTMLå†…å®¹æ­£å¸¸æ¸²æŸ“")
            else:
                print("âš ï¸ HTMLå†…å®¹éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                raise ValueError("HTMLéªŒè¯å¤±è´¥")
        except Exception as e:
            print(f"HTMLæ¸²æŸ“é”™è¯¯: {e}")
            # å›é€€åˆ°ç®€å•çš„æ–‡æœ¬æ˜¾ç¤ºä½†ä¿ç•™åŸºæœ¬æ ¼å¼
            st.markdown("**ğŸ“„ å†…å®¹ï¼š**")
            if found_keywords:
                # ç®€å•çš„å…³é”®è¯é«˜äº®ï¼ˆéHTMLï¼‰
                display_content = content
                for keyword in found_keywords[:3]:  # åªå¤„ç†å‰3ä¸ªå…³é”®è¯
                    if keyword in display_content:
                        display_content = display_content.replace(keyword, f"**{keyword}**")
                st.markdown(display_content)
                st.caption(f"ğŸ” åŒ¹é…å…³é”®è¯: {', '.join(found_keywords[:5])}")
            else:
                st.markdown(content)
            print("âœ… å›é€€åˆ°ç®€å•æ–‡æœ¬æ˜¾ç¤º")
        
        # Additional metadata if available
        if found_keywords:
            st.markdown(f"**ğŸ” åŒ¹é…å…³é”®è¯:** {', '.join(found_keywords[:5])}")  # Show first 5 keywords

def is_valid_html_content(html_content):
    """
    éªŒè¯HTMLå†…å®¹çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
    
    Args:
        html_content: å¾…éªŒè¯çš„HTMLå†…å®¹
    
    Returns:
        bool: HTMLå†…å®¹æ˜¯å¦æœ‰æ•ˆ
    """
    if not html_content or not isinstance(html_content, str):
        return False
    
    # æ£€æŸ¥åŸºæœ¬çš„HTMLç»“æ„
    if not ('<div' in html_content and '</div>' in html_content):
        return False
    
    # è®¡ç®—æ ‡ç­¾åŒ¹é…
    import re
    
    # æå–æ‰€æœ‰HTMLæ ‡ç­¾
    opening_tags = re.findall(r'<(\w+)[^>]*>', html_content)
    closing_tags = re.findall(r'</(\w+)>', html_content)
    
    # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦é…å¯¹ï¼ˆå¿½ç•¥è‡ªé—­åˆæ ‡ç­¾å¦‚<br/>ï¼‰
    self_closing_tags = {'br', 'hr', 'img', 'input', 'meta', 'link'}
    
    tag_stack = []
    for tag in opening_tags:
        if tag not in self_closing_tags:
            tag_stack.append(tag)
    
    for tag in closing_tags:
        if tag_stack and tag_stack[-1] == tag:
            tag_stack.pop()
        else:
            print(f"âš ï¸ HTMLæ ‡ç­¾ä¸åŒ¹é…: æœŸæœ› {tag_stack[-1] if tag_stack else 'None'}, å®é™… {tag}")
            return False
    
    # å¦‚æœè¿˜æœ‰æœªåŒ¹é…çš„å¼€æ”¾æ ‡ç­¾ï¼Œåˆ™æ— æ•ˆ
    if tag_stack:
        print(f"âš ï¸ æœªå…³é—­çš„HTMLæ ‡ç­¾: {tag_stack}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ½œåœ¨çš„æœ‰å®³å†…å®¹
    dangerous_patterns = [
        r'<script[^>]*>',
        r'javascript:',
        r'on\w+\s*=',  # äº‹ä»¶å¤„ç†å™¨
        r'<iframe[^>]*>',
        r'<object[^>]*>',
        r'<embed[^>]*>'
    ]
    
    for pattern in dangerous_patterns:
        if re.search(pattern, html_content, re.IGNORECASE):
            print(f"âš ï¸ æ£€æµ‹åˆ°æ½œåœ¨å±é™©çš„HTMLå†…å®¹")
            return False
    
    return True

def highlight_main_content(content, keywords):
    """
    æ™ºèƒ½é«˜äº®æ˜¾ç¤ºä¸»è¦åŒ¹é…å†…å®¹å’Œä¸Šä¸‹æ–‡ - æ”¹è¿›ç‰ˆ
    
    Args:
        content: åŸå§‹æ–‡æœ¬å†…å®¹
        keywords: åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨
    
    Returns:
        å¸¦æœ‰HTMLé«˜äº®æ ‡è®°çš„å†…å®¹
    """
    # ç¡®ä¿è¾“å…¥å®‰å…¨
    if not content or not isinstance(content, str):
        return create_fallback_html("å†…å®¹ä¸ºç©º")
    
    if not keywords:
        return create_simple_content_html(content)
    
    try:
        # ç¡®ä¿å†…å®¹å®‰å…¨ - è½¬ä¹‰HTMLç‰¹æ®Šå­—ç¬¦
        import html
        safe_content = html.escape(content)
        
        # æ”¹è¿›çš„å¥å­åˆ†å‰²ï¼Œæ”¯æŒä¸­è‹±æ–‡
        import re
        
        # æ›´æ™ºèƒ½çš„å¥å­åˆ†å‰²
        sentence_pattern = r'[.!?ã€‚ï¼ï¼Ÿ]+\s*'
        sentences = re.split(sentence_pattern, safe_content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # å¦‚æœåˆ†å‰²åå¥å­å¤ªå°‘ï¼Œå°è¯•å…¶ä»–åˆ†å‰²æ–¹å¼
        if len(sentences) <= 1:
            sentences = [s.strip() for s in safe_content.split('\n') if s.strip()]
        
        if len(sentences) == 1 and len(safe_content) > 200:
            parts = re.split(r'[,;ï¼Œï¼›]\s*', safe_content)
            if len(parts) > 1:
                sentences = [s.strip() for s in parts if s.strip()]
        
        # æ‰¾å‡ºåŒ…å«å…³é”®è¯çš„ä¸»è¦å¥å­
        main_sentences = []
        context_sentences = []
        
        for i, sentence in enumerate(sentences):
            sentence_lower = sentence.lower()
            has_keyword = False
            matched_keywords = []
            
            for keyword in keywords:
                if keyword.lower() in sentence_lower:
                    has_keyword = True
                    matched_keywords.append(keyword)
            
            if has_keyword:
                main_sentences.append((i, sentence, matched_keywords))
            else:
                context_sentences.append((i, sentence))
        
        # æ„å»ºHTMLå†…å®¹
        if not main_sentences:
            # æ²¡æœ‰æ˜ç¡®åŒ¹é…ï¼Œæ£€æŸ¥éƒ¨åˆ†åŒ¹é…
            partial_matches = []
            for i, sentence in enumerate(sentences):
                sentence_lower = sentence.lower()
                for keyword in keywords:
                    if any(word in sentence_lower for word in keyword.lower().split()):
                        partial_matches.append((i, sentence))
                        break
            
            if partial_matches:
                main_text = '. '.join([sent for _, sent in partial_matches[:2]]) + '.'
                return create_highlighted_html(main_text, [], "ç›¸å…³å†…å®¹")
            else:
                return create_simple_content_html(safe_content)
        else:
            # æœ‰æ˜ç¡®çš„å…³é”®è¯åŒ¹é…
            main_text = '. '.join([sent for _, sent, _ in main_sentences])
            if not main_text.endswith('.'):
                main_text += '.'
            
            # é«˜äº®å…³é”®è¯
            for keyword in keywords:
                escaped_keyword = html.escape(keyword)
                pattern = r'\b' + re.escape(escaped_keyword) + r'\b'
                highlight_span = f'<mark style="background-color: #ffeb3b; padding: 2px 4px; border-radius: 3px; font-weight: bold; color: #333;">{escaped_keyword}</mark>'
                main_text = re.sub(pattern, highlight_span, main_text, flags=re.IGNORECASE)
            
            # æ”¶é›†åŒ¹é…çš„å…³é”®è¯
            all_matched_keywords = []
            for _, _, matched_kw in main_sentences:
                all_matched_keywords.extend(matched_kw)
            unique_matched = list(set(all_matched_keywords))
            
            # æ·»åŠ ä¸Šä¸‹æ–‡
            context_text = ""
            if context_sentences and len(context_sentences) <= 2:
                context_text = '. '.join([sent for _, sent in context_sentences])
                if context_text and not context_text.endswith('.'):
                    context_text += '.'
            
            return create_comprehensive_html(main_text, context_text, unique_matched)
            
    except Exception as e:
        print(f"HTMLç”Ÿæˆé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return create_fallback_html(f"å¤„ç†å‡ºé”™: {str(e)}")

def create_simple_content_html(content):
    """åˆ›å»ºç®€å•å†…å®¹çš„HTML"""
    return f'''
    <div style="background-color: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #6c757d; font-family: 'PingFang SC', 'Microsoft YaHei', sans-serif;">
        <p style="margin: 0; color: #495057; line-height: 1.6;">{content}</p>
    </div>
    '''

def create_highlighted_html(main_text, keywords, title="ä¸»è¦åŒ¹é…å†…å®¹"):
    """åˆ›å»ºé«˜äº®å†…å®¹çš„HTML"""
    return f'''
    <div style="background-color: #ffffff; padding: 18px; border-radius: 10px; margin: 12px 0; border: 1px solid #e0e0e0; box-shadow: 0 2px 8px rgba(0,0,0,0.08); font-family: 'PingFang SC', 'Microsoft YaHei', sans-serif;">
        <div style="background-color: #d1ecf1; padding: 12px; border-radius: 6px;">
            <strong style="color: #0c5460;">ğŸ¯ {title}:</strong>
            <p style="margin: 5px 0; color: #0c5460; line-height: 1.6;">{main_text}</p>
        </div>
    </div>
    '''

def create_comprehensive_html(main_text, context_text, matched_keywords):
    """åˆ›å»ºç»¼åˆå†…å®¹çš„HTML"""
    keyword_count = len(matched_keywords)
    
    html_parts = [f'''
    <div style="background-color: #ffffff; padding: 18px; border-radius: 10px; margin: 12px 0; border: 1px solid #e0e0e0; box-shadow: 0 2px 8px rgba(0,0,0,0.08); font-family: 'PingFang SC', 'Microsoft YaHei', sans-serif;">
        <div style="background-color: #e8f5e8; padding: 14px; border-radius: 8px; margin-bottom: 10px; border-left: 5px solid #4caf50; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
            <div style="display: flex; align-items: center; margin-bottom: 8px;">
                <strong style="color: #2e7d32; font-size: 16px;">ğŸ¯ ä¸»è¦åŒ¹é…å†…å®¹</strong>
                <span style="background-color: #4caf50; color: white; padding: 2px 8px; border-radius: 12px; font-size: 12px; margin-left: 10px;">
                    {keyword_count} ä¸ªå…³é”®è¯åŒ¹é…
                </span>
            </div>
            <p style="margin: 0; color: #1b5e20; line-height: 1.7; font-size: 15px;">{main_text}</p>
        </div>''']
    
    if context_text:
        html_parts.append(f'''
        <div style="background-color: #fff3e0; padding: 12px; border-radius: 6px; border-left: 4px solid #ff9800;">
            <strong style="color: #f57c00;">ğŸ“„ ç›¸å…³ä¸Šä¸‹æ–‡:</strong>
            <p style="margin: 5px 0; color: #e65100; line-height: 1.6; font-size: 14px;">{context_text}</p>
        </div>''')
    
    html_parts.append('    </div>')
    
    return ''.join(html_parts)

def create_fallback_html(error_msg):
    """åˆ›å»ºé”™è¯¯å›é€€çš„HTML"""
    return f'''
    <div style="background-color: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #dc3545; font-family: 'PingFang SC', 'Microsoft YaHei', sans-serif;">
        <p style="margin: 0; color: #721c24; line-height: 1.6;">âš ï¸ {error_msg}</p>
    </div>
    '''

def simple_highlight_content(content, keywords):
    """
    ç®€åŒ–ç‰ˆé«˜äº®å‡½æ•° - å¤‡ç”¨æ–¹æ¡ˆ
    """
    if not keywords or not content:
        return content
    
    # ç®€å•çš„æ–‡æœ¬é«˜äº®ï¼Œä½¿ç”¨Markdownè¯­æ³•
    display_content = content
    for keyword in keywords[:5]:  # æœ€å¤šå¤„ç†5ä¸ªå…³é”®è¯
        # ä½¿ç”¨Markdownçš„ç²—ä½“è¯­æ³•è¿›è¡Œé«˜äº®
        display_content = display_content.replace(keyword, f"**{keyword}**")
    
    return display_content

# --- UI Layout ---

def main():
    """ä¸»å‡½æ•° - ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–ç‰ˆ"""
    logger.info("ğŸš€ åº”ç”¨ç¨‹åºå¯åŠ¨...")
    
    # è·å–ç”¨æˆ·ä¼šè¯ID
    user_id = get_user_session_id()
    
    # æ£€æŸ¥é€Ÿç‡é™åˆ¶
    if not global_state.check_rate_limit(user_id):
        st.error("âš ï¸ æ‚¨å·²è¾¾åˆ°æ¯å°æ—¶è¯·æ±‚é™åˆ¶ã€‚è¯·ç¨åå†è¯•ã€‚")
        st.info(f"æ¯å°æ—¶æœ€å¤šå…è®¸ {CONFIG['RATE_LIMIT_PER_HOUR']} æ¬¡è¯·æ±‚")
        return
    
    try:
        st.set_page_config(
            page_title="Quotation Search - æ™ºèƒ½æ–‡æœ¬åˆ†æå¹³å°", 
            layout="wide", 
            initial_sidebar_state="collapsed",
            page_icon="ğŸ“š"
        )
        logger.info("âœ… Streamlité¡µé¢é…ç½®å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ é¡µé¢é…ç½®å¤±è´¥: {e}")
        st.error("é¡µé¢é…ç½®å¤±è´¥ï¼Œè¯·åˆ·æ–°é‡è¯•")
        return

    # æ˜¾ç¤ºæœåŠ¡çŠ¶æ€
    with st.sidebar:
        st.header("ğŸ“Š æœåŠ¡çŠ¶æ€")
        st.metric("å½“å‰ç”¨æˆ·", len(global_state._user_sessions))
        st.metric("ä»Šæ—¥è¯·æ±‚", global_state._rate_limits.get(user_id, {}).get(datetime.now().strftime("%Y%m%d%H"), 0))
        
        # ç³»ç»Ÿä¿¡æ¯
        with st.expander("ğŸ”§ ç³»ç»Ÿä¿¡æ¯"):
            import psutil
            st.write(f"CPUä½¿ç”¨ç‡: {psutil.cpu_percent()}%")
            st.write(f"å†…å­˜ä½¿ç”¨ç‡: {psutil.virtual_memory().percent}%")
            st.write(f"ç£ç›˜ä½¿ç”¨ç‡: {psutil.disk_usage('/').percent}%")

    # æ·»åŠ æ€§èƒ½ç›‘æ§çš„CSSæ ·å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
    st.markdown("""
    <style>
    .stApp {
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', sans-serif;
        background-color: #f8f9fa;
    }
    .main .block-container {
        background: white;
        border-radius: 12px;
        padding: 2rem;
        margin-top: 1rem;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', sans-serif;
    }
    /* ç®€åŒ–çš„æ ·å¼ä»¥æé«˜æ€§èƒ½ */
    .stButton > button {
        width: 100%;
        margin-top: 1rem;
        background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
        border: none;
        border-radius: 8px;
        padding: 0.75rem 2rem;
        font-weight: 600;
        color: white;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # ä¸»æ ‡é¢˜
    st.markdown('<div class="main-header">', unsafe_allow_html=True)
    st.title("ğŸ“š æ™ºèƒ½æ–‡æœ¬åˆ†æå¹³å°")
    st.markdown("ä¸“ä¸šçš„æ–‡å­¦ä½œå“æœç´¢ã€åˆ†æå’Œå¯è§†åŒ–å·¥å…·")
    st.markdown('</div>', unsafe_allow_html=True)
    
    # å°è¯•è·å–æ¨¡å‹è·¯å¾„
    try:
        model_path = global_state.get_model_path()
        if model_path:
            st.success("âœ… AIæ¨¡å‹å·²å°±ç»ª")
        else:
            st.warning("âš ï¸ ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼ˆåŠŸèƒ½å¯èƒ½å—é™ï¼‰")
        logger.info(f"âœ… æ¨¡å‹çŠ¶æ€æ£€æŸ¥å®Œæˆ: {model_path}")
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")
        st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·è”ç³»ç®¡ç†å‘˜")
        model_path = None

    # ä¸»è¦é…ç½®åŒºåŸŸ
    with st.container():
        st.header("âš™ï¸ æ–‡æ¡£åˆ†æé…ç½®")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("ğŸ“„ ä¸Šä¼ PDFæ–‡ä»¶")
            uploaded_file = st.file_uploader(
                "é€‰æ‹©è¦åˆ†æçš„PDFæ–‡ä»¶",
                type="pdf",
                help=f"æ”¯æŒPDFæ ¼å¼ï¼Œæœ€å¤§æ–‡ä»¶å¤§å°: {CONFIG['MAX_FILE_SIZE'] // (1024*1024)}MB"
            )
            
            if uploaded_file:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                if len(uploaded_file.getvalue()) > CONFIG['MAX_FILE_SIZE']:
                    st.error(f"æ–‡ä»¶å¤ªå¤§ï¼æœ€å¤§æ”¯æŒ {CONFIG['MAX_FILE_SIZE'] // (1024*1024)}MB")
                    return
                
                # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
                file_size = len(uploaded_file.getvalue()) / 1024 / 1024
                st.info(f"ğŸ“„ {uploaded_file.name} ({file_size:.1f}MB)")

        with col2:
            st.subheader("ğŸ” æœç´¢å…³é”®è¯")
            query = st.text_input(
                "è¾“å…¥æœç´¢å…³é”®è¯",
                placeholder="ä¾‹å¦‚: çˆ±æƒ…, æ­»äº¡, æƒåŠ›",
                help="è¾“å…¥æ‚¨è¦æœç´¢çš„å…³é”®è¯æˆ–çŸ­è¯­"
            )

        # é«˜çº§é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
        with st.expander("âš™ï¸ é«˜çº§è®¾ç½®", expanded=False):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                use_keyword_expansion = st.checkbox("å¯ç”¨å…³é”®è¯æ‰©å±•", value=True)
                min_results = st.slider("æœ€å°‘è¿”å›ç»“æœ", 1, 10, 3)
                enable_retry = st.checkbox("å¯ç”¨æ™ºèƒ½é‡è¯•", value=True)
            
            with col2:
                final_max_results = st.slider("æœ€ç»ˆç»“æœæ•°é‡", 3, 20, 10)
                similarity_threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 0.5, 0.1, 0.05)
                reranker_type = st.selectbox("æ’åºæ–¹æ³•", 
                    ["enhanced_cross_encoder", "simple", "none"], 
                    index=0)
            
            with col3:
                auto_perform_analysis = st.checkbox("è‡ªåŠ¨åˆ†æ", value=True)
                perform_visualization = st.checkbox("ç”Ÿæˆå¯è§†åŒ–", value=True)
                chunk_size = st.slider("æ–‡æœ¬å—å¤§å°", 500, 1500, 800)

        # æœç´¢æŒ‰é’®
        search_button = st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True)

    # å¤„ç†æœç´¢è¯·æ±‚
    if uploaded_file and query and search_button:
        process_search_request(
            uploaded_file, query, model_path, user_id,
            use_keyword_expansion, min_results, enable_retry,
            final_max_results, similarity_threshold, reranker_type,
            auto_perform_analysis, perform_visualization, chunk_size
        )

    # æ˜¾ç¤ºç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    display_cached_results()

def process_search_request(uploaded_file, query, model_path, user_id, 
                         use_keyword_expansion, min_results, enable_retry,
                         final_max_results, similarity_threshold, reranker_type,
                         auto_perform_analysis, perform_visualization, chunk_size):
    """å¤„ç†æœç´¢è¯·æ±‚ - ä¼˜åŒ–ç‰ˆ"""
    
    start_time = time.time()
    logger.info(f"ç”¨æˆ· {user_id} å¼€å§‹æœç´¢: {query}")
    
    with st.spinner("æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚..."):
        progress_bar = st.progress(0)
        status_placeholder = st.empty()
        
        try:
            # 1. å¤„ç†PDFæ–‡ä»¶
            status_placeholder.info("ğŸ“„ æ­£åœ¨å¤„ç†PDFæ–‡ä»¶...")
            progress_bar.progress(10)
            
            file_bytes = uploaded_file.getvalue()
            file_hash = get_file_hash(file_bytes)
            
            # å°è¯•ä»ç¼“å­˜è·å–
            texts = cached_process_pdf(file_hash, chunk_size, 80, False)
            
            if texts is None:
                # å¤„ç†PDF
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf", dir=CONFIG['TEMP_DIR']) as tmp_file:
                    tmp_file.write(file_bytes)
                    texts = process_pdf(tmp_file.name, chunk_size=chunk_size, chunk_overlap=80, remove_stopwords=False)
                    os.unlink(tmp_file.name)
                
                # ä¿å­˜åˆ°ç¼“å­˜
                save_pdf_cache(file_hash, texts, chunk_size, 80, False)
                logger.info(f"PDFå¤„ç†å®Œæˆï¼Œæå–äº† {len(texts)} ä¸ªæ–‡æœ¬å—")
            else:
                logger.info(f"ä»ç¼“å­˜è·å–PDFæ•°æ®ï¼Œ{len(texts)} ä¸ªæ–‡æœ¬å—")
            
            if not texts:
                st.error("æ— æ³•ä»PDFä¸­æå–æ–‡æœ¬")
                return
            
            progress_bar.progress(30)
            
            # 2. å…³é”®è¯æ‰©å±•
            final_query = query
            if use_keyword_expansion:
                status_placeholder.info("ğŸ” æ­£åœ¨æ‰©å±•å…³é”®è¯...")
                try:
                    expander = KeywordExpander(method="semantic", document_type="literary")
                    expanded_keywords = expander.expand_keywords(
                        query.split(), max_synonyms_per_word=3, max_related_per_word=2
                    )
                    final_query = " ".join(expanded_keywords.keys())
                    st.session_state.expanded_keywords = expanded_keywords
                    logger.info(f"å…³é”®è¯æ‰©å±•: {len(query.split())} â†’ {len(expanded_keywords)}")
                except Exception as e:
                    logger.warning(f"å…³é”®è¯æ‰©å±•å¤±è´¥: {e}")
                    final_query = query
            
            progress_bar.progress(50)
            
            # 3. æœç´¢
            status_placeholder.info("ğŸ” æ­£åœ¨æœç´¢ç›¸å…³å†…å®¹...")
            try:
                if enable_retry:
                    results = perform_smart_search(final_query, texts, model_path, final_max_results, min_results)
                else:
                    results = standard_search(final_query, texts, model_path)
                    results = results[:final_max_results] if results else []
                
                logger.info(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            except Exception as e:
                logger.error(f"æœç´¢å¤±è´¥: {e}")
                st.error(f"æœç´¢å¤±è´¥: {e}")
                return
            
            progress_bar.progress(70)
            
            # 4. ç»“æœå¤„ç†å’Œé‡æ’
            if results and reranker_type != "none":
                status_placeholder.info("ğŸ“Š æ­£åœ¨ä¼˜åŒ–ç»“æœæ’åº...")
                try:
                    reranker = create_reranker(reranker_type, keywords=final_query.split())
                    candidates = [{'content': r.get('text', ''), 'score': r.get('score', 0), 
                                 'page_num': r.get('page_num', 1), 'index': i} 
                                 for i, r in enumerate(results)]
                    reranked = reranker.rerank(candidates, k=final_max_results)
                    results = [{'text': c['content'], 'score': c.get('rerank_score', c.get('score', 0)),
                              'page_num': c.get('page_num', 1)} for c in reranked]
                    logger.info(f"é‡æ’å®Œæˆï¼Œæœ€ç»ˆ {len(results)} ä¸ªç»“æœ")
                except Exception as e:
                    logger.warning(f"é‡æ’å¤±è´¥: {e}")
            
            progress_bar.progress(90)
            
            # 5. ä¿å­˜ç»“æœ
            st.session_state.results = results
            st.session_state.texts = texts
            st.session_state.query = query
            st.session_state.processing_time = time.time() - start_time
            
            # 6. è‡ªåŠ¨åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if auto_perform_analysis and results:
                status_placeholder.info("ğŸ­ æ­£åœ¨è¿›è¡Œæ–‡å­¦åˆ†æ...")
                try:
                    perform_auto_analysis(texts)
                except Exception as e:
                    logger.warning(f"è‡ªåŠ¨åˆ†æå¤±è´¥: {e}")
            
            progress_bar.progress(100)
            status_placeholder.success(f"âœ… å¤„ç†å®Œæˆï¼ç”¨æ—¶ {time.time() - start_time:.1f} ç§’")
            
            logger.info(f"ç”¨æˆ· {user_id} æœç´¢å®Œæˆï¼Œç”¨æ—¶ {time.time() - start_time:.1f} ç§’")
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            st.error(f"å¤„ç†å¤±è´¥: {e}")
            progress_bar.empty()
            status_placeholder.empty()

def perform_smart_search(query, texts, model_path, max_results, min_results):
    """æ‰§è¡Œæ™ºèƒ½æœç´¢"""
    # ç®€åŒ–çš„æœç´¢é€»è¾‘
    results = standard_search(query, texts, model_path)
    if len(results) < min_results and len(texts) > min_results:
        # ç®€å•çš„è¡¥å……æœºåˆ¶
        import random
        additional_needed = min_results - len(results)
        remaining_texts = [texts[i] for i in range(len(texts)) if i not in [r.get('index', -1) for r in results]]
        if remaining_texts:
            additional = random.sample(remaining_texts, min(additional_needed, len(remaining_texts)))
            for i, text in enumerate(additional):
                results.append({
                    'text': text,
                    'score': 0.1,
                    'page_num': len(results) + i + 1,
                    'is_fallback': True
                })
    
    return results[:max_results]

def perform_auto_analysis(texts):
    """æ‰§è¡Œè‡ªåŠ¨åˆ†æ"""
    try:
        full_text_chunks = [{'content': text, 'page_num': i+1} for i, text in enumerate(texts)]
        
        # ç®€åŒ–çš„åˆ†æ
        theme_analyzer = LiteraryThemeAnalyzer()
        theme_result = theme_analyzer.analyze_text_themes(full_text_chunks)
        
        literary_analyzer = LiteraryAnalyzer()
        comprehensive_results = literary_analyzer.generate_comprehensive_analysis(full_text_chunks)
        
        analysis_results = {
            'theme_analysis': theme_result,
            'character_analysis': comprehensive_results.get('characters', {}),
            'emotion_analysis': comprehensive_results.get('emotions', {}),
            'narrative_analysis': comprehensive_results.get('narrative', {}),
            'comprehensive_results': comprehensive_results
        }
        
        st.session_state.analysis_results = analysis_results
        logger.info("è‡ªåŠ¨åˆ†æå®Œæˆ")
        
    except Exception as e:
        logger.error(f"è‡ªåŠ¨åˆ†æå¤±è´¥: {e}")
        raise

def display_cached_results():
    """æ˜¾ç¤ºç¼“å­˜çš„ç»“æœ"""
    if 'results' in st.session_state and st.session_state.results:
        st.markdown("---")
        st.header("ğŸ¯ æœç´¢ç»“æœ")
        
        # æ˜¾ç¤ºå¤„ç†æ—¶é—´
        if 'processing_time' in st.session_state:
            st.info(f"â±ï¸ å¤„ç†ç”¨æ—¶: {st.session_state.processing_time:.1f} ç§’")
        
        display_results(st.session_state.results)
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        if 'analysis_results' in st.session_state:
            display_analysis_results()
        
        # æ˜¾ç¤ºå¯è§†åŒ–
        if st.session_state.get('perform_visualization', False):
            display_visualizations()

def display_analysis_results():
    """æ˜¾ç¤ºåˆ†æç»“æœ"""
    st.markdown("---")
    st.header("ğŸ­ æ–‡å­¦åˆ†æç»“æœ")
    
    results = st.session_state.analysis_results
    
    if results.get('comprehensive_results'):
        st.subheader("ğŸ“ˆ ç»¼åˆåˆ†æå¯è§†åŒ–")
        try:
            visualizer = AdvancedVisualizer(language='zh')
            fig = visualizer.plot_literary_analysis(results['comprehensive_results'])
            if fig:
                st.pyplot(fig)
                st.success("âœ… åˆ†æå›¾è¡¨ç”ŸæˆæˆåŠŸ")
        except Exception as e:
            logger.error(f"åˆ†æå¯è§†åŒ–å¤±è´¥: {e}")
            st.error("åˆ†æå¯è§†åŒ–ç”Ÿæˆå¤±è´¥")

def display_visualizations():
    """æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨"""
    st.markdown("---")
    st.header("ğŸ“Š æ•°æ®å¯è§†åŒ–")
    
    try:
        visualizer = AdvancedVisualizer(language='zh')
        results = st.session_state.results
        keywords = st.session_state.get('query', '').split()
        
        col1, col2 = st.columns(2)
        
        with col1:
            # é¡µé¢åˆ†å¸ƒå›¾
            fig1 = visualizer.plot_page_distribution(results, keywords)
            if fig1:
                st.pyplot(fig1)
            
            # è¯äº‘å›¾  
            fig3 = visualizer.plot_word_cloud(results, keywords, max_words=100)
            if fig3:
                st.pyplot(fig3)
        
        with col2:
            # ä¸»é¢˜é¢‘ç‡å›¾
            fig2 = visualizer.plot_theme_frequency(results, keywords)
            if fig2:
                st.pyplot(fig2)
            
            # å…±ç°çƒ­åŠ›å›¾
            fig4 = visualizer.plot_cooccurrence_heatmap(results, keywords)
            if fig4:
                st.pyplot(fig4)
                
    except Exception as e:
        logger.error(f"å¯è§†åŒ–å¤±è´¥: {e}")
        st.error("å¯è§†åŒ–ç”Ÿæˆå¤±è´¥")

    # Enhanced CSS for modern, readable styling
    st.markdown("""
    <style>
    /* Import Google Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
    
    /* Global Styles - Clean background with Chinese font support */
    .stApp {
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', 'Arial Unicode MS', sans-serif;
        background-color: #f8f9fa;
    }
    
    /* Main content area with subtle styling and Chinese font support */
    .main .block-container {
        background: white;
        border-radius: 12px;
        padding: 2rem;
        margin-top: 1rem;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', 'Arial Unicode MS', sans-serif;
    }
    
    /* Header styling */
    .main-header {
        text-align: center;
        padding: 2rem 0;
        color: #1f2937;
        margin-bottom: 2rem;
        position: relative;
    }
    
    .main-header::after {
        content: '';
        position: absolute;
        bottom: 0;
        left: 50%;
        transform: translateX(-50%);
        width: 100px;
        height: 3px;
        background: linear-gradient(90deg, #3b82f6, #8b5cf6);
        border-radius: 2px;
    }
    
    /* Configuration sections with clean styling */
    .config-section {
        background: #f8fafc;
        padding: 2rem;
        border-radius: 12px;
        margin: 1.5rem 0;
        border: 1px solid #e2e8f0;
        transition: all 0.3s ease;
    }
    
    .config-section:hover {
        transform: translateY(-2px);
        box-shadow: 0 10px 25px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        border-color: #3b82f6;
    }
    
    /* Button styling - consistent and readable */
    .stButton > button {
        width: 100%;
        margin-top: 1rem;
        background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
        border: none;
        border-radius: 8px;
        padding: 0.75rem 2rem;
        font-weight: 600;
        font-size: 1.1rem;
        color: white;
        transition: all 0.3s ease;
        box-shadow: 0 4px 14px 0 rgba(59, 130, 246, 0.3);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px 0 rgba(59, 130, 246, 0.4);
        background: linear-gradient(135deg, #2563eb 0%, #7c3aed 100%);
    }
    
    /* Input field styling - better visibility */
    .stTextInput > div > div > input,
    .stSelectbox > div > div > select {
        background: white;
        border: 2px solid #d1d5db;
        border-radius: 8px;
        color: #374151;
        font-size: 1rem;
    }
    
    .stTextInput > div > div > input:focus,
    .stSelectbox > div > div > select:focus {
        border-color: #3b82f6;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
    }
    
    /* File uploader styling */
    .stFileUploader {
        background: #f8fafc;
        border: 2px dashed #d1d5db;
        border-radius: 12px;
        padding: 2rem;
        transition: all 0.3s ease;
    }
    
    .stFileUploader:hover {
        border-color: #3b82f6;
        background: #eff6ff;
    }
    
    /* Expander styling */
    .streamlit-expanderHeader {
        background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
        border-radius: 8px;
        padding: 0.75rem 1rem;
        font-weight: 600;
        color: white;
    }
    
    .streamlit-expanderContent {
        background: #f8fafc;
        border-radius: 0 0 8px 8px;
        padding: 1.5rem;
        border: 1px solid #e2e8f0;
    }
    
    /* Progress bar */
    .stProgress .progress-bar {
        background: linear-gradient(90deg, #3b82f6, #8b5cf6);
    }
    
    /* Success/Error/Warning messages - better contrast */
    .stSuccess {
        background: #ecfdf5;
        border-left: 4px solid #10b981;
        border-radius: 8px;
        color: #047857;
    }
    
    .stError {
        background: #fef2f2;
        border-left: 4px solid #ef4444;
        border-radius: 8px;
        color: #dc2626;
    }
    
    .stWarning {
        background: #fffbeb;
        border-left: 4px solid #f59e0b;
        border-radius: 8px;
        color: #d97706;
    }
    
    .stInfo {
        background: #eff6ff;
        border-left: 4px solid #3b82f6;
        border-radius: 8px;
        color: #2563eb;
    }
    
    /* Custom result cards */
    .result-card {
        background: white;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        border: 1px solid #e2e8f0;
        transition: all 0.3s ease;
        box-shadow: 0 2px 4px -1px rgba(0, 0, 0, 0.06), 0 1px 2px -1px rgba(0, 0, 0, 0.06);
    }
    
    .result-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 10px 25px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        border-color: #3b82f6;
    }
    
    /* Score badge styling - better contrast */
    .score-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-weight: 600;
        font-size: 0.9rem;
        margin: 0.25rem;
    }
    
    .score-high {
        background: #10b981;
        color: white;
        box-shadow: 0 2px 8px rgba(16, 185, 129, 0.3);
    }
    
    .score-medium {
        background: #f59e0b;
        color: white;
        box-shadow: 0 2px 8px rgba(245, 158, 11, 0.3);
    }
    
    .score-low {
        background: #6b7280;
        color: white;
        box-shadow: 0 2px 8px rgba(107, 114, 128, 0.3);
    }
    
    /* Gradient text for headers */
    .gradient-text {
        background: linear-gradient(-45deg, #3b82f6, #8b5cf6);
        background-size: 400% 400%;
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        animation: gradientShift 3s ease infinite;
        font-weight: 700;
    }
    
    @keyframes gradientShift {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    
    /* Floating animation */
    @keyframes float {
        0% { transform: translateY(0px); }
        50% { transform: translateY(-5px); }
        100% { transform: translateY(0px); }
    }
    
    .floating {
        animation: float 3s ease-in-out infinite;
    }
    
    /* Text colors for better readability with Chinese font support */
    h1, h2, h3, h4, h5, h6 {
        color: #1f2937;
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', 'Arial Unicode MS', sans-serif !important;
    }
    
    p, span, div {
        color: #374151;
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', 'Arial Unicode MS', sans-serif !important;
    }
    
    /* Force Chinese font support for all elements */
    * {
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', 'Arial Unicode MS', sans-serif !important;
    }
    
    /* Checkbox and radio button styling */
    .stCheckbox > label {
        color: #374151;
        font-weight: 500;
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', 'Arial Unicode MS', sans-serif !important;
    }
    
    .stRadio > label {
        color: #374151;
        font-weight: 500;
        font-family: 'PingFang SC', 'Microsoft YaHei', 'Inter', 'Arial Unicode MS', sans-serif !important;
    }
    
    /* Slider styling */
    .stSlider > div > div > div > div {
        background-color: #3b82f6;
    }
    </style>
    """, unsafe_allow_html=True)
    print("âœ… CSSæ ·å¼åŠ è½½å®Œæˆ")

    # Main header with clean styling
    st.markdown('<div class="main-header floating">', unsafe_allow_html=True)
    st.title("ğŸ“š å¼•æ–‡æœç´¢ç³»ç»Ÿ")
    st.markdown("æ™ºèƒ½æ–‡æœ¬æœç´¢ã€åˆ†æå’Œå¯è§†åŒ–çš„ç»¼åˆå·¥å…· - ä¸“ä¸ºæ•™å¸ˆå’Œå­¦ç”Ÿè®¾è®¡")
    st.markdown('</div>', unsafe_allow_html=True)
    print("âœ… é¡µé¢æ ‡é¢˜æ˜¾ç¤ºå®Œæˆ")

    try:
        model_path = get_model_path()
        print(f"âœ… æ¨¡å‹è·¯å¾„è·å–å®Œæˆ: {model_path}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è·¯å¾„è·å–å¤±è´¥: {e}")
        st.error(f"æ¨¡å‹è·¯å¾„è·å–å¤±è´¥: {e}")
        model_path = 'sentence-transformers/all-MiniLM-L6-v2'
    
    # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
    try:
        from modules.retriever import load_sentence_transformer_model
        test_model = load_sentence_transformer_model(model_path)
        if test_model is not None:
            # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„SentenceTransformeræ¨¡å‹
            if hasattr(test_model, 'encode') and hasattr(test_model, '_modules'):
                # æ£€æŸ¥å…·ä½“ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„
                local_paths = [
                    'final_model_data/sentence_transformer_model',
                    'models_cache/models--sentence-transformers--all-MiniLM-L6-v2'
                ]
                local_model_found = any(os.path.exists(path) for path in local_paths)
                
                if local_model_found:
                    st.success("âœ… æœ¬åœ°è¯­ä¹‰æ¨¡å‹åŠ è½½æˆåŠŸ! (ä½¿ç”¨ç¼“å­˜æ¨¡å‹)")
                else:
                    st.success("âœ… åœ¨çº¿è¯­ä¹‰æ¨¡å‹åŠ è½½æˆåŠŸ!")
            else:
                st.warning("âš ï¸ ä½¿ç”¨ç®€å•è¯æ±‡æ¨¡å‹ (è¯­ä¹‰æ¨¡å‹ä¸å¯ç”¨)")
        else:
            st.warning("âš ï¸ è¯­ä¹‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€å•è¯æ±‡æ¨¡å‹ã€‚åŠŸèƒ½å¯èƒ½å—é™ã€‚")
    except Exception as e:
        st.error(f"âŒ æ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")
        st.info("ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡å¯åº”ç”¨ç¨‹åº")

    # --- Main Configuration Section ---
    st.markdown('<div class="config-section">', unsafe_allow_html=True)
    st.header("âš™ï¸ åŸºæœ¬é…ç½®")
    
    # Create columns for the main input section
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # --- File Upload ---
        st.subheader("ğŸ“„ ä¸Šä¼ PDFæ–‡ä»¶")
        uploaded_file = st.file_uploader("é€‰æ‹©è¦æœç´¢çš„PDFæ–‡ä»¶ã€‚", type="pdf", 
                                        help="ä¸Šä¼ æ‚¨è¦æœç´¢çš„PDFæ–‡æ¡£ã€‚")

    with col2:
        # --- Search Query ---
        st.subheader("ğŸ” è¾“å…¥æœç´¢å…³é”®è¯")
        query = st.text_input("å…³é”®è¯:", placeholder="ä¾‹å¦‚: çˆ±æƒ…, æ­»äº¡, æƒåŠ›, é‡å¿ƒ",
                             help="è¾“å…¥æ‚¨è¦æœç´¢çš„å…³é”®è¯æˆ–çŸ­è¯­ã€‚")

    # --- åˆå§‹åŒ–é»˜è®¤é…ç½®å˜é‡ (ç¡®ä¿åœ¨å…¨å±€ä½œç”¨åŸŸå†…) ---
    # Core analysis settings
    perform_literary_analysis = True
    min_cooccurrence = 2
    perform_theme_analysis = True
    perform_visualization = True
    auto_perform_analysis = False
    
    # Text processing defaults
    chunk_size = 800
    chunk_overlap = 80
    remove_stopwords = False
    
    # Search configuration defaults
    use_keyword_expansion = True
    min_results = 3
    enable_retry = True
    use_hybrid_search = True
    
    # Keyword expansion defaults
    expander_method = "semantic"
    document_type = "literary"  
    max_synonyms = 3
    max_related_words = 2
    semantic_threshold = 0.75
    use_hierarchical = False
    
    # Hybrid search defaults
    fusion_method = "rrf"
    rrf_k = 50
    bm25_weight = 0.3
    embedding_weight = 0.7
    enable_parallel = True
    
    # Result processing defaults
    reranker_type = "enhanced_cross_encoder"
    initial_max_results = 20
    final_max_results = 10
    similarity_threshold = 0.1
    
    # Reranker defaults
    simple_reranker_threshold = 0.2
    cross_encoder_model = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    cross_encoder_threshold = 0.2
    enable_ensemble = True
    enable_consistency_check = True
    enable_diversity = True
    enable_context_aware = True
    diversity_threshold = 0.7
    consistency_weight = 0.3
    
    # Visualization defaults  
    wordcloud_max_words = 100
    heatmap_top_n = 8
    plot_style = "whitegrid"
    figure_size = "medium"
    
    # System defaults
    cache_expire_days = 7

    # --- Advanced Configuration (moved above search button) ---
    st.markdown("---")
    with st.expander("âš™ï¸ é«˜çº§æœç´¢é…ç½® (å¯é€‰)", expanded=False):
        st.info("ğŸ’¡ ä»¥ä¸‹å‚æ•°å·²è®¾ç½®ä¸ºé€‚åˆæ•™å¸ˆå’Œå­¦ç”Ÿä½¿ç”¨çš„æ¨èå€¼ã€‚å¦‚éœ€è°ƒæ•´ï¼Œè¯·å‚è€ƒè¯´æ˜ã€‚")
        
        col1, col2, col3 = st.columns(3)

        with col1:
            st.subheader("ğŸ” æœç´¢é…ç½®")
            use_keyword_expansion = st.checkbox("å¯ç”¨å…³é”®è¯æ‰©å±•", value=True,
                                              help="ğŸ” **å…³é”®è¯æ‰©å±•**: è‡ªåŠ¨æ·»åŠ ç›¸å…³è¯æ±‡æé«˜æœç´¢æ•ˆæœã€‚æ¨èå¯ç”¨ã€‚")
            
            min_results = st.slider("æœ€å°‘è¿”å›ç»“æœ", 1, 10, 3,
                                   help="ğŸ“Š **æœ€å°‘è¿”å›ç»“æœ**: è´¨é‡ä¿è¯çš„åº•çº¿ã€‚å³ä½¿ç›¸ä¼¼åº¦å¾ˆä½ï¼Œä¹Ÿè‡³å°‘è¿”å›è¿™ä¹ˆå¤šç»“æœã€‚é€šå¸¸è®¾ä¸º3ä¸ªç¡®ä¿æœ‰å†…å®¹æŸ¥çœ‹ã€‚")
            
            enable_retry = st.checkbox("å¯ç”¨æ™ºèƒ½é‡è¯•", value=True,
                                     help="ğŸ”„ **æ™ºèƒ½é‡è¯•**: ç»“æœä¸è¶³æ—¶è‡ªåŠ¨å°è¯•å…¶ä»–æœç´¢ç­–ç•¥ã€‚å¼ºçƒˆæ¨èå¯ç”¨ã€‚")
            
            if use_keyword_expansion:
                expander_method = st.selectbox("æ‰©å±•æ–¹æ³•", ["semantic", "wordnet"], index=0,
                                             format_func=lambda x: "è¯­ä¹‰æ‰©å±• (æ¨è)" if x == "semantic" else "è¯å…¸æ‰©å±•",
                                             help="ğŸ“š **æ‰©å±•æ–¹æ³•**: è¯­ä¹‰æ‰©å±•æ›´æ™ºèƒ½ï¼Œè¯å…¸æ‰©å±•æ›´ä¿å®ˆã€‚")
                document_type = st.selectbox("æ–‡æ¡£ç±»å‹", ["literary", "general"], index=0,
                                           format_func=lambda x: "æ–‡å­¦ä½œå“ (æ¨è)" if x == "literary" else "ä¸€èˆ¬æ–‡æ¡£",
                                           help="ğŸ“– **æ–‡æ¡£ç±»å‹**: æ–‡å­¦ä½œå“é’ˆå¯¹è¯—æ­Œã€å°è¯´ç­‰ä¼˜åŒ–ã€‚")
                max_synonyms = st.slider("åŒä¹‰è¯æ•°é‡", 1, 10, 3,
                                       help="ğŸ“ **åŒä¹‰è¯æ•°é‡**: æ¯ä¸ªè¯çš„åŒä¹‰è¯ä¸ªæ•°ã€‚3ä¸ªå¹³è¡¡æ•ˆæœå’Œå‡†ç¡®æ€§ã€‚")
                max_related_words = st.slider("ç›¸å…³è¯æ•°é‡", 1, 10, 2,
                                            help="ğŸ”— **ç›¸å…³è¯æ•°é‡**: æ‰©å±•çš„ç›¸å…³è¯ä¸ªæ•°ã€‚2ä¸ªé¿å…åç¦»ä¸»é¢˜ã€‚")
                semantic_threshold = st.slider("è¯­ä¹‰ç›¸ä¼¼åº¦", 0.1, 1.0, 0.75, 0.05,
                                             help="ğŸ¯ **è¯­ä¹‰ç›¸ä¼¼åº¦**: æ‰©å±•è¯çš„ç›¸ä¼¼åº¦è¦æ±‚ã€‚0.75ç¡®ä¿é«˜è´¨é‡æ‰©å±•ã€‚")
                use_hierarchical = st.checkbox("åˆ†å±‚æ‰©å±•", value=False,
                                             help="ğŸŒ³ **åˆ†å±‚æ‰©å±•**: å¤šå±‚æ¬¡æ‰©å±•å…³é”®è¯ã€‚å¯èƒ½å¢åŠ æ— å…³ç»“æœã€‚")
            else:
                expander_method = "semantic"
                document_type = "literary"
                max_synonyms = 3
                max_related_words = 2
                semantic_threshold = 0.75
                use_hierarchical = False

            st.markdown("**ğŸ”„ æ··åˆæœç´¢**")
            use_hybrid_search = st.checkbox("å¯ç”¨æ··åˆæœç´¢", value=True,
                                          help="ğŸš€ **æ··åˆæœç´¢**: ç»“åˆå…³é”®è¯å’Œè¯­ä¹‰æœç´¢ã€‚å¼ºçƒˆæ¨èå¯ç”¨ã€‚")
            if use_hybrid_search:
                fusion_method = st.selectbox("èåˆæ–¹æ³•", ["rrf", "weighted"], index=0,
                                           format_func=lambda x: "RRFèåˆ (æ¨è)" if x == "rrf" else "åŠ æƒèåˆ",
                                           help="âš–ï¸ **èåˆæ–¹æ³•**: RRFæ›´å¹³è¡¡ï¼ŒåŠ æƒèåˆå¯è°ƒæ€§æ›´å¼ºã€‚")
                rrf_k = st.slider("RRFå‚æ•°", 10, 100, 50,
                                help="ğŸ”§ **RRFå‚æ•°**: æ§åˆ¶æ’åèåˆå¼ºåº¦ã€‚50ä¸ºå¹³è¡¡å€¼ã€‚")
                bm25_weight = st.slider("å…³é”®è¯æœç´¢æƒé‡", 0.0, 1.0, 0.3, 0.1,
                                      help="ğŸ”¤ **å…³é”®è¯æƒé‡**: ä¼ ç»Ÿå…³é”®è¯æœç´¢çš„é‡è¦æ€§ã€‚0.3é€‚åˆæ–‡å­¦åˆ†æã€‚")
                embedding_weight = st.slider("è¯­ä¹‰æœç´¢æƒé‡", 0.0, 1.0, 0.7, 0.1,
                                           help="ğŸ§  **è¯­ä¹‰æƒé‡**: æ™ºèƒ½è¯­ä¹‰ç†è§£çš„é‡è¦æ€§ã€‚0.7é€‚åˆæ·±åº¦åˆ†æã€‚")
                enable_parallel = st.checkbox("å¹¶è¡Œå¤„ç†", value=True,
                                             help="âš¡ **å¹¶è¡Œå¤„ç†**: åŠ é€Ÿæœç´¢è¿‡ç¨‹ã€‚æ¨èå¯ç”¨ã€‚")
            else:
                fusion_method = "rrf"
                rrf_k = 50
                bm25_weight = 0.3
                embedding_weight = 0.7
                enable_parallel = True

        with col2:
            st.subheader("ğŸ“Š ç»“æœå¤„ç†")
            reranker_type = st.selectbox("ç»“æœæ’åºæ–¹æ³•", 
                                       ["enhanced_cross_encoder", "cross_encoder", "simple", "none"], 
                                       index=0,  # enhanced_cross_encoder as default
                                       format_func=lambda x: {
                                           "enhanced_cross_encoder": "æ™ºèƒ½å¢å¼ºæ’åº (æ¨è)",
                                           "cross_encoder": "äº¤å‰ç¼–ç å™¨æ’åº",
                                           "simple": "ç®€å•æ’åº",
                                           "none": "ä¸æ’åº"
                                       }.get(x, x),
                                       help="ğŸ¯ **æ’åºæ–¹æ³•**: æ™ºèƒ½å¢å¼ºæ’åºå‡†ç¡®æ€§æœ€é«˜ï¼Œé€‚åˆå­¦æœ¯ç ”ç©¶ã€‚")
            
            # ç»“æœæ•°é‡æ§åˆ¶ - åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µ
            st.markdown("**ğŸ“Š ç»“æœæ•°é‡æ§åˆ¶**")
            st.info("ğŸ’¡ **ä¸¤é˜¶æ®µæœç´¢**: å…ˆå¤§èŒƒå›´æœç´¢å€™é€‰ç»“æœï¼Œå†ç²¾ç¡®é‡æ’é€‰å‡ºæœ€ä½³ç»“æœï¼Œæé«˜æœç´¢è´¨é‡")
            col1, col2 = st.columns(2)
            
            with col1:
                initial_max_results = st.slider("åˆæ­¥æœç´¢ç»“æœæ•°", 10, 100, 20, 5,
                                               help="ï¿½ **åˆæ­¥æœç´¢**: ç¬¬ä¸€é˜¶æ®µè¿”å›çš„å€™é€‰ç»“æœæ•°é‡ã€‚æ›´å¤šç»“æœå¯ä»¥æä¾›æ›´å¤šé€‰æ‹©ç»™é‡æ’å™¨ã€‚")
            
            with col2:
                final_max_results = st.slider("æœ€ç»ˆè¿”å›ç»“æœæ•°", 3, 30, 10, 1,
                                             help="âœ… **æœ€ç»ˆç»“æœ**: é‡æ’å’Œè¿‡æ»¤åæœ€ç»ˆæ˜¾ç¤ºç»™ç”¨æˆ·çš„ç»“æœæ•°é‡ã€‚è¿™æ˜¯æ‚¨å®é™…çœ‹åˆ°çš„ç»“æœæ•°é‡ã€‚å»ºè®®10ä¸ªä¾¿äºé˜…è¯»ã€‚")
            
            similarity_threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 0.5, 0.1, 0.05,
                                           help="ğŸ“ **ç›¸ä¼¼åº¦é˜ˆå€¼**: è¿‡æ»¤ä½è´¨é‡ç»“æœã€‚0.1å¹³è¡¡æ•°é‡å’Œè´¨é‡ã€‚")

            if reranker_type == "simple":
                simple_reranker_threshold = st.slider("ç®€å•æ’åºé˜ˆå€¼", 0.0, 1.0, 0.2, 0.05,
                                                     help="âš–ï¸ **æ’åºé˜ˆå€¼**: ç®€å•æ’åºçš„è¿‡æ»¤æ ‡å‡†ã€‚")
            elif reranker_type == "cross_encoder":
                cross_encoder_model = st.text_input("äº¤å‰ç¼–ç å™¨æ¨¡å‹", 
                                                   "cross-encoder/ms-marco-MiniLM-L-6-v2",
                                                   help="ğŸ¤– **æ¨¡å‹åç§°**: ä½¿ç”¨çš„AIæ¨¡å‹ã€‚é»˜è®¤é€‚åˆä¸­è‹±æ–‡ã€‚")
                cross_encoder_threshold = st.slider("äº¤å‰ç¼–ç å™¨é˜ˆå€¼", 0.0, 1.0, 0.2, 0.05,
                                                   help="ğŸ¯ **ç¼–ç å™¨é˜ˆå€¼**: è´¨é‡è¿‡æ»¤æ ‡å‡†ã€‚")
            elif reranker_type == "enhanced_cross_encoder":
                enable_ensemble = st.checkbox("å¯ç”¨é›†æˆæ¨¡å‹", value=True,
                                             help="ğŸ¤ **é›†æˆæ¨¡å‹**: å¤šä¸ªAIæ¨¡å‹åä½œæé«˜å‡†ç¡®æ€§ã€‚")
                enable_consistency_check = st.checkbox("ä¸€è‡´æ€§æ£€æŸ¥", value=True,
                                                      help="âœ… **ä¸€è‡´æ€§æ£€æŸ¥**: ç¡®ä¿ç»“æœç¨³å®šå¯é ã€‚")
                enable_diversity = st.checkbox("ç»“æœå¤šæ ·æ€§", value=True,
                                             help="ğŸŒˆ **å¤šæ ·æ€§**: é¿å…ç›¸ä¼¼ç»“æœé‡å¤ã€‚")
                enable_context_aware = st.checkbox("ä¸Šä¸‹æ–‡æ„ŸçŸ¥", value=True,
                                                  help="ğŸ§  **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**: ç†è§£æ–‡æœ¬èƒŒæ™¯æé«˜ç›¸å…³æ€§ã€‚")
                # Additional enhanced options with Chinese descriptions
                diversity_threshold = st.slider("å¤šæ ·æ€§é˜ˆå€¼", 0.1, 1.0, 0.7, 0.1,
                                              help="ğŸ¨ **å¤šæ ·æ€§é˜ˆå€¼**: æ§åˆ¶ç»“æœå·®å¼‚ç¨‹åº¦ã€‚0.7ç¡®ä¿é€‚åº¦å·®å¼‚ã€‚")
                consistency_weight = st.slider("ä¸€è‡´æ€§æƒé‡", 0.0, 1.0, 0.3, 0.1,
                                             help="âš–ï¸ **ä¸€è‡´æ€§æƒé‡**: ç¨³å®šæ€§åœ¨æ’åºä¸­çš„é‡è¦æ€§ã€‚")
            
            # Set optimal default values for reranker parameters
            if reranker_type not in ["simple"]:
                simple_reranker_threshold = 0.2
            if reranker_type not in ["cross_encoder"]:
                cross_encoder_model = "cross-encoder/ms-marco-MiniLM-L-6-v2"
                cross_encoder_threshold = 0.2
            if reranker_type not in ["enhanced_cross_encoder"]:
                enable_ensemble = True
                enable_consistency_check = True
                enable_diversity = True
                enable_context_aware = True
                diversity_threshold = 0.7
                consistency_weight = 0.3

        with col3:
            st.subheader("ğŸ¨ åˆ†æä¸å¯è§†åŒ–")
            auto_perform_analysis = st.checkbox("æœç´¢åè‡ªåŠ¨åˆ†æ", value=False,
                                                  help="âœ… **è‡ªåŠ¨åˆ†æ**: æœç´¢å®Œæˆåç«‹å³è‡ªåŠ¨æ‰§è¡Œä¸‹æ–¹é€‰å®šçš„åˆ†æã€‚å¦‚æœç¦ç”¨ï¼Œåˆ™éœ€è¦æ‰‹åŠ¨ç‚¹å‡»æŒ‰é’®å¼€å§‹åˆ†æã€‚")
            perform_literary_analysis = st.checkbox("æ–‡å­¦åˆ†æ", value=True,
                                                   help="ğŸ“š **æ–‡å­¦åˆ†æ**: åˆ†æäººç‰©å…³ç³»ã€ä¸»é¢˜ç­‰ã€‚æ¨èæ–‡å­¦ä½œå“å¯ç”¨ã€‚")
            if perform_literary_analysis:
                min_cooccurrence = st.slider("æœ€å°å…±ç°é¢‘ç‡", 1, 10, 2,
                                           help="ğŸ”— **å…±ç°é¢‘ç‡**: äººç‰©/æ¦‚å¿µåŒæ—¶å‡ºç°çš„æœ€å°‘æ¬¡æ•°ã€‚2æ•è·æœ‰æ„ä¹‰å…³ç³»ã€‚")
            else:
                min_cooccurrence = 2
                
            perform_theme_analysis = st.checkbox("ä¸»é¢˜åˆ†æ", value=True,
                                                help="ğŸ­ **ä¸»é¢˜åˆ†æ**: è¯†åˆ«æ–‡æœ¬ä¸»é¢˜å’Œæƒ…æ„Ÿã€‚æœ‰åŠ©äºæ·±åº¦ç†è§£ã€‚")
            perform_visualization = st.checkbox("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨", value=True,
                                              help="ğŸ“Š **å¯è§†åŒ–**: ç”Ÿæˆå›¾è¡¨å¸®åŠ©ç†è§£åˆ†æç»“æœã€‚")
            
            # ä¿å­˜å¯è§†åŒ–é…ç½®åˆ° session_state
            st.session_state.perform_visualization = perform_visualization
            st.session_state.perform_theme_analysis = perform_theme_analysis
            
            # ä¿å­˜æ–‡æœ¬å¤„ç†è®¾ç½®åˆ° session_state
            st.session_state.chunk_size = chunk_size
            st.session_state.chunk_overlap = chunk_overlap  
            st.session_state.remove_stopwords = remove_stopwords
            
            if perform_visualization:
                wordcloud_max_words = st.slider("è¯äº‘æœ€å¤§è¯æ•°", 50, 300, 100,
                                               help="â˜ï¸ **è¯äº‘è¯æ•°**: è¯äº‘å›¾æ˜¾ç¤ºçš„è¯è¯­æ•°é‡ã€‚100ä¸ªä¾¿äºè§‚å¯Ÿã€‚")
                heatmap_top_n = st.slider("çƒ­åŠ›å›¾æ˜¾ç¤ºæ•°é‡", 5, 20, 8,
                                        help="ğŸ”¥ **çƒ­åŠ›å›¾**: çƒ­åŠ›å›¾æ˜¾ç¤ºçš„ä¸»è¦å…ƒç´ æ•°é‡ã€‚8ä¸ªæ¸…æ™°æ˜“è¯»ã€‚")
                # Additional visualization options with Chinese
                plot_style = st.selectbox("å›¾è¡¨æ ·å¼", ["whitegrid", "darkgrid", "white", "dark"], index=0,
                                        format_func=lambda x: {
                                            "whitegrid": "ç™½è‰²ç½‘æ ¼ (æ¨è)",
                                            "darkgrid": "æ·±è‰²ç½‘æ ¼", 
                                            "white": "çº¯ç™½èƒŒæ™¯",
                                            "dark": "æ·±è‰²èƒŒæ™¯"
                                        }.get(x, x),
                                        help="ğŸ¨ **å›¾è¡¨æ ·å¼**: å¯è§†åŒ–å›¾è¡¨çš„å¤–è§‚é£æ ¼ã€‚")
                figure_size = st.selectbox("å›¾è¡¨å¤§å°", ["small", "medium", "large"], index=1,
                                         format_func=lambda x: {"small": "å°", "medium": "ä¸­ (æ¨è)", "large": "å¤§"}.get(x, x),
                                         help="ğŸ“ **å›¾è¡¨å¤§å°**: ç”Ÿæˆå›¾è¡¨çš„å°ºå¯¸ã€‚ä¸­ç­‰é€‚åˆå±å¹•æ˜¾ç¤ºã€‚")
                chart_language = st.selectbox("å›¾è¡¨è¯­è¨€", ["zh", "en"], index=0,
                                            format_func=lambda x: {"zh": "ä¸­æ–‡", "en": "English"}.get(x, x),
                                            help="ğŸŒ **å›¾è¡¨è¯­è¨€**: é€‰æ‹©å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾çš„æ˜¾ç¤ºè¯­è¨€ã€‚")
            else:
                wordcloud_max_words = 100
                heatmap_top_n = 8
                plot_style = "whitegrid"
                figure_size = "medium"
                chart_language = "zh"
                
            # ä¿å­˜å¯è§†åŒ–å‚æ•°åˆ° session_state
            st.session_state.wordcloud_max_words = wordcloud_max_words
            st.session_state.heatmap_top_n = heatmap_top_n
            st.session_state.plot_style = plot_style
            st.session_state.figure_size = figure_size
            st.session_state.chart_language = chart_language

            st.markdown("**âš™ï¸ ç³»ç»Ÿè®¾ç½®**")
            cache_expire_days = st.slider("ç¼“å­˜ä¿ç•™å¤©æ•°", 1, 30, 7,
                                        help="ğŸ’¾ **ç¼“å­˜**: ä¿å­˜å¤„ç†ç»“æœçš„å¤©æ•°ã€‚7å¤©å¹³è¡¡æ€§èƒ½å’Œå­˜å‚¨ã€‚")
            
            # Text processing options with Chinese
            st.markdown("**ğŸ“ æ–‡æœ¬å¤„ç†**")
            chunk_size = st.slider("æ–‡æœ¬å—å¤§å°", 500, 2000, 800,
                                 help="ğŸ“„ **æ–‡æœ¬å—**: å¤„ç†æ–‡æœ¬çš„åˆ†æ®µå¤§å°ã€‚800å­—é€‚åˆä¸­æ–‡åˆ†æã€‚")
            chunk_overlap = st.slider("æ–‡æœ¬å—é‡å ", 0, 200, 80,
                                    help="ğŸ”— **é‡å **: æ–‡æœ¬å—é—´çš„é‡å å­—æ•°ã€‚80å­—ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ã€‚")
            remove_stopwords = st.checkbox("ç§»é™¤åœç”¨è¯", value=False,
                                         help="ğŸš« **åœç”¨è¯**: ç§»é™¤'çš„ã€äº†ã€åœ¨'ç­‰å¸¸ç”¨è¯ã€‚æ–‡å­¦åˆ†æä¸å»ºè®®å¯ç”¨ã€‚")

    # --- Search Button ---
    search_button = st.button("ğŸš€ å¼€å§‹æœç´¢", type="primary", use_container_width=True)
    st.markdown('</div>', unsafe_allow_html=True)

    # --- Instructions and Status ---
    if not uploaded_file or not query:
        st.markdown("---")
        st.header("ğŸ“ ä½¿ç”¨æŒ‡å—")
        
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“ ç¬¬ä¸€æ­¥: ä¸Šä¼ PDFæ–‡ä»¶")
            st.markdown("""
            - ç‚¹å‡»ä¸Šæ–¹çš„"æµè§ˆæ–‡ä»¶"é€‰æ‹©æ‚¨çš„PDFæ–‡æ¡£
            - æ”¯æŒçš„æ–‡ä»¶ç±»å‹: ä»…PDF
            - æ–‡ä»¶å°†è‡ªåŠ¨å¤„ç†
            """)
        with col2:
            st.subheader("ğŸ” ç¬¬äºŒæ­¥: è¾“å…¥æœç´¢å…³é”®è¯")
            st.markdown("""
            - åœ¨å…³é”®è¯å­—æ®µä¸­è¾“å…¥æ‚¨çš„æœç´¢è¯
            - ä¾‹å¦‚: "çˆ±æƒ…", "æ­»äº¡", "æƒåŠ›", "é‡å¿ƒ"
            - å¤šä¸ªå…³é”®è¯å°†è‡ªåŠ¨æ‰©å±•
            """)
        
        st.subheader("âš™ï¸ ç¬¬ä¸‰æ­¥: é…ç½®è®¾ç½® (å¯é€‰)")
        st.markdown("""
        - å±•å¼€"é«˜çº§æœç´¢é…ç½®"æ¥è°ƒæ•´å‚æ•°
        - é»˜è®¤è®¾ç½®å·²é’ˆå¯¹å¤§å¤šæ•°æƒ…å†µä¼˜åŒ–
        - æ ¹æ®éœ€è¦å¯ç”¨/ç¦ç”¨åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½
        """)
        
        st.subheader("ğŸš€ ç¬¬å››æ­¥: ç‚¹å‡»'å¼€å§‹æœç´¢'è¿›è¡Œåˆ†æ")
        
        # Show sample data info
        st.markdown("---")
        st.markdown("### ğŸ“š å¯ç”¨ç¤ºä¾‹æ•°æ®")
        data_dir = "data"
        if os.path.exists(data_dir):
            sample_files = [f for f in os.listdir(data_dir) if f.endswith('.pdf')]
            if sample_files:
                st.success(f"åœ¨dataç›®å½•ä¸­æ‰¾åˆ° {len(sample_files)} ä¸ªç¤ºä¾‹PDFæ–‡ä»¶:")
                for file in sample_files:
                    st.markdown(f"- `{file}`")
                st.info("æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›ç¤ºä¾‹æ–‡ä»¶æµ‹è¯•åº”ç”¨ç¨‹åºã€‚")
            else:
                st.warning("åœ¨dataç›®å½•ä¸­æœªæ‰¾åˆ°ç¤ºä¾‹PDFæ–‡ä»¶ã€‚")
        else:
            st.info("è¦ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œè¯·å°†PDFæ–‡ä»¶æ”¾åœ¨'data'ç›®å½•ä¸­ã€‚")

    # --- Main Content ---
    if uploaded_file and query and search_button:
        # Create progress container
        progress_container = st.container()
        
        try:
            with st.spinner("æ­£åœ¨å¤„ç†..."):
                with progress_container:
                    st.markdown("### ğŸ”„ å¤„ç†è¿›åº¦")
                    progress_placeholder = st.empty()
                    
                    def update_progress(message):
                        try:
                            with progress_placeholder.container():
                                st.info(f"â„¹ï¸ {message}")
                        except Exception as e:
                            st.error(f"è¿›åº¦æ›´æ–°é”™è¯¯: {e}")
                            print(f"è¿›åº¦æ›´æ–°é”™è¯¯: {e}")
                    
                    # Process PDF
                    update_progress("ğŸ“„ æ­£åœ¨å¤„ç†PDFæ–‡æ¡£...")
                    try:
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                            tmp_file.write(uploaded_file.getvalue())
                            update_progress("ğŸ“„ ä¸´æ—¶æ–‡ä»¶åˆ›å»ºæˆåŠŸ...")
                            # Pass text processing settings to process_pdf
                            # Use session state values if available, otherwise use current values
                            current_chunk_size = st.session_state.get('chunk_size', chunk_size)
                            current_chunk_overlap = st.session_state.get('chunk_overlap', chunk_overlap)
                            current_remove_stopwords = st.session_state.get('remove_stopwords', remove_stopwords)
                            
                            texts = process_pdf(tmp_file.name, 
                                              chunk_size=current_chunk_size, 
                                              chunk_overlap=current_chunk_overlap, 
                                              remove_stopwords=current_remove_stopwords)
                            update_progress(f"ğŸ“„ PDFè§£æå®Œæˆ - ä½¿ç”¨è®¾ç½®: å—å¤§å°={current_chunk_size}, é‡å ={current_chunk_overlap}, ç§»é™¤åœç”¨è¯={current_remove_stopwords}")
                        os.unlink(tmp_file.name)
                        update_progress("ğŸ“„ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ...")
                    except Exception as e:
                        st.error(f"PDFå¤„ç†å¤±è´¥: {e}")
                        traceback.print_exc()
                        return

                    if not texts:
                        st.error("æ— æ³•ä»PDFä¸­æå–æ–‡æœ¬ã€‚")
                        return

                    update_progress(f"âœ… PDFå¤„ç†æˆåŠŸ - æå–äº† {len(texts)} ä¸ªæ–‡æœ¬å—")

                    # Keyword Expansion
                    final_query = query
                    if use_keyword_expansion:
                        update_progress(f"ğŸ” æ­£åœ¨æ‰©å±•å…³é”®è¯: '{query}'...")
                        try:
                            expander = KeywordExpander(method=expander_method, document_type=document_type)
                            update_progress(f"ğŸ” KeywordExpanderåˆå§‹åŒ–æˆåŠŸ...")
                            expanded_keywords = expander.expand_keywords(query.split(), max_synonyms_per_word=max_synonyms, max_related_per_word=max_related_words, semantic_threshold=semantic_threshold, use_hierarchical=use_hierarchical)
                            final_query = " ".join(expanded_keywords.keys())
                            st.session_state.expanded_keywords = expanded_keywords
                            update_progress(f"âœ… å…³é”®è¯æ‰©å±•å®Œæˆ: {len(query.split())} â†’ {len(expanded_keywords)} ä¸ªå…³é”®è¯")
                        except Exception as e:
                            update_progress(f"âŒ å…³é”®è¯æ‰©å±•å¤±è´¥: {e}")
                            traceback.print_exc()
                            final_query = query  # ä½¿ç”¨åŸå§‹æŸ¥è¯¢
                    
                    # Search with retry mechanism
                    if enable_retry:
                        # å…ˆå°è¯•ä¸»è¦æœç´¢æ–¹æ³•
                        if use_hybrid_search:
                            update_progress("ğŸ”„ åˆå§‹åŒ–æ··åˆæœç´¢å¼•æ“...")
                            from modules.retriever import EmbeddingRetriever, BM25Retriever
                            from modules.hybrid_search import create_hybrid_search_engine

                            embedding_retriever = EmbeddingRetriever(model_path)
                            bm25_retriever = BM25Retriever()
                            hybrid_search_engine = create_hybrid_search_engine(
                                embedding_retriever=embedding_retriever,
                                bm25_retriever=bm25_retriever,
                                fusion_method=fusion_method,
                                rrf_k=rrf_k,
                                weights={"bm25": bm25_weight, "embedding": embedding_weight},
                                enable_parallel=enable_parallel
                            )
                            
                            # è®¡ç®—ç›®æ ‡æœç´¢ç»“æœæ•°é‡ï¼Œç¡®ä¿è‡³å°‘èƒ½æ»¡è¶³æœ€ç»ˆéœ€æ±‚
                            target_search_results = max(initial_max_results, final_max_results)
                            
                            update_progress(f"ğŸš€ å¼€å§‹æ··åˆæœç´¢ï¼Œä½¿ç”¨ {len(final_query.split())} ä¸ªå…³é”®è¯...")
                            results = hybrid_search_engine.search([{'content': t} for t in texts], final_query.split(), k=target_search_results, min_results=min_results)
                        else:
                            update_progress("ğŸ” æ‰§è¡Œæ ‡å‡†è¯­ä¹‰æœç´¢...")
                            results = standard_search(final_query, texts, model_path)

                        update_progress(f"ğŸ¯ åˆå§‹æœç´¢å®Œæˆ - æ‰¾åˆ° {len(results)} ä¸ªå€™é€‰ç»“æœ (ç›®æ ‡: {initial_max_results})")
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
                        filtered_results = [r for r in results if r.get('score', 0.0) >= similarity_threshold]
                        
                        # è®¡ç®—ç›®æ ‡ç»“æœæ•°é‡ï¼šç¡®ä¿ä¸å°‘äºmin_resultsï¼Œä½†å¦‚æœç”¨æˆ·è®¾ç½®äº†æ›´å¤§çš„final_max_resultsï¼Œåˆ™ä½¿ç”¨æ›´å¤§çš„å€¼
                        target_results = max(min_results, final_max_results)
                        
                        if len(filtered_results) < target_results:
                            update_progress(f"ğŸ”„ ç»“æœä¸è¶³({len(filtered_results)}<{target_results})ï¼Œå¯åŠ¨æ™ºèƒ½é‡è¯•...")
                            retry_results = retry_search_with_fallbacks(
                                query=final_query,
                                texts=texts,
                                model_path=model_path,
                                target_count=target_results,
                                similarity_threshold=similarity_threshold,
                                update_progress=update_progress
                            )
                            
                            # åˆå¹¶ä¸»æœç´¢å’Œé‡è¯•ç»“æœï¼Œå»é‡
                            existing_indices = {r.get('index', -1) for r in filtered_results}
                            new_retry_results = [r for r in retry_results if r.get('index', -1) not in existing_indices]
                            
                            filtered_results.extend(new_retry_results)
                            update_progress(f"âœ… é‡è¯•å®Œæˆ - æ€»ç»“æœæ•°: {len(filtered_results)}")
                        else:
                            update_progress(f"âœ… åˆå§‹æœç´¢å·²æ»¡è¶³è¦æ±‚ - {len(filtered_results)} ä¸ªç»“æœ")
                            
                    else:
                        # ä¼ ç»Ÿæœç´¢æ–¹å¼ï¼ˆä¸ä½¿ç”¨é‡è¯•ï¼‰
                        if use_hybrid_search:
                            update_progress("ğŸ”„ åˆå§‹åŒ–æ··åˆæœç´¢å¼•æ“...")
                            from modules.retriever import EmbeddingRetriever, BM25Retriever
                            from modules.hybrid_search import create_hybrid_search_engine

                            embedding_retriever = EmbeddingRetriever(model_path)
                            bm25_retriever = BM25Retriever()
                            hybrid_search_engine = create_hybrid_search_engine(
                                embedding_retriever=embedding_retriever,
                                bm25_retriever=bm25_retriever,
                                fusion_method=fusion_method,
                                rrf_k=rrf_k,
                                weights={"bm25": bm25_weight, "embedding": embedding_weight},
                                enable_parallel=enable_parallel
                            )
                            
                            # è®¡ç®—ç›®æ ‡æœç´¢ç»“æœæ•°é‡ï¼Œç¡®ä¿è‡³å°‘èƒ½æ»¡è¶³æœ€ç»ˆéœ€æ±‚
                            target_search_results = max(initial_max_results, final_max_results)
                            
                            update_progress(f"ğŸš€ å¼€å§‹æ··åˆæœç´¢ï¼Œä½¿ç”¨ {len(final_query.split())} ä¸ªå…³é”®è¯...")
                            results = hybrid_search_engine.search([{'content': t} for t in texts], final_query.split(), k=target_search_results, min_results=min_results)
                        else:
                            update_progress("ğŸ” æ‰§è¡Œæ ‡å‡†è¯­ä¹‰æœç´¢...")
                            results = standard_search(final_query, texts, model_path)

                        update_progress(f"ğŸ¯ æœç´¢å®Œæˆ - æ‰¾åˆ° {len(results)} ä¸ªå€™é€‰ç»“æœ (ç›®æ ‡: {initial_max_results})")

                        # Filtering results
                        filtered_results = [r for r in results if r.get('score', 0.0) >= similarity_threshold]
                        
                        # è®¡ç®—ç›®æ ‡ç»“æœæ•°é‡ï¼šç¡®ä¿ä¸å°‘äºmin_resultsï¼Œä½†å¦‚æœç”¨æˆ·è®¾ç½®äº†æ›´å¤§çš„final_max_resultsï¼Œåˆ™ä½¿ç”¨æ›´å¤§çš„å€¼
                        target_results = max(min_results, final_max_results)
                        
                        # ä¼ ç»Ÿçš„æœ€å°‘è¿”å›ç»“æœæœºåˆ¶
                        if len(filtered_results) < target_results and len(results) > 0:
                            update_progress(f"ğŸ”§ ç»“æœä¸è¶³({len(filtered_results)}<{target_results})ï¼Œå¯åŠ¨åŸºç¡€è¡¥å……æœºåˆ¶...")
                            # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰target_resultsä¸ª
                            sorted_results = sorted(results, key=lambda x: x.get('score', 0.0), reverse=True)
                            filtered_results = sorted_results[:min(target_results, len(sorted_results))]
                            
                            # å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œè€Œä¸”æœ‰æ–‡æœ¬ï¼Œå°±éšæœºé€‰æ‹©ä¸€äº›
                            if len(filtered_results) < target_results and len(texts) >= target_results:
                                update_progress(f"ğŸ² éšæœºè¡¥å……ç»“æœä»¥è¾¾åˆ°ç›®æ ‡æ•°é‡({target_results})...")
                                # åˆ›å»ºåŸºç¡€ç»“æœ
                                import random
                                available_indices = list(range(len(texts)))
                                used_indices = {r.get('index', -1) for r in filtered_results}
                                remaining_indices = [i for i in available_indices if i not in used_indices]
                                
                                # éšæœºé€‰æ‹©è¡¥å……ç»“æœ
                                needed = target_results - len(filtered_results)
                                if len(remaining_indices) >= needed:
                                    selected_indices = random.sample(remaining_indices, needed)
                                else:
                                    selected_indices = remaining_indices
                                if len(remaining_indices) >= needed:
                                    selected_indices = random.sample(remaining_indices, needed)
                                else:
                                    selected_indices = remaining_indices
                                
                                for idx in selected_indices:
                                    filtered_results.append({
                                        'text': texts[idx],
                                        'content': texts[idx],
                                        'score': 0.1,  # ä½åˆ†æ•°è¡¨ç¤ºè¿™æ˜¯è¡¥å……ç»“æœ
                                        'page_num': idx + 1,
                                        'index': idx,
                                        'is_fallback': True  # æ ‡è®°ä¸ºè¡¥å……ç»“æœ
                                    })
                                
                                update_progress(f"ğŸ“ å·²è¡¥å…… {len(selected_indices)} ä¸ªç»“æœï¼Œæ€»è®¡ {len(filtered_results)} ä¸ª")
                
                # Show search statistics with score details
                if results:
                    scores = [r.get('score', 0.0) for r in results]
                    update_progress(f"ğŸ“Š åˆ†æ•°åˆ†æ: èŒƒå›´ {min(scores):.4f} - {max(scores):.4f}, å¹³å‡: {sum(scores)/len(scores):.4f}")
                    update_progress(f"âœ… è¿‡æ»¤å (é˜ˆå€¼ {similarity_threshold:.3f}): {len(filtered_results)} ä¸ªç»“æœ")
                    if len(filtered_results) == 0 and len(results) > 0:
                        st.warning(f"âš ï¸ æ‰€æœ‰ç»“æœéƒ½è¢«è¿‡æ»¤æ‰äº†ã€‚è¯·å°è¯•å°†ç›¸ä¼¼åº¦é˜ˆå€¼é™ä½åˆ° {min(scores):.4f} ä»¥ä¸‹")

                # Reranking
                if reranker_type != "none" and filtered_results:
                    update_progress(f"ğŸ”„ åº”ç”¨{reranker_type.replace('_', ' ').title()}é‡æ–°æ’åº...")
                    try:
                        reranker_params = {"keywords": final_query.split()}
                        if reranker_type == "simple":
                            reranker_params["threshold"] = simple_reranker_threshold
                        elif reranker_type == "cross_encoder":
                            reranker_params["model_name"] = cross_encoder_model
                            reranker_params["threshold"] = cross_encoder_threshold
                        elif reranker_type == "enhanced_cross_encoder":
                            reranker_params["enable_ensemble"] = enable_ensemble
                            reranker_params["enable_consistency_check"] = enable_consistency_check
                            reranker_params["enable_diversity"] = enable_diversity
                            reranker_params["enable_context_aware"] = enable_context_aware

                        reranker = create_reranker(reranker_type, **reranker_params)
                        candidates = [{'content': r.get('text', r.get('content', '')), 'score': r.get('score', 0.0), 
                                     'page_num': r.get('page_num', 1), 'index': r.get('index', i)} 
                                     for i, r in enumerate(filtered_results)]
                        
                        update_progress(f"ğŸ”„ å¼€å§‹é‡æ’ {len(candidates)} ä¸ªå€™é€‰ç»“æœ...")
                        reranked_candidates = reranker.rerank(candidates, k=final_max_results)
                        update_progress(f"âœ… é‡æ’å®Œæˆï¼Œè·å¾— {len(reranked_candidates)} ä¸ªç»“æœ")
                        
                        final_results = [{'text': c['content'], 'score': c.get('rerank_score', c.get('score', 0)),
                                        'page_num': c.get('page_num', 1), 'index': c.get('index', i)} 
                                        for i, c in enumerate(reranked_candidates)]
                        update_progress(f"âœ… é‡æ–°æ’åºå®Œæˆ - ä» {len(candidates)} ä¸ªå€™é€‰ä¸­é€‰å‡º {len(final_results)} ä¸ªæœ€ç»ˆç»“æœ")
                        
                    except Exception as e:
                        update_progress(f"âŒ é‡æ’å¤±è´¥: {str(e)}")
                        print(f"é‡æ’é”™è¯¯è¯¦æƒ…: {e}")
                        traceback.print_exc()
                        # å¦‚æœé‡æ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ
                        final_results = []
                        for r in sorted(filtered_results, key=lambda x: x.get('score', 0.0), reverse=True)[:final_max_results]:
                            final_results.append({
                                'text': r.get('text', r.get('content', '')),
                                'score': r.get('score', 0.0),
                                'page_num': r.get('page_num', 1),
                                'index': r.get('index', len(final_results))
                            })
                        update_progress(f"ğŸ”„ ä½¿ç”¨åŸå§‹æ’åºç»“æœï¼Œå…± {len(final_results)} ä¸ª")
                else:
                    # Ensure consistent format for results without reranking
                    final_results = []
                    for r in sorted(filtered_results, key=lambda x: x.get('score', 0.0), reverse=True)[:final_max_results]:
                        final_results.append({
                            'text': r.get('text', r.get('content', '')),
                            'score': r.get('score', 0.0),
                            'page_num': r.get('page_num', 1),
                            'index': r.get('index', len(final_results))
                        })
                
                # Normalize scores for better user experience (show 30-100 range)
                if final_results:
                    raw_scores = [r.get('score', 0.0) for r in final_results]
                    if max(raw_scores) > min(raw_scores):
                        # Normalize to 30-100 scale for better user experience
                        min_score, max_score = min(raw_scores), max(raw_scores)
                        for result in final_results:
                            raw_score = result.get('score', 0.0)
                            # Scale from original range to 30-100 range
                            normalized_score = 30 + ((raw_score - min_score) / (max_score - min_score)) * 70
                            result['score'] = normalized_score
                            # Store original raw score for debugging if needed
                            result['raw_score'] = raw_score
                    else:
                        # All scores are the same, set to high score
                        for result in final_results:
                            result['score'] = 85.0  # Set to a good score when all are equal
                            result['raw_score'] = result.get('score', 0.0)
                    
                    update_progress(f"ğŸ¯ åˆ†æ•°å½’ä¸€åŒ–å®Œæˆ - å·²åˆ†é…30-100åˆ†æ•°èŒƒå›´")
                else:
                    # å¦‚æœæ²¡æœ‰ç»“æœï¼Œåˆ›å»ºç©ºåˆ—è¡¨
                    final_results = []
                    update_progress("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœ")
                
                # ä¿å­˜æœç´¢ç»“æœåˆ° session_stateï¼ˆè¿™æ˜¯å…³é”®ï¼ï¼‰
                st.session_state.results = final_results
                st.session_state.texts = texts
                st.session_state.query = query
                
                # è°ƒè¯•ä¿¡æ¯
                if final_results:
                    update_progress(f"ğŸ¯ å·²ä¿å­˜ {len(final_results)} ä¸ªæœç´¢ç»“æœåˆ° session_state")
                    print(f"DEBUG: final_results é•¿åº¦: {len(final_results)}")
                    print(f"DEBUG: ç¬¬ä¸€ä¸ªç»“æœ: {final_results[0] if final_results else 'æ— '}")
                else:
                    update_progress("â„¹ï¸ æ²¡æœ‰æœç´¢ç»“æœä¿å­˜åˆ° session_state")
                    print("DEBUG: final_results ä¸ºç©º")
                
                update_progress("âœ… æœç´¢å¤„ç†å®Œæˆ!")

                # æ ¹æ®ç”¨æˆ·é€‰æ‹©ï¼Œå†³å®šæ˜¯å¦ç«‹å³æ‰§è¡Œåˆ†æ
                if auto_perform_analysis:
                    update_progress("ğŸ¤– å·²å¯ç”¨è‡ªåŠ¨åˆ†æï¼Œå¼€å§‹æ‰§è¡Œ...")
                    try:
                        # ä½¿ç”¨å®Œæ•´çš„æ–‡æœ¬å—è¿›è¡Œåˆ†æ
                        full_text_chunks = [{'content': text, 'page_num': i+1} for i, text in enumerate(texts)]
                        analysis_results = {}

                        if perform_theme_analysis:
                            theme_analyzer = LiteraryThemeAnalyzer()
                            analysis_results['theme_analysis'] = theme_analyzer.analyze_text_themes(
                                full_text_chunks, progress_callback=lambda msg: update_progress(f"ğŸ¨ {msg}")
                            )
                        
                        if perform_literary_analysis:
                            literary_analyzer = LiteraryAnalyzer()
                            comprehensive_results = literary_analyzer.generate_comprehensive_analysis(
                                full_text_chunks, progress_callback=lambda msg: update_progress(f"ğŸ“š {msg}")
                            )
                            analysis_results['character_analysis'] = comprehensive_results['characters']
                            analysis_results['emotion_analysis'] = comprehensive_results['emotions']
                            analysis_results['narrative_analysis'] = comprehensive_results['narrative']

                        st.session_state.analysis_results = analysis_results
                        update_progress("âœ… è‡ªåŠ¨åˆ†æå®Œæˆ!")

                    except Exception as e:
                        update_progress(f"âŒ è‡ªåŠ¨åˆ†æå¤±è´¥: {e}")
                        traceback.print_exc()
                else:
                    # æ¸…é™¤æ—§çš„åˆ†æç»“æœï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨è§¦å‘
                    if 'analysis_results' in st.session_state:
                        del st.session_state['analysis_results']
                    update_progress("â„¹ï¸ è‡ªåŠ¨åˆ†æå·²ç¦ç”¨ã€‚è¯·åœ¨ä¸‹æ–¹æ‰‹åŠ¨å¼€å§‹åˆ†æã€‚")
                
                update_progress("ğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆ! è¯·æŸ¥çœ‹ä¸‹æ–¹ç»“æœã€‚")
                
        except Exception as e:
            st.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            st.code(traceback.format_exc())
            print(f"å…¨å±€é”™è¯¯è¯¦æƒ…: {e}")
            traceback.print_exc()

    # --- Results, Analysis, and Visualization Display ---
    if 'results' in st.session_state:
        print(f"DEBUG: åœ¨ session_state ä¸­æ‰¾åˆ° resultsï¼Œé•¿åº¦: {len(st.session_state.results) if st.session_state.results else 'ç©ºæˆ–None'}")
        
        # Enhanced results header
        st.markdown("---")
        st.header("ğŸ¯ æœç´¢ç»“æœ")
        
        display_results(st.session_state.results)
        
    # --- Analysis Section (Always available after search or when texts are loaded) ---
    if 'texts' in st.session_state:
        st.markdown("---")
        
        # Enhanced analysis header
        st.header("ğŸ­ æ–‡å­¦åˆ†æ")
        st.info("ğŸ“ æ³¨æ„ï¼šæ–‡å­¦åˆ†ææ˜¯é’ˆå¯¹æ•´ä¸ªæ–‡æ¡£è¿›è¡Œçš„ï¼Œä»¥æä¾›æ›´å…¨é¢çš„è§è§£ã€‚")

        # Enhanced analysis options with cool layout
        st.subheader("é€‰æ‹©åˆ†æç±»å‹")
        
        # Create columns for analysis options
        col1, col2 = st.columns(2)
        
        with col1:
            analysis_options = {
                'theme_analysis': st.checkbox("ğŸ¨ æ·±å…¥ä¸»é¢˜åˆ†æ", value=True, help="åˆ†ææ–‡æœ¬çš„ä¸»é¢˜ã€æƒ…æ„Ÿå’Œæ–‡å­¦æ‰‹æ³•ã€‚"),
                'character_analysis': st.checkbox("ğŸ‘¥ äººç‰©å…³ç³»åˆ†æ", value=True, help="åˆ†æäººç‰©å…±ç°ã€é¢‘ç‡å’Œå…³ç³»ç½‘ç»œã€‚"),
            }
        
        with col2:
            analysis_options.update({
                'emotion_analysis': st.checkbox("ğŸ’­ æƒ…æ„Ÿå€¾å‘åˆ†æ", value=True, help="åˆ†ææ–‡æœ¬ä¸­çš„æƒ…æ„Ÿåˆ†å¸ƒå’Œå¼ºåº¦ã€‚"),
                'narrative_analysis': st.checkbox("ğŸ“– å™äº‹ç»“æ„åˆ†æ", value=True, help="åˆ†æåœºæ™¯åˆ†å¸ƒå’Œå™äº‹èŠ‚å¥ã€‚")
            })

        if st.button("ğŸš€ å¼€å§‹è¿›è¡Œæ–‡å­¦åˆ†æ", key="start_analysis"):
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬æ•°æ®
            if 'texts' not in st.session_state:
                st.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬æ•°æ®ã€‚è¯·å…ˆæ‰§è¡Œæœç´¢ã€‚")
            else:
                with st.spinner("æ­£åœ¨æ‰§è¡Œæ–‡å­¦åˆ†æ... è¯·ç¨å€™..."):
                    try:
                        analysis_results = {}
                        # ä½¿ç”¨å®Œæ•´çš„æ–‡æœ¬å—è¿›è¡Œåˆ†æ
                        full_text_chunks = [{'content': text, 'page_num': i+1} for i, text in enumerate(st.session_state.texts)]
                        
                        if analysis_options['theme_analysis']:
                            st.info("ğŸ¨ å¼€å§‹ä¸»é¢˜åˆ†æ...")
                            theme_analyzer = LiteraryThemeAnalyzer()
                            theme_result = theme_analyzer.analyze_text_themes(
                                full_text_chunks, progress_callback=st.info
                            )
                            analysis_results['theme_analysis'] = theme_result
                            st.success(f"âœ… ä¸»é¢˜åˆ†æå®Œæˆï¼ç»“æœç±»å‹: {type(theme_result)}")
                        
                        # LiteraryAnalyzerå¯ä»¥ä¸€æ¬¡æ€§å¤„ç†å¤šç§åˆ†æ
                        if analysis_options['character_analysis'] or analysis_options['emotion_analysis'] or analysis_options['narrative_analysis']:
                            st.info("ğŸ‘¥ å¼€å§‹æ–‡å­¦åˆ†æ...")
                            literary_analyzer = LiteraryAnalyzer()
                            comprehensive_results = literary_analyzer.generate_comprehensive_analysis(
                                full_text_chunks, progress_callback=st.info
                            )
                            st.success(f"âœ… æ–‡å­¦åˆ†æå®Œæˆï¼ç»“æœç±»å‹: {type(comprehensive_results)}")
                            
                            if analysis_options['character_analysis']:
                                analysis_results['character_analysis'] = comprehensive_results['characters']
                            if analysis_options['emotion_analysis']:
                                analysis_results['emotion_analysis'] = comprehensive_results['emotions']
                            if analysis_options['narrative_analysis']:
                                analysis_results['narrative_analysis'] = comprehensive_results['narrative']
                            
                            # å­˜å‚¨å®Œæ•´çš„åˆ†æç»“æœç”¨äºå¯è§†åŒ–
                            analysis_results['comprehensive_results'] = comprehensive_results

                        # ä¿å­˜ç»“æœå¹¶æ˜¾ç¤ºæ‘˜è¦
                        st.session_state.analysis_results = analysis_results
                        
                        # æ˜¾ç¤ºåˆ†ææ‘˜è¦
                        st.success("ğŸ‰ æ–‡å­¦åˆ†æå®Œæˆï¼")
                        st.info(f"ğŸ“Š ç”Ÿæˆäº† {len(analysis_results)} ç§åˆ†æç»“æœ")
                        for key in analysis_results.keys():
                            if key == 'comprehensive_results':
                                continue  # è·³è¿‡ç»¼åˆç»“æœçš„æ˜¾ç¤º
                            result_data = analysis_results[key]
                            if result_data:
                                if isinstance(result_data, dict):
                                    st.write(f"âœ… {key}: {len(result_data)} ä¸ªæ•°æ®é¡¹")
                                elif isinstance(result_data, list):
                                    st.write(f"âœ… {key}: {len(result_data)} ä¸ªæ¡ç›®")
                                else:
                                    st.write(f"âœ… {key}: å·²å®Œæˆ (ç±»å‹: {type(result_data)})")
                            else:
                                st.warning(f"âš ï¸ {key}: æ— æ•°æ®")

                    except Exception as e:
                        st.error(f"æ–‡å­¦åˆ†æå¤±è´¥: {e}")
                        st.code(traceback.format_exc())

        # æ˜¾ç¤ºåˆ†æç»“æœ
        if 'analysis_results' in st.session_state:
            st.header("ğŸ“Š åˆ†æç»“æœ")
            results = st.session_state.analysis_results
            
            # æ·»åŠ å¯è§†åŒ–éƒ¨åˆ†
            if results.get('comprehensive_results'):
                st.subheader("ğŸ“ˆ æ–‡å­¦åˆ†æå¯è§†åŒ–")
                comprehensive_data = results['comprehensive_results']
                
                try:
                    # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€åˆ›å»ºå¯è§†åŒ–å™¨
                    chart_lang = st.session_state.get('chart_language', 'zh')
                    visualizer = AdvancedVisualizer(language=chart_lang)
                    
                    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
                    fig = visualizer.plot_literary_analysis(comprehensive_data)
                    if fig:
                        st.pyplot(fig)
                        st.success("âœ… å¯è§†åŒ–å›¾è¡¨ç”ŸæˆæˆåŠŸï¼")
                        
                        # æä¾›å›¾è¡¨è¯´æ˜
                        st.info("""
                        ğŸ“Š **å›¾è¡¨è¯´æ˜**ï¼š
                        - **å·¦ä¸Š**: ä¸»è¦äººç‰©å‡ºç°é¢‘ç‡ï¼Œæ˜¾ç¤ºä½œå“ä¸­æœ€é‡è¦çš„è§’è‰²
                        - **å³ä¸Š**: ä¸»é¢˜åˆ†å¸ƒé¥¼å›¾ï¼Œå±•ç¤ºæ–‡å­¦ä½œå“çš„ä¸»è¦ä¸»é¢˜æ¯”ä¾‹
                        - **å·¦ä¸­**: æƒ…æ„Ÿå€¾å‘åˆ†æï¼Œæ˜¾ç¤ºä½œå“ä¸­ç§¯æã€æ¶ˆæå’Œä¸­æ€§æƒ…æ„Ÿçš„æ¯”ä¾‹
                        - **å³ä¸­**: äººç‰©å…±ç°å…³ç³»ï¼Œå±•ç¤ºå“ªäº›äººç‰©ç»å¸¸ä¸€èµ·å‡ºç°
                        - **å·¦ä¸‹**: å†…å®¹åœ¨å„é¡µé¢çš„åˆ†å¸ƒï¼Œå¸®åŠ©ç†è§£å™äº‹ç»“æ„
                        - **å³ä¸‹**: å…³é”®ç»Ÿè®¡ä¿¡æ¯æ±‡æ€»
                        """)
                    else:
                        st.warning("âš ï¸ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå¤±è´¥")
                        
                except Exception as e:
                    st.error(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                    st.code(traceback.format_exc())
            
            if results.get('theme_analysis'):
                with st.expander("ğŸ¨ æ·±å…¥ä¸»é¢˜åˆ†æç»“æœ", expanded=False):
                    st.write("**åˆ†æå®Œæˆï¼ä»¥ä¸‹æ˜¯è¯¦ç»†ç»“æœï¼š**")
                    # å°è¯•æ›´å‹å¥½çš„æ˜¾ç¤ºæ–¹å¼
                    theme_data = results['theme_analysis']
                    if isinstance(theme_data, dict):
                        for key, value in theme_data.items():
                            st.subheader(f"ğŸ“Œ {key}")
                            if isinstance(value, (dict, list)):
                                st.json(value)
                            else:
                                st.write(value)
                    else:
                        st.json(theme_data)

            if results.get('character_analysis'):
                with st.expander("ğŸ‘¥ äººç‰©å…³ç³»åˆ†æç»“æœ", expanded=False):
                    st.write("**åˆ†æå®Œæˆï¼ä»¥ä¸‹æ˜¯è¯¦ç»†ç»“æœï¼š**")
                    char_data = results['character_analysis']
                    if isinstance(char_data, dict):
                        for key, value in char_data.items():
                            st.subheader(f"ğŸ‘¤ {key}")
                            if isinstance(value, (dict, list)):
                                st.json(value)
                            else:
                                st.write(value)
                    else:
                        st.json(char_data)

            if results.get('emotion_analysis'):
                with st.expander("ğŸ’­ æƒ…æ„Ÿå€¾å‘åˆ†æç»“æœ", expanded=False):
                    st.write("**åˆ†æå®Œæˆï¼ä»¥ä¸‹æ˜¯è¯¦ç»†ç»“æœï¼š**")
                    emotion_data = results['emotion_analysis']
                    if isinstance(emotion_data, dict):
                        for key, value in emotion_data.items():
                            st.subheader(f"ğŸ˜Š {key}")
                            if isinstance(value, (dict, list)):
                                st.json(value)
                            else:
                                st.write(value)
                    else:
                        st.json(emotion_data)

            if results.get('narrative_analysis'):
                with st.expander("ğŸ“– å™äº‹ç»“æ„åˆ†æç»“æœ", expanded=False):
                    st.write("**åˆ†æå®Œæˆï¼ä»¥ä¸‹æ˜¯è¯¦ç»†ç»“æœï¼š**")
                    narrative_data = results['narrative_analysis']
                    if isinstance(narrative_data, dict):
                        for key, value in narrative_data.items():
                            st.subheader(f"ğŸ“š {key}")
                            if isinstance(value, (dict, list)):
                                st.json(value)
                            else:
                                st.write(value)
                    else:
                        st.json(narrative_data)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            with st.expander("ğŸ”§ è°ƒè¯•ä¿¡æ¯ (å¼€å‘ç”¨)", expanded=False):
                st.write("åˆ†æç»“æœçš„æ•°æ®ç±»å‹å’Œå†…å®¹ï¼š")
                st.write(f"æ•°æ®ç±»å‹: {type(results)}")
                st.write(f"é”®å€¼: {list(results.keys()) if isinstance(results, dict) else 'Not a dict'}")
                st.json(results)
    else:
        print("DEBUG: session_state ä¸­æ²¡æœ‰æ‰¾åˆ° results æˆ– texts")

    # --- Enhanced visualizations section (Always check after results/analysis) ---
    if st.session_state.get('perform_visualization', False) and 'results' in st.session_state:
        st.markdown("---")
        st.header("ğŸ“Š å¯è§†åŒ–å›¾è¡¨")
        st.info("é€šè¿‡ç²¾ç¾çš„å›¾è¡¨æ·±å…¥ç†è§£æœç´¢ç»“æœå’Œæ–‡æœ¬æ¨¡å¼")
        
        with st.spinner("ğŸ¨ æ­£åœ¨ç”Ÿæˆç²¾ç¾çš„å¯è§†åŒ–å›¾è¡¨..."):
            try:
                # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€åˆ›å»ºå¯è§†åŒ–å™¨
                chart_lang = st.session_state.get('chart_language', 'zh')
                visualizer = AdvancedVisualizer(language=chart_lang)
                
                # å®‰å…¨åœ°è·å–å…³é”®è¯
                expanded_keywords = st.session_state.get('expanded_keywords', {})
                query_keywords = st.session_state.get('query', '').split() if st.session_state.get('query') else []
                keywords_for_viz = list(expanded_keywords.keys()) or query_keywords
                
                # å¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œè·³è¿‡å¯è§†åŒ–
                if not keywords_for_viz:
                    st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœç´¢å…³é”®è¯ï¼Œæ— æ³•ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚")
                else:
                    results_for_viz = st.session_state.results.copy()
                    
                    # ä¸ºå¯è§†åŒ–æ·»åŠ found_keywordså­—æ®µ
                    search_keywords = keywords_for_viz
                    for result in results_for_viz:
                        if 'found_keywords' not in result:
                            # ç®€å•åŒ¹é…ï¼šæ£€æŸ¥å“ªäº›æœç´¢å…³é”®è¯åœ¨ç»“æœæ–‡æœ¬ä¸­
                            text = result.get('text', '').lower()
                            found = []
                            for kw in search_keywords:
                                if kw.lower() in text:
                                    # è®¡ç®—å‡ºç°æ¬¡æ•°
                                    count = text.count(kw.lower())
                                    found.extend([kw] * count)
                            result['found_keywords'] = found

                    # ä» session_state è·å–å¯è§†åŒ–å‚æ•°
                    wordcloud_max_words = st.session_state.get('wordcloud_max_words', 100)
                    heatmap_top_n = st.session_state.get('heatmap_top_n', 8)

                    # Generate charts in a grid layout
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # ç”Ÿæˆå›¾è¡¨å¹¶æ˜¾ç¤º
                        fig1 = visualizer.plot_page_distribution(results_for_viz, keywords_for_viz)
                        if fig1:
                            st.pyplot(fig1)
                            
                        fig3 = visualizer.plot_word_cloud(results_for_viz, keywords_for_viz, max_words=wordcloud_max_words)
                        if fig3:
                            st.pyplot(fig3)
                    
                    with col2:
                        try:
                            print("DEBUG: å¼€å§‹ç”Ÿæˆä¸»é¢˜é¢‘ç‡å›¾")
                            fig2 = visualizer.plot_theme_frequency(results_for_viz, keywords_for_viz, top_n=heatmap_top_n)
                            if fig2:
                                st.pyplot(fig2)
                                print("DEBUG: ä¸»é¢˜é¢‘ç‡å›¾æ˜¾ç¤ºæˆåŠŸ")
                            else:
                                st.error("ä¸»é¢˜é¢‘ç‡å›¾ç”Ÿæˆå¤±è´¥")
                        except Exception as e:
                            print(f"DEBUG: ä¸»é¢˜é¢‘ç‡å›¾ç”Ÿæˆé”™è¯¯: {e}")
                            st.error(f"ä¸»é¢˜é¢‘ç‡å›¾ç”Ÿæˆå¤±è´¥: {e}")
                            
                        try:
                            print("DEBUG: å¼€å§‹ç”Ÿæˆå…±ç°çƒ­åŠ›å›¾")
                            fig4 = visualizer.plot_cooccurrence_heatmap(results_for_viz, keywords_for_viz, top_n=heatmap_top_n)
                            if fig4:
                                st.pyplot(fig4)
                                print("DEBUG: å…±ç°çƒ­åŠ›å›¾æ˜¾ç¤ºæˆåŠŸ")
                            else:
                                st.error("å…±ç°çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥")
                        except Exception as e:
                            print(f"DEBUG: å…±ç°çƒ­åŠ›å›¾ç”Ÿæˆé”™è¯¯: {e}")
                            import traceback
                            traceback.print_exc()
                            st.error(f"å…±ç°çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
                            # æ˜¾ç¤ºé”™è¯¯è¯¦æƒ…
                            with st.expander("é”™è¯¯è¯¦æƒ…", expanded=False):
                                st.code(traceback.format_exc())
            
            except Exception as e:
                st.error(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                st.code(traceback.format_exc())

def retry_search_with_fallbacks(query, texts, model_path, target_count=3, similarity_threshold=0.3, update_progress=None):
    """
    æ™ºèƒ½é‡è¯•æœç´¢æœºåˆ¶ - å½“ç»“æœä¸è¶³æ—¶å°è¯•å¤šç§æœç´¢ç­–ç•¥
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        texts: æ–‡æœ¬åˆ—è¡¨
        model_path: æ¨¡å‹è·¯å¾„
        target_count: ç›®æ ‡ç»“æœæ•°é‡
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        update_progress: è¿›åº¦æ›´æ–°å‡½æ•°
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    def log_progress(msg):
        if update_progress:
            update_progress(msg)
        else:
            print(msg)
    
    all_results = []
    used_methods = []
    
    # ç­–ç•¥1: æ ‡å‡†è¯­ä¹‰æœç´¢
    try:
        log_progress("ğŸ”„ å°è¯•ç­–ç•¥1: æ ‡å‡†è¯­ä¹‰æœç´¢...")
        results = standard_search(query, texts, model_path)
        filtered_results = [r for r in results if r.get('score', 0.0) >= similarity_threshold]
        
        if len(filtered_results) >= target_count:
            log_progress(f"âœ… ç­–ç•¥1æˆåŠŸ: æ‰¾åˆ° {len(filtered_results)} ä¸ªç»“æœ")
            return filtered_results[:target_count]
        
        all_results.extend(filtered_results)
        used_methods.append("è¯­ä¹‰æœç´¢")
        log_progress(f"ğŸ” ç­–ç•¥1éƒ¨åˆ†æˆåŠŸ: {len(filtered_results)} ä¸ªç»“æœï¼Œç»§ç»­å°è¯•...")
        
    except Exception as e:
        log_progress(f"âš ï¸ ç­–ç•¥1å¤±è´¥: {e}")
    
    # ç­–ç•¥2: é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼çš„è¯­ä¹‰æœç´¢
    if len(all_results) < target_count:
        try:
            log_progress("ğŸ”„ å°è¯•ç­–ç•¥2: é™ä½é˜ˆå€¼çš„è¯­ä¹‰æœç´¢...")
            lower_threshold = max(0.1, similarity_threshold * 0.5)
            results = standard_search(query, texts, model_path)
            filtered_results = [r for r in results if r.get('score', 0.0) >= lower_threshold]
            
            # å»é‡å¹¶åˆå¹¶ç»“æœ
            existing_indices = {r.get('index', -1) for r in all_results}
            new_results = [r for r in filtered_results if r.get('index', -1) not in existing_indices]
            
            all_results.extend(new_results)
            used_methods.append("ä½é˜ˆå€¼è¯­ä¹‰æœç´¢")
            log_progress(f"ğŸ” ç­–ç•¥2: æ–°å¢ {len(new_results)} ä¸ªç»“æœï¼Œå½“å‰æ€»è®¡ {len(all_results)} ä¸ª")
            
        except Exception as e:
            log_progress(f"âš ï¸ ç­–ç•¥2å¤±è´¥: {e}")
    
    # ç­–ç•¥3: BM25å…³é”®è¯æœç´¢
    if len(all_results) < target_count:
        try:
            log_progress("ğŸ”„ å°è¯•ç­–ç•¥3: BM25å…³é”®è¯æœç´¢...")
            from modules.retriever import BM25Retriever
            
            bm25_retriever = BM25Retriever()
            bm25_retriever.keywords = query.split()
            
            text_chunks = [{'content': text, 'page_num': i+1, 'id': f'chunk_{i}'} for i, text in enumerate(texts)]
            bm25_results = bm25_retriever.retrieve(text_chunks, len(texts))
            
            # è½¬æ¢æ ¼å¼å¹¶å»é‡
            existing_indices = {r.get('index', -1) for r in all_results}
            new_results = []
            
            for result in bm25_results:
                if result.get('index', -1) not in existing_indices:
                    new_results.append({
                        'text': result.get('content', ''),
                        'content': result.get('content', ''),
                        'score': result.get('bm25_score', 0.0) * 10,  # è°ƒæ•´åˆ†æ•°èŒƒå›´
                        'page_num': result.get('page_num', 1),
                        'index': result.get('index', -1),
                        'method': 'BM25'
                    })
            
            all_results.extend(new_results)
            used_methods.append("BM25æœç´¢")
            log_progress(f"ğŸ” ç­–ç•¥3: æ–°å¢ {len(new_results)} ä¸ªç»“æœï¼Œå½“å‰æ€»è®¡ {len(all_results)} ä¸ª")
            
        except Exception as e:
            log_progress(f"âš ï¸ ç­–ç•¥3å¤±è´¥: {e}")
    
    # ç­–ç•¥4: å…³é”®è¯æ‰©å±•åé‡æ–°æœç´¢
    if len(all_results) < target_count:
        try:
            log_progress("ğŸ”„ å°è¯•ç­–ç•¥4: å…³é”®è¯æ‰©å±•æœç´¢...")
            from modules.keyword_expander import KeywordExpander
            
            expander = KeywordExpander(method='wordnet', document_type='literary')
            expanded_keywords = expander.expand_keywords(query.split(), max_synonyms_per_word=2, max_related_per_word=1)
            expanded_query = " ".join(expanded_keywords.keys())
            
            results = standard_search(expanded_query, texts, model_path)
            lower_threshold = max(0.05, similarity_threshold * 0.3)
            filtered_results = [r for r in results if r.get('score', 0.0) >= lower_threshold]
            
            # å»é‡å¹¶åˆå¹¶ç»“æœ
            existing_indices = {r.get('index', -1) for r in all_results}
            new_results = [r for r in filtered_results if r.get('index', -1) not in existing_indices]
            
            all_results.extend(new_results)
            used_methods.append("æ‰©å±•å…³é”®è¯æœç´¢")
            log_progress(f"ğŸ” ç­–ç•¥4: æ–°å¢ {len(new_results)} ä¸ªç»“æœï¼Œå½“å‰æ€»è®¡ {len(all_results)} ä¸ª")
            
        except Exception as e:
            log_progress(f"âš ï¸ ç­–ç•¥4å¤±è´¥: {e}")
    
    # ç­–ç•¥5: ç®€å•æ–‡æœ¬åŒ¹é…ï¼ˆæœ€åæ‰‹æ®µï¼‰
    if len(all_results) < target_count:
        try:
            log_progress("ğŸ”„ å°è¯•ç­–ç•¥5: ç®€å•æ–‡æœ¬åŒ¹é…...")
            query_words = query.lower().split()
            existing_indices = {r.get('index', -1) for r in all_results}
            new_results = []
            
            for i, text in enumerate(texts):
                if i not in existing_indices:
                    text_lower = text.lower()
                    score = 0
                    for word in query_words:
                        score += text_lower.count(word)
                    
                    if score > 0:  # åªè¦åŒ…å«ä»»ä½•æŸ¥è¯¢è¯å°±ç®—åŒ¹é…
                        # å½’ä¸€åŒ–åˆ†æ•°
                        max_possible_score = len(query_words) * max(1, len(text.split()) // 20)
                        normalized_score = min(score / max_possible_score if max_possible_score > 0 else 0, 1.0)
                        
                        new_results.append({
                            'text': text,
                            'content': text,
                            'score': normalized_score * 100,  # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
                            'page_num': i + 1,
                            'index': i,
                            'method': 'æ–‡æœ¬åŒ¹é…'
                        })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶å–æœ€å¥½çš„ç»“æœ
            new_results.sort(key=lambda x: x.get('score', 0), reverse=True)
            needed = target_count - len(all_results)
            all_results.extend(new_results[:needed])
            used_methods.append("æ–‡æœ¬åŒ¹é…")
            log_progress(f"ğŸ” ç­–ç•¥5: æ–°å¢ {len(new_results[:needed])} ä¸ªç»“æœï¼Œå½“å‰æ€»è®¡ {len(all_results)} ä¸ª")
            
        except Exception as e:
            log_progress(f"âš ï¸ ç­–ç•¥5å¤±è´¥: {e}")
    
    # æœ€åçš„éšæœºè¡¥å……ï¼ˆå¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼‰
    if len(all_results) < target_count and len(texts) > len(all_results):
        log_progress("ğŸ² æœ€ç»ˆç­–ç•¥: éšæœºè¡¥å……...")
        import random
        
        existing_indices = {r.get('index', -1) for r in all_results}
        available_indices = [i for i in range(len(texts)) if i not in existing_indices]
        
        needed = target_count - len(all_results)
        if len(available_indices) >= needed:
            selected_indices = random.sample(available_indices, needed)
        else:
            selected_indices = available_indices
        
        for idx in selected_indices:
            all_results.append({
                'text': texts[idx],
                'content': texts[idx],
                'score': 0.1,
                'page_num': idx + 1,
                'index': idx,
                'is_fallback': True,
                'method': 'éšæœºè¡¥å……'
            })
        
        used_methods.append("éšæœºè¡¥å……")
        log_progress(f"ğŸ“ éšæœºè¡¥å……: æ–°å¢ {len(selected_indices)} ä¸ªç»“æœ")
    
    # æ’åºç»“æœï¼ˆæŒ‰åˆ†æ•°é™åºï¼‰
    all_results.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    # æ€»ç»“æŠ¥å‘Š
    log_progress(f"ğŸ¯ é‡è¯•æœç´¢å®Œæˆ: å…± {len(all_results)} ä¸ªç»“æœï¼Œä½¿ç”¨ç­–ç•¥: {', '.join(used_methods)}")
    
    return all_results

if __name__ == "__main__":
    try:
        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        os.makedirs(CONFIG['UPLOAD_DIR'], exist_ok=True)
        os.makedirs(CONFIG['CACHE_DIR'], exist_ok=True) 
        os.makedirs(CONFIG['TEMP_DIR'], exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        
        logger.info("ğŸš€ å¯åŠ¨æ™ºèƒ½æ–‡æœ¬åˆ†æå¹³å°...")
        main()
    except Exception as e:
        logger.error(f"âŒ åº”ç”¨å¯åŠ¨å¤±è´¥: {str(e)}")
        st.error(f"âŒ åº”ç”¨å¯åŠ¨å¤±è´¥: {str(e)}")
        print(f"åº”ç”¨å¯åŠ¨é”™è¯¯: {e}")
        traceback.print_exc()
